{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 步骤 0: 环境设置\n",
    "\n",
    " 首先，安装所有必需的库。`accelerate` 和 `evaluate` 是 Hugging Face 生态系统的重要组成部分，可以简化训练和评估流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 步骤 1: 配置文件\n",
    "\n",
    " 我们将所有可配置的参数集中在一个类中，以便于管理和修改。\n",
    "\n",
    " 这包括模型名称、数据路径、训练参数等。\n",
    "\n",
    " 您可以在这里轻松切换 `eval_only` 和 `resume_from_checkpoint` 模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # --- 模型和数据路径配置 ---\n",
    "    model_checkpoint: str = \"google/pegasus-base\"\n",
    "    train_file_path: str = \"/kaggle/working/data/samsum/train.csv\"\n",
    "    validation_file_path: str = \"/kaggle/working/data/samsum/validation.csv\"\n",
    "    test_file_path: str = \"/kaggle/input/nanogpt-fudan-cs-30040/test.csv\"\n",
    "    output_dir: str = \"/kaggle/working/pegasus-samsum-finetuned\"\n",
    "    submission_file: str = \"/kaggle/working/submission.csv\"\n",
    "\n",
    "    # --- 运行模式配置 ---\n",
    "    # 如果为 True，将只进行评估，不进行训练。需要指定一个有效的 checkpoint 路径\n",
    "    eval_only: bool = False \n",
    "    # 从指定的 checkpoint 继续训练。设为 True 或 checkpoint 路径（例如 \"output_dir/checkpoint-500\"）\n",
    "    resume_from_checkpoint: bool = False\n",
    "\n",
    "    # --- 数据处理配置 ---\n",
    "    max_input_length: int = 1024  # 输入（对话）的最大长度\n",
    "    max_target_length: int = 128   # 输出（摘要）的最大长度\n",
    "    \n",
    "    # --- 训练参数配置 (Seq2SeqTrainingArguments) ---\n",
    "    # 评估策略，\"epoch\" 表示每个 epoch 结束后进行一次评估\n",
    "    evaluation_strategy: str = \"steps\"\n",
    "    eval_steps: int = 100 # 每100步评估一次\n",
    "    save_steps: int = 100 # 每100步保存一次\n",
    "    logging_steps: int = 25 # 每25步打印一次日志\n",
    "    \n",
    "    # 学习率和优化器\n",
    "    learning_rate: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    \n",
    "    # 批次大小和梯度累积\n",
    "    per_device_train_batch_size: int = 2\n",
    "    per_device_eval_batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 8  # 有效批次大小 = 2 * 8 = 16\n",
    "    \n",
    "    # 训练周期\n",
    "    num_train_epochs: int = 3\n",
    "    \n",
    "    # 混合精度训练 (bf16 适用于 Ampere 架构如 T4, A100; fp16 适用于 P100)\n",
    "    bf16: bool = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    fp16: bool = not bf16 and torch.cuda.is_available()\n",
    "    \n",
    "    # 其他\n",
    "    save_total_limit: int = 2 # 只保留最新的 2 个 checkpoint\n",
    "    predict_with_generate: bool = True # 在评估时使用 generate 方法，以计算 ROUGE\n",
    "    load_best_model_at_end: bool = True # 训练结束后加载最佳模型\n",
    "\n",
    "    # --- 生成参数配置 (用于评估和推理) ---\n",
    "    generation_num_beams: int = 4\n",
    "    generation_max_length: int = 128\n",
    "\n",
    "# 实例化配置\n",
    "config = TrainingConfig()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 步骤 2: 数据准备\n",
    "\n",
    " 这部分代码与您提供的原始代码相同。它读取原始的 `train.csv`，\n",
    "\n",
    " 将其打乱并划分为 95% 的训练集和 5% 的验证集，以便后续使用。\n",
    "\n",
    " 我们只取前1000条数据用于快速演示。如果要训练完整数据集，请注释掉 `df = df[:1000]` 这一行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"准备 SAMSum 数据集划分\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 读取原始CSV文件\n",
    "input_csv = '/kaggle/input/nanogpt-fudannlp-cs-30040/train.csv'\n",
    "print(f\"\\n读取数据: {input_csv}\")\n",
    "\n",
    "df = pd.read_csv(input_csv)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# !!! 注意: 为了快速演示，我们只使用前100个样本。\n",
    "# !!! 如果要进行完整训练，请注释掉或删除下面这一行。\n",
    "df = df[:100]\n",
    "\n",
    "total_samples = len(df)\n",
    "print(f\"总样本数: {total_samples}\")\n",
    "\n",
    "# 计算划分点（5%作为验证集）\n",
    "val_size = int(total_samples * 0.05)\n",
    "train_size = total_samples - val_size\n",
    "\n",
    "print(f\"训练集样本数: {train_size} ({train_size/total_samples*100:.1f}%)\")\n",
    "print(f\"验证集样本数: {val_size} ({val_size/total_samples*100:.1f}%)\")\n",
    "\n",
    "# 划分数据\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:]\n",
    "\n",
    "# 创建输出目录\n",
    "output_dir = os.path.dirname(config.train_file_path)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 保存训练集和验证集\n",
    "train_df.to_csv(config.train_file_path, index=False)\n",
    "print(f\"\\n训练集已保存: {config.train_file_path}\")\n",
    "\n",
    "val_df.to_csv(config.validation_file_path, index=False)\n",
    "print(f\"验证集已保存: {config.validation_file_path}\")\n",
    "\n",
    "print(\"\\n数据集划分完成！\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 步骤 3: 加载数据、模型和 Tokenizer\n",
    "\n",
    " 我们使用 `datasets` 库加载 CSV 文件，并加载 PEGASUS 模型及其对应的 Tokenizer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# 加载数据集\n",
    "raw_datasets = load_dataset('csv', data_files={'train': config.train_file_path, 'validation': config.validation_file_path})\n",
    "\n",
    "print(\"\\n数据集结构:\")\n",
    "print(raw_datasets)\n",
    "\n",
    "# 加载 Tokenizer 和模型\n",
    "print(f\"\\n加载预训练模型: {config.model_checkpoint}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 步骤 4: 数据预处理\n",
    "\n",
    " 定义一个函数来对数据进行分词（tokenize）。\n",
    "\n",
    " - `dialogue` 列作为模型的输入。\n",
    "\n",
    " - `summary` 列作为标签（label）。\n",
    "\n",
    " Tokenizer 会自动处理截断和填充的准备工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"dialogue\"], max_length=config.max_input_length, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # 将摘要作为标签进行分词\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=config.max_target_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# 使用 .map() 方法将预处理函数应用到整个数据集\n",
    "# batched=True 可以一次性处理多个样本，加快速度\n",
    "print(\"\\n开始对数据集进行分词...\")\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "print(\"分词完成！\")\n",
    "\n",
    "print(\"\\n处理后的数据集结构:\")\n",
    "print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 步骤 5: 定义评估指标 (ROUGE)\n",
    "\n",
    " 我们需要定义一个函数，在评估过程中计算 ROUGE 分数。\n",
    "\n",
    " 这个函数会在每个评估步骤被 `Trainer` 自动调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# 加载 ROUGE 评估指标\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # 解码生成的摘要\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # 将标签中的 -100 替换为 padding token ID，以便解码\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE 需要在每句话后添加换行符\n",
    "    decoded_preds = [\"\\n\".join(pred.strip().split()) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.strip().split()) for label in decoded_labels]\n",
    "    \n",
    "    # 计算 ROUGE 分数\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    # 提取关键指标\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # 添加生成文本的平均长度\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 步骤 6: 配置和初始化 Trainer\n",
    "\n",
    " - `Seq2SeqTrainingArguments`: 定义所有训练超参数。\n",
    "\n",
    " - `DataCollatorForSeq2Seq`: 负责在每个批次中智能地填充（pad）输入和标签。\n",
    "\n",
    " - `Seq2SeqTrainer`: 封装了所有训练和评估逻辑的核心类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练参数\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    eval_strategy=config.evaluation_strategy,\n",
    "    eval_steps=config.eval_steps,\n",
    "    save_steps=config.save_steps,\n",
    "    logging_steps=config.logging_steps,\n",
    "    learning_rate=config.learning_rate,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    weight_decay=config.weight_decay,\n",
    "    save_total_limit=config.save_total_limit,\n",
    "    num_train_epochs=config.num_train_epochs,\n",
    "    predict_with_generate=config.predict_with_generate,\n",
    "    fp16=config.fp16,\n",
    "    bf16=config.bf16,\n",
    "    load_best_model_at_end=config.load_best_model_at_end,\n",
    "    report_to=\"none\",  # 可设置为 \"wandb\", \"tensorboard\" 等\n",
    ")\n",
    "\n",
    "# 定义数据整理器\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# 初始化 Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 步骤 7: 执行训练或评估\n",
    "\n",
    " 根据 `config.eval_only` 的值，执行相应的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.eval_only:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"模式: 只进行评估\")\n",
    "    print(\"=\"*80)\n",
    "    # 确保 resume_from_checkpoint 指向一个有效的模型路径\n",
    "    if not os.path.isdir(str(config.resume_from_checkpoint)):\n",
    "         raise ValueError(f\"eval_only=True, 但 resume_from_checkpoint ('{config.resume_from_checkpoint}') 不是一个有效的目录。\")\n",
    "    \n",
    "    print(f\"从 checkpoint 加载模型进行评估: {config.resume_from_checkpoint}\")\n",
    "    eval_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])\n",
    "    print(\"\\n评估结果:\")\n",
    "    print(eval_results)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"模式: 开始训练\")\n",
    "    print(\"=\"*80)\n",
    "    # 如果设置了 resume_from_checkpoint，则从断点继续训练\n",
    "    train_result = trainer.train(resume_from_checkpoint=config.resume_from_checkpoint)\n",
    "    \n",
    "    # 保存最终的模型和训练状态\n",
    "    trainer.save_model()\n",
    "    trainer.save_state()\n",
    "    \n",
    "    print(\"\\n训练完成!\")\n",
    "    metrics = train_result.metrics\n",
    "    metrics[\"train_samples\"] = len(tokenized_datasets[\"train\"])\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 步骤 8: 在测试集上进行推理并生成提交文件\n",
    "\n",
    " 训练（或加载）好的最佳模型现在可用于为测试集生成摘要。\n",
    "\n",
    " 我们将结果保存为 `submission.csv`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"开始在测试集上进行推理...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 加载测试数据\n",
    "test_df = pd.read_csv(config.test_file_path)\n",
    "print(f\"加载了 {len(test_df)} 条测试样本。\")\n",
    "\n",
    "# 使用 trainer.predict 进行高效推理\n",
    "# 注意：trainer 内部已经加载了训练过程中的最佳模型（因为 load_best_model_at_end=True）\n",
    "test_dataset = load_dataset(\"csv\", data_files={\"test\": config.test_file_path})[\"test\"]\n",
    "\n",
    "def tokenize_test_data(examples):\n",
    "    return tokenizer(examples[\"dialogue\"], max_length=config.max_input_length, truncation=True)\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_test_data, batched=True)\n",
    "\n",
    "print(\"开始生成预测...\")\n",
    "predictions = trainer.predict(\n",
    "    tokenized_test_dataset,\n",
    "    max_length=config.generation_max_length,\n",
    "    num_beams=config.generation_num_beams,\n",
    ")\n",
    "\n",
    "# 解码预测结果\n",
    "decoded_summaries = tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True)\n",
    "\n",
    "# 清理生成的文本\n",
    "cleaned_summaries = [s.strip() for s in decoded_summaries]\n",
    "\n",
    "# 创建提交文件\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'summary': cleaned_summaries\n",
    "})\n",
    "\n",
    "submission_df.to_csv(config.submission_file, index=False)\n",
    "\n",
    "print(f\"\\n推理完成！提交文件已保存至: {config.submission_file}\")\n",
    "print(\"\\n提交文件预览:\")\n",
    "print(submission_df.head())\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
