{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Step 1: Install Dependencies\n",
    "\n",
    " 首先，安装完成任务所需的库，包括 `transformers`、`datasets` 用于模型和数据处理，`rouge_score` 和 `evaluate` 用于评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"transformers[torch]\" datasets evaluate rouge_score sacrebleu --quiet\n",
    "# !pip install accelerate -U --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Step 2: Data Preprocessing\n",
    "\n",
    " 这部分代码与您提供的原始代码逻辑一致。\n",
    "\n",
    "\n",
    "\n",
    " - 从 Kaggle 输入的 `train.csv` 读取数据。\n",
    "\n",
    " - 随机划分 95% 的数据作为训练集，5% 作为验证集。\n",
    "\n",
    " - 将划分后的数据保存到 `/kaggle/working/data/samsum/` 目录下，以便后续加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# 确保 argparse 在 notebook 环境中能正常工作\n",
    "import sys\n",
    "if 'ipykernel' in sys.modules:\n",
    "    sys.argv = ['']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"准备 SAMSum 数据集划分\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 读取原始CSV文件\n",
    "# 注意：请确保已将比赛数据添加到 notebook 的输入目录 /kaggle/input/\n",
    "input_csv = '/kaggle/input/nanogpt-fudannlp-cs-30040/train.csv'\n",
    "test_csv_path = '/kaggle/input/nanogpt-fudannlp-cs-30040/test.csv'\n",
    "print(f\"\\n读取训练数据: {input_csv}\")\n",
    "\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# 随机打乱数据\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df = df[:100]\n",
    "\n",
    "# 计算划分点（5%作为验证集）\n",
    "total_samples = len(df)\n",
    "val_size = int(total_samples * 0.05)\n",
    "train_size = total_samples - val_size\n",
    "\n",
    "print(f\"总样本数: {total_samples}\")\n",
    "print(f\"训练集样本数: {train_size} ({train_size/total_samples*100:.1f}%)\")\n",
    "print(f\"验证集样本数: {val_size} ({val_size/total_samples*100:.1f}%)\")\n",
    "\n",
    "# 划分数据\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:]\n",
    "\n",
    "# 创建输出目录\n",
    "output_dir = '/kaggle/working/data/samsum'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 保存训练集\n",
    "train_csv_path = os.path.join(output_dir, 'train.csv')\n",
    "train_df.to_csv(train_csv_path, index=False)\n",
    "print(f\"\\n训练集已保存: {train_csv_path}\")\n",
    "\n",
    "# 保存验证集\n",
    "val_csv_path = os.path.join(output_dir, 'validation.csv')\n",
    "val_df.to_csv(val_csv_path, index=False)\n",
    "print(f\"验证集已保存: {val_csv_path}\")\n",
    "\n",
    "print(\"\\n数据集划分完成！\")\n",
    "print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Step 3: Model Fine-tuning with PEGASUS-Base\n",
    "\n",
    "\n",
    "\n",
    " 这是重写后的核心部分，使用 `PEGASUS-Base` 模型和 Hugging Face `Trainer` API。\n",
    "\n",
    "\n",
    "\n",
    " ## 主要改动和特性：\n",
    "\n",
    "\n",
    "\n",
    " 1.  **模型和分词器**:\n",
    "\n",
    "     *   加载 `google/pegasus-base` 预训练模型和对应的 `PegasusTokenizer`。\n",
    "\n",
    "\n",
    "\n",
    " 2.  **数据加载与处理**:\n",
    "\n",
    "     *   使用 `datasets` 库从上一步生成的 CSV 文件中加载数据。\n",
    "\n",
    "     *   定义 `preprocess_function` 来对对话（`dialogue`）和摘要（`summary`）进行分词。这是 Seq2Seq 模型的标准做法。\n",
    "\n",
    "\n",
    "\n",
    " 3.  **训练框架**:\n",
    "\n",
    "     *   采用 `Seq2SeqTrainer` 和 `Seq2SeqTrainingArguments`，这是 Hugging Face 为序列到序列任务（如摘要）设计的标准高级 API。\n",
    "\n",
    "     *   **TPU 支持**: `Trainer` API 内部自动处理 PyTorch/XLA，无需手动编写 TPU 特定代码。\n",
    "\n",
    "     *   **混合精度**: 通过在 `Seq2SeqTrainingArguments` 中设置 `bf16=True` 来启用 bfloat16 训练。\n",
    "\n",
    "\n",
    "\n",
    " 4.  **评估逻辑**:\n",
    "\n",
    "     *   定义 `compute_metrics` 函数，该函数在每次评估时被 `Trainer` 调用。\n",
    "\n",
    "     *   函数内部对模型生成的 `predictions` 和真实的 `labels` 进行解码，然后使用 `evaluate.load('rouge')` 计算 ROUGE 分数，这与原始评估逻辑的目标完全一致。\n",
    "\n",
    "\n",
    "\n",
    " 5.  **参数化**:\n",
    "\n",
    "     *   使用 `argparse`（并兼容 notebook）来配置 `eval_only` 和 `resume_from_checkpoint` 参数。你可以通过修改 `parser.parse_args()` 中的参数来控制脚本行为。\n",
    "\n",
    "\n",
    "\n",
    " 6.  **模型导出**:\n",
    "\n",
    "     *   训练完成后，`Trainer` 会自动将模型权重（`pytorch_model.bin`）、配置文件等保存在指定的 `output_dir` 中。\n",
    "\n",
    "     *   这个目录包含了完整的模型，可以被 `transformers` 库在任何支持 PyTorch 的环境（包括 P100 GPU）中加载。\n",
    "\n",
    "\n",
    "\n",
    " 7.  **Loss 掩码**:\n",
    "\n",
    "     *   在数据预处理和 `DataCollatorForSeq2Seq` 中，用于 padding 的 label token 会被自动设置为 `-100`。PyTorch 的交叉熵损失函数会忽略这些值为 `-100` 的标签，从而实现了正确的 loss 掩码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import evaluate # Hugging Face's new evaluation library\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    PegasusForConditionalGeneration,\n",
    "    PegasusTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "# --- Configuration using argparse ---\n",
    "# We use argparse to handle parameters like eval_only and resume\n",
    "parser = argparse.ArgumentParser(description=\"Fine-tune PEGASUS-Base on SAMSum dataset.\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"google/pegasus-base\", help=\"Name of the pretrained model.\")\n",
    "parser.add_argument(\"--output_dir\", type=str, default=\"/kaggle/working/pegasus-samsum-finetuned\", help=\"Directory to save the model and results.\")\n",
    "parser.add_argument(\"--train_file\", type=str, default=train_csv_path, help=\"Path to the training CSV file.\")\n",
    "parser.add_argument(\"--validation_file\", type=str, default=val_csv_path, help=\"Path to the validation CSV file.\")\n",
    "parser.add_argument(\"--eval_only\", action=\"store_true\", help=\"If set, only run evaluation on the validation set.\")\n",
    "parser.add_argument(\"--resume_from_checkpoint\", type=str, default=None, help=\"Path to a checkpoint to resume training from.\")\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "parser.add_argument(\"--num_train_epochs\", type=int, default=3, help=\"Number of training epochs.\")\n",
    "parser.add_argument(\"--per_device_train_batch_size\", type=int, default=4, help=\"Batch size per device for training.\")\n",
    "parser.add_argument(\"--per_device_eval_batch_size\", type=int, default=8, help=\"Batch size per device for evaluation.\")\n",
    "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=4, help=\"Number of steps for gradient accumulation.\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=5.6e-5, help=\"Learning rate.\")\n",
    "parser.add_argument(\"--warmup_steps\", type=int, default=500, help=\"Number of warmup steps.\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=0.01, help=\"Weight decay.\")\n",
    "parser.add_argument(\"--max_input_length\", type=int, default=1024, help=\"Maximum length of input sequences.\")\n",
    "parser.add_argument(\"--max_target_length\", type=int, default=128, help=\"Maximum length of target (summary) sequences.\")\n",
    "parser.add_argument(\"--logging_steps\", type=int, default=100, help=\"Log every N steps.\")\n",
    "parser.add_argument(\"--eval_steps\", type=int, default=500, help=\"Evaluate every N steps.\")\n",
    "parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every N steps.\")\n",
    "\n",
    "# Parse arguments\n",
    "# In a script, you would use: args = parser.parse_args()\n",
    "# For Kaggle notebook compatibility, we manually set the args:\n",
    "args = parser.parse_args([\n",
    "    '--num_train_epochs', '1', # Use 1 epoch for a quick demonstration\n",
    "    '--per_device_train_batch_size', '1', # For TPU v5e-8, you can increase this to 8 or 16\n",
    "    '--per_device_eval_batch_size', '2',\n",
    "    '--gradient_accumulation_steps', '8', # Effective batch size = 1 * 8 * 8 (cores) = 64\n",
    "])\n",
    "print(\"Script arguments:\", args)\n",
    "\n",
    "\n",
    "# --- 1. Load Tokenizer and Model ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Loading tokenizer and model for '{args.model_name}'...\")\n",
    "# The tokenizer will be used to convert text to numbers (tokens)\n",
    "tokenizer = PegasusTokenizer.from_pretrained(args.model_name)\n",
    "# The model is the neural network architecture we will fine-tune\n",
    "model = PegasusForConditionalGeneration.from_pretrained(args.model_name)\n",
    "print(\"Tokenizer and model loaded successfully.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "# --- 2. Load and Preprocess Dataset ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Loading and preprocessing data...\")\n",
    "# Load data from the CSV files we created earlier\n",
    "raw_datasets = load_dataset('csv', data_files={'train': args.train_file, 'validation': args.validation_file})\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenizes the dialogue and summary.\"\"\"\n",
    "    # The `text_target` argument is for the labels/summary.\n",
    "    inputs = tokenizer(examples['dialogue'], max_length=args.max_input_length, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['summary'], max_length=args.max_target_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # The DataCollator will handle creating the decoder_input_ids\n",
    "    # Padded labels are automatically set to -100 to be ignored in loss calculation.\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    return inputs\n",
    "\n",
    "# Apply the preprocessing function to all splits of the dataset\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "print(\"Data preprocessing complete.\")\n",
    "print(\"Sample of tokenized data:\", tokenized_datasets[\"train\"][0])\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "# --- 3. Define Evaluation Metric (ROUGE) ---\n",
    "# The logic here is consistent with the original script's goal: calculate ROUGE scores.\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes ROUGE scores for a batch of predictions.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Decode predictions and labels back to text\n",
    "    # The tokenizer.decode function converts token IDs back to strings\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(pred.strip().split()) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.strip().split()) for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    # Extract F-measures\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length to metrics\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "\n",
    "# --- 4. Configure Training ---\n",
    "# Data collator pads inputs and labels dynamically for each batch\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Define training arguments\n",
    "# This object contains all the settings for the training run\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=args.output_dir,\n",
    "    num_train_epochs=args.num_train_epochs,\n",
    "    per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    learning_rate=args.learning_rate,\n",
    "    warmup_steps=args.warmup_steps,\n",
    "    weight_decay=args.weight_decay,\n",
    "    \n",
    "    # Logging, evaluation, and saving settings\n",
    "    logging_dir=f\"{args.output_dir}/logs\",\n",
    "    logging_steps=args.logging_steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=args.eval_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=args.save_steps,\n",
    "    save_total_limit=3, # Only keep the last 3 checkpoints\n",
    "    \n",
    "    # Enable bfloat16 for TPU training\n",
    "    bf16=True, \n",
    "    \n",
    "    # Enable generation for evaluation to calculate ROUGE\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=args.max_target_length,\n",
    "    \n",
    "    # Load best model at the end\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    \n",
    "    # Other settings\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\", # Disable wandb/tensorboard logging for simplicity\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. Run Training or Evaluation ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if args.eval_only:\n",
    "    print(\"Running evaluation only...\")\n",
    "    # Make sure to specify a checkpoint to evaluate\n",
    "    if args.resume_from_checkpoint is None:\n",
    "        raise ValueError(\"Must provide a checkpoint path via --resume_from_checkpoint for eval_only mode.\")\n",
    "    metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])\n",
    "    print(\"Evaluation metrics:\", metrics)\n",
    "else:\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    # The `resume_from_checkpoint` argument can be a boolean (True) to auto-find the last\n",
    "    # checkpoint in output_dir, or a string path to a specific checkpoint.\n",
    "    resume_path = args.resume_from_checkpoint if args.resume_from_checkpoint is not None else False\n",
    "    train_result = trainer.train(resume_from_checkpoint=resume_path)\n",
    "    \n",
    "    # Save training metrics and final model\n",
    "    trainer.save_model() # This saves the final model, tokenizer, and config\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    print(\"Fine-tuning complete.\")\n",
    "    \n",
    "print(\"=\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Step 4: Generate Submission File\n",
    "\n",
    "\n",
    "\n",
    " 这一部分使用微调好的模型对官方的 `test.csv` 文件进行推理，并生成 `submission.csv` 文件。\n",
    "\n",
    "\n",
    "\n",
    " - 加载 `test.csv`。\n",
    "\n",
    " - 遍历每一条对话。\n",
    "\n",
    " - 使用 `trainer.predict` 方法高效地生成摘要。\n",
    "\n",
    " - 将结果格式化并保存到 CSV 文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Generating summaries for the test set...\")\n",
    "\n",
    "# Load the test data\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "test_dialogues = test_df['dialogue'].tolist()\n",
    "\n",
    "# Create a prediction dataset\n",
    "from datasets import Dataset\n",
    "test_dataset = Dataset.from_dict({\"dialogue\": test_dialogues})\n",
    "\n",
    "# Tokenize the test data\n",
    "def tokenize_test_data(examples):\n",
    "    return tokenizer(examples['dialogue'], max_length=args.max_input_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_test_data, batched=True)\n",
    "\n",
    "# Generate predictions using the trainer\n",
    "print(\"Running prediction on the test set...\")\n",
    "test_predictions = trainer.predict(tokenized_test_dataset)\n",
    "\n",
    "# Decode the generated summaries\n",
    "print(\"Decoding predictions...\")\n",
    "decoded_summaries = tokenizer.batch_decode(test_predictions.predictions, skip_special_tokens=True)\n",
    "\n",
    "# Clean up the summaries\n",
    "cleaned_summaries = [s.strip() for s in decoded_summaries]\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'summary': cleaned_summaries\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_path = os.path.join(args.output_dir, \"submission.csv\")\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file saved to: {submission_path}\")\n",
    "print(\"Sample of the submission file:\")\n",
    "print(submission_df.head())\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
