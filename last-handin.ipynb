{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":120001,"databundleVersionId":14409389,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"671e3c9c-a725-480d-a86b-cfe9a2d95140","cell_type":"markdown","source":"# åŠ©æ•™è¯·çœ‹å…¨å±€é…ç½®å¤„çš„é…ç½®ï¼Œç°åœ¨é…ç½®çš„æ˜¯ä»…æ¨ç†ï¼š\n```python\nclass Config:\n    # --- æ ¸å¿ƒå¼€å…³ (åŠ©æ•™è¯·æ³¨æ„è¿™é‡Œ) ---\n    ONLY_INFERENCE = True  # å¦‚æœåªæƒ³ç”Ÿæˆ submission.csv è€Œä¸è¿›è¡Œè®­ç»ƒï¼Œè¯·å°†æ­¤è®¾ç½®ä¸º True\n```\næ¨ç†äº§ç”Ÿsubmission.csvå¯èƒ½éœ€è¦10åˆ†é’Ÿå·¦å³\n# å‚æ•°é‡çš„æ‰“å°ä»£ç ä½äºæ¨ç†äº§ç”Ÿsubmissionçš„ä»£ç å‰20è¡Œçš„ä½ç½®\n# å¦‚æœéœ€è¦æŸ¥çœ‹å‚æ•°é‡çš„æ‰“å°å†…å®¹ï¼Œçƒ¦è¯·ç¨ç­‰2åˆ†é’Ÿï¼Œä»huggingfaceä¸Šä¸‹è½½æˆ‘å¾®è°ƒå®Œæˆçš„æ¨¡å‹æƒé‡ï¼›\n# å³å°†å¼€å§‹æ¨ç†æ—¶ï¼Œå‚æ•°é‡ä¼šå½©è‰²é«˜äº®æ‰“å°å‡ºæ¥","metadata":{}},{"id":"934d5e86","cell_type":"code","source":"# !pip install datasets rouge_score emoji","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:30:09.080794Z","iopub.execute_input":"2025-12-07T13:30:09.081421Z","iopub.status.idle":"2025-12-07T13:30:09.085429Z","shell.execute_reply.started":"2025-12-07T13:30:09.081391Z","shell.execute_reply":"2025-12-07T13:30:09.084747Z"}},"outputs":[],"execution_count":1},{"id":"23d95314-b014-4300-97f7-4c8455e5faee","cell_type":"markdown","source":"è¿™ä¸ªerrorä¸ç”¨ç®¡ï¼Œä¼šæ‰“å°5æ¬¡ä¹‹åæ­£å¸¸è¿è¡Œï¼š\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'","metadata":{}},{"id":"2f07a9bb-89af-4811-86f0-9bf9646d79ef","cell_type":"code","source":"import pandas as pd\nimport os\nimport re\nimport numpy as np\nimport csv\nimport time\nimport math\nimport emoji\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom contextlib import nullcontext\nfrom collections import OrderedDict\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    BartForConditionalGeneration, \n    BartConfig, \n    AutoTokenizer,\n    get_scheduler\n)\nfrom huggingface_hub import snapshot_download","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:30:09.086657Z","iopub.execute_input":"2025-12-07T13:30:09.087282Z","iopub.status.idle":"2025-12-07T13:30:39.916663Z","shell.execute_reply.started":"2025-12-07T13:30:09.087257Z","shell.execute_reply":"2025-12-07T13:30:39.916097Z"}},"outputs":[{"name":"stderr","text":"2025-12-07 13:30:23.870921: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765114224.043288      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765114224.091714      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":2},{"id":"c338912d-dec7-4b62-8d40-182501f320ce","cell_type":"markdown","source":"# 1. å…¨å±€é…ç½® (Configuration)","metadata":{}},{"id":"4efea237-5448-4789-ac4f-038dbca91fff","cell_type":"code","source":"# =============================================================================\n# 1. å…¨å±€é…ç½® (Configuration)\n# =============================================================================\n\nclass Config:\n    # --- æ ¸å¿ƒå¼€å…³ (åŠ©æ•™è¯·æ³¨æ„è¿™é‡Œ) ---\n    ONLY_INFERENCE = True  # å¦‚æœåªæƒ³ç”Ÿæˆ submission.csv è€Œä¸è¿›è¡Œè®­ç»ƒï¼Œè¯·å°†æ­¤è®¾ç½®ä¸º True\n    \n    \n    input_ta_train = '/kaggle/input/nanogpt-fudannlp-cs-30040/train.csv'   \n    input_test = '/kaggle/input/nanogpt-fudannlp-cs-30040/test.csv'        \n\n    # --- å·¥ä½œç›®å½• ---\n    dataset_path = '/kaggle/working/data'\n    train_clean_csv = 'train_clean.csv'       \n    val_clean_csv = 'validation_clean.csv'    \n\n    # --- HuggingFace æƒé‡é…ç½®  ---\n    hf_repo_id = 'Kevin36277/finetuned-model'\n    hf_subfolder = 'out-new-v1' \n    download_dir = '/kaggle/working/downloaded_model'\n\n    # --- æ¨¡å‹é…ç½® ---\n    base_model_name = 'facebook/bart-large'\n    \n    # è®­ç»ƒè¾“å‡ºè·¯å¾„\n    out_dir = '/kaggle/working/out-v1-optimized'\n    \n    # --- è®­ç»ƒè¶…å‚æ•° ---\n    batch_size = 8#64\n    gradient_accumulation_steps = 1\n    max_iters = 2#3600\n    \n    # é•¿åº¦æ§åˆ¶\n    max_dialogue_tokens = 364  \n    max_summary_tokens = 64    \n    max_source_length = 384\n    max_target_length = 64\n    min_length = 11\n    \n    # ä¼˜åŒ–å™¨\n    learning_rate = 5e-5\n    weight_decay = 0.01\n    beta1 = 0.9\n    beta2 = 0.999\n    grad_clip = 1.0\n    label_smoothing = 0.0\n    \n    # æ­£åˆ™åŒ– & å‰ªæ\n    use_rdrop = True\n    rdrop_alpha = 0.7\n    dropout_rate = 0.07\n    attention_dropout = 0.1\n    activation_dropout = 0.1\n    decoder_layers = 11\n    \n    # ç”Ÿæˆé…ç½®\n    num_beams = 8\n    length_penalty = 1.0\n    no_repeat_ngram_size = 3\n    \n    # è°ƒåº¦å™¨\n    lr_scheduler_type = \"cosine\"\n    warmup_iters = 200\n    \n    # æ—©åœ\n    use_early_stopping = True\n    early_stopping_patience = 5\n    \n    # I/O\n    eval_interval = 200\n    log_interval = 50\n    eval_iters = 40\n    eval_rouge_during_training = True\n    rouge_eval_samples = 500\n    \n    # ç³»ç»Ÿ\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    dtype = 'float16' \n    compile = False#ä¸æ˜¯A100ï¼Œä¸æ”¯æŒTrue\n    use_multi_gpu = True\n    num_workers = 4 \n    persistent_workers = True\n\nconfig = Config()\n\n# ç²¾åº¦å›é€€æ£€æŸ¥\nif torch.cuda.is_available():\n    if not (torch.cuda.is_bf16_supported() and config.dtype == 'bfloat16'):\n        print(\"âš ï¸  GPU ä¸æ”¯æŒ bfloat16 æˆ–æœªå¯ç”¨ï¼Œå›é€€åˆ° float16\")\n        config.dtype = 'float16'\nelse:\n    config.device = 'cpu'\n    config.dtype = 'float32'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:30:39.917833Z","iopub.execute_input":"2025-12-07T13:30:39.918332Z","iopub.status.idle":"2025-12-07T13:30:40.057104Z","shell.execute_reply.started":"2025-12-07T13:30:39.918313Z","shell.execute_reply":"2025-12-07T13:30:40.056486Z"}},"outputs":[{"name":"stdout","text":"âš ï¸  GPU ä¸æ”¯æŒ bfloat16 æˆ–æœªå¯ç”¨ï¼Œå›é€€åˆ° float16\n","output_type":"stream"}],"execution_count":3},{"id":"40707671-2955-4b3a-bc76-5642763e5fc1","cell_type":"markdown","source":"# 2. å·¥å…·å‡½æ•°ï¼šä¸‹è½½ä¸åŠ è½½","metadata":{}},{"id":"558b97da-f065-4b3a-b6c8-2428b28417e9","cell_type":"code","source":"# =============================================================================\n# 2. å·¥å…·å‡½æ•°ï¼šä¸‹è½½ä¸åŠ è½½\n# =============================================================================\n\ndef download_model_from_hf():\n    \"\"\"ä» HuggingFace ä¸‹è½½æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ¨¡å‹æ–‡ä»¶\"\"\"\n    print(f\"\\n>>> æ­£åœ¨ä» HuggingFace ä¸‹è½½æ¨¡å‹æƒé‡...\")\n    print(f\"    Repo: {config.hf_repo_id}\")\n    print(f\"    Folder: {config.hf_subfolder}\")\n    \n    try:\n        local_path = snapshot_download(\n            repo_id=config.hf_repo_id,\n            allow_patterns=f\"{config.hf_subfolder}/*\",\n            local_dir=config.download_dir,\n            local_dir_use_symlinks=False \n        )\n        \n        model_path = os.path.join(local_path, config.hf_subfolder)\n        print(f\"    âœ… ä¸‹è½½å®Œæˆã€‚æ¨¡å‹ä½äº: {model_path}\")\n        return model_path\n    except Exception as e:\n        print(f\"    âŒ ä¸‹è½½å¤±è´¥: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:30:40.057767Z","iopub.execute_input":"2025-12-07T13:30:40.058013Z","iopub.status.idle":"2025-12-07T13:30:40.062880Z","shell.execute_reply.started":"2025-12-07T13:30:40.057993Z","shell.execute_reply":"2025-12-07T13:30:40.062208Z"}},"outputs":[],"execution_count":4},{"id":"ce87d5e5-4b16-4661-b240-44df57feb8ba","cell_type":"markdown","source":"# 3. æ•°æ®å¤„ç† (Preprocessing)","metadata":{}},{"id":"d0a2239d-b15a-4b3e-9d83-ce6ca568e144","cell_type":"code","source":"# =============================================================================\n# 3. æ•°æ®å¤„ç† (Preprocessing)\n# =============================================================================\n\ndef clean_text_remove_emoji(text):\n    \"\"\"ä½¿ç”¨ emoji åº“ç²¾å‡†åˆ é™¤è¡¨æƒ…\"\"\"\n    if not isinstance(text, str): \n        return \"\"\n    text = emoji.replace_emoji(text, replace='')\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndef process_dataframe(df, tokenizer, name_tag, is_test=False):\n    \"\"\"å¤„ç†DataFrameï¼Œç»Ÿä¸€åˆ—åã€æ¸…æ´—ã€ç­›é€‰\"\"\"\n    if df is None or len(df) == 0: \n        return pd.DataFrame()\n    print(f\"    >>> å¤„ç†: {name_tag} (åŸå§‹: {len(df)})\")\n    \n    df = df.copy()\n    # ç»Ÿä¸€åˆ—å\n    col_map = {}\n    for col in df.columns:\n        if col.lower() in ['text', 'document', 'dialogue', 'content']: \n            col_map[col] = 'dialogue'\n        elif col.lower() in ['summary', 'target', 'headline']: \n            col_map[col] = 'summary'\n    if col_map: \n        df.rename(columns=col_map, inplace=True)\n\n    if 'id' not in df.columns: \n        df['id'] = [f\"{name_tag}_{i}\" for i in range(len(df))]\n    \n    df.replace(\"\", pd.NA, inplace=True)\n    subset = ['dialogue'] if is_test else ['dialogue', 'summary']\n    df.dropna(subset=subset, inplace=True)\n    \n    # å»é™¤è¡¨æƒ…\n    df['dialogue'] = df['dialogue'].apply(clean_text_remove_emoji)\n    if not is_test: \n        df['summary'] = df['summary'].apply(clean_text_remove_emoji)\n    \n    # é•¿åº¦ç­›é€‰ (ä»…è®­ç»ƒ)\n    if not is_test:\n        df['d_len'] = df['dialogue'].apply(lambda x: len(tokenizer.encode(str(x), add_special_tokens=False)))\n        df['s_len'] = df['summary'].apply(lambda x: len(tokenizer.encode(str(x), add_special_tokens=False)))\n        before_len = len(df)\n        df = df[\n            (df['d_len'] <= config.max_dialogue_tokens) & \n            (df['s_len'] <= config.max_summary_tokens) &\n            (df['d_len'] > 10) & \n            (df['s_len'] > 10)\n        ]\n        print(f\"    [é•¿åº¦ç­›é€‰] ç§»é™¤äº† {before_len - len(df)} è¡Œ\")\n    \n    return df\n\ndef prepare_data_pipeline():\n    \"\"\"æ•°æ®å‡†å¤‡æµæ°´çº¿\"\"\"\n    print(\"=\" * 60)\n    print(\"STEP 1: æ•°æ®å‡†å¤‡ (Data Prep)\")\n    print(\"=\" * 60)\n    os.makedirs(config.dataset_path, exist_ok=True)\n    \n    # å°è¯•åŠ è½½ tokenizer ç”¨äºé•¿åº¦è®¡ç®—\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(config.base_model_name)\n    except:\n        tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large')\n\n    # 1. TA æ•°æ®\n    if os.path.exists(config.input_ta_train):\n        df_ta = pd.read_csv(config.input_ta_train)\n        df_ta = process_dataframe(df_ta, tokenizer, \"TA_Train\")\n        \n        # ç®€å•åˆ’åˆ†\n        df_ta = df_ta.sample(frac=1, random_state=42).reset_index(drop=True)\n        val_size = 700 if len(df_ta) > 2000 else int(len(df_ta) * 0.1)\n        df_val = df_ta.iloc[-val_size:].copy()\n        df_train = df_ta.iloc[:-val_size].copy()\n        \n        # ä¿å­˜æ—¶å»é™¤é•¿åº¦åˆ—\n        train_path = os.path.join(config.dataset_path, config.train_clean_csv)\n        val_path = os.path.join(config.dataset_path, config.val_clean_csv)\n        save_cols = ['id', 'dialogue', 'summary']\n        df_train[save_cols].to_csv(train_path, index=False)\n        df_val[save_cols].to_csv(val_path, index=False)\n        print(f\"    âœ… è®­ç»ƒé›†: {len(df_train)} æ¡\")\n        print(f\"    âœ… éªŒè¯é›†: {len(df_val)} æ¡\")\n        print(f\"    âœ… æ•°æ®å·²ä¿å­˜è‡³ {config.dataset_path}\")\n    else:\n        print(f\"    âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ® {config.input_ta_train}ï¼Œè·³è¿‡æ•°æ®å‡†å¤‡(å¦‚æœæ˜¯æ¨ç†æ¨¡å¼å¯å¿½ç•¥)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:30:40.064377Z","iopub.execute_input":"2025-12-07T13:30:40.064733Z","iopub.status.idle":"2025-12-07T13:30:40.081812Z","shell.execute_reply.started":"2025-12-07T13:30:40.064716Z","shell.execute_reply":"2025-12-07T13:30:40.080989Z"}},"outputs":[],"execution_count":5},{"id":"4ca9cbbc-4ba1-4a54-b0cc-10040928690f","cell_type":"markdown","source":"# 4. Dataset ç±»","metadata":{}},{"id":"38b98f90-579d-49c5-a253-b2dfdcdb40e1","cell_type":"code","source":"# =============================================================================\n# 4. Dataset ç±»\n# =============================================================================\n\nclass SummarizationDataset(Dataset):\n    \"\"\"ä¼˜åŒ–ç‰ˆæ•°æ®é›†ç±»\"\"\"\n    def __init__(self, csv_path, tokenizer, max_source_length, max_target_length, mode='train'):\n        self.tokenizer = tokenizer\n        self.max_source_length = max_source_length\n        self.max_target_length = max_target_length\n        self.mode = mode\n        \n        self.dialogues = []\n        self.summaries = []\n        self.ids = []\n        \n        if os.path.exists(csv_path):\n            with open(csv_path, 'r', encoding='utf-8') as f:\n                reader = csv.DictReader(f)\n                for row in reader:\n                    self.dialogues.append(row['dialogue'])\n                    if 'id' in row:\n                        self.ids.append(row['id'])\n                    if mode != 'test' and 'summary' in row:\n                        self.summaries.append(row['summary'])\n            print(f\"  åŠ è½½äº† {len(self.dialogues)} ä¸ªæ ·æœ¬ from {csv_path}\")\n        else:\n            raise FileNotFoundError(f\"æ‰¾ä¸åˆ°æ–‡ä»¶: {csv_path}\")\n    \n    def __len__(self):\n        return len(self.dialogues)\n    \n    def __getitem__(self, idx):\n        dialogue = str(self.dialogues[idx])\n        \n        source = self.tokenizer(\n            dialogue,\n            max_length=self.max_source_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        item = {\n            'input_ids': source['input_ids'].squeeze(),\n            'attention_mask': source['attention_mask'].squeeze(),\n        }\n        \n        if self.mode != 'test' and idx < len(self.summaries):\n            summary = str(self.summaries[idx])\n            target = self.tokenizer(\n                text_target=summary,\n                max_length=self.max_target_length,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )\n            \n            labels = target['input_ids'].squeeze()\n            labels[labels == self.tokenizer.pad_token_id] = -100\n            item['labels'] = labels\n        \n        if idx < len(self.ids):\n            item['id'] = self.ids[idx]\n        \n        return item","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:30:40.082483Z","iopub.execute_input":"2025-12-07T13:30:40.082696Z","iopub.status.idle":"2025-12-07T13:30:40.095815Z","shell.execute_reply.started":"2025-12-07T13:30:40.082680Z","shell.execute_reply":"2025-12-07T13:30:40.095209Z"}},"outputs":[],"execution_count":6},{"id":"25933f84-cec0-4bed-8109-1d1278f9c59e","cell_type":"markdown","source":"# 5. è¾…åŠ©å‡½æ•°","metadata":{}},{"id":"ee7cd737-f076-4a34-ba5a-65e5dfc8b98c","cell_type":"code","source":"# =============================================================================\n# 5. è¾…åŠ©å‡½æ•°\n# =============================================================================\n\ndef get_grouped_parameters(model, weight_decay):\n    \"\"\"\n    ä¼˜åŒ–å™¨å‚æ•°åˆ†ç»„ï¼šBias å’Œ LayerNorm ä¸ä½¿ç”¨ weight decay\n    è¿™æ˜¯ BERT/BART è®­ç»ƒçš„æ ‡å‡†åšæ³•\n    \"\"\"\n    no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() \n                      if not any(nd in n for nd in no_decay) and p.requires_grad],\n            \"weight_decay\": weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() \n                      if any(nd in n for nd in no_decay) and p.requires_grad],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    return optimizer_grouped_parameters\n\ndef compute_kl_loss(logits1, logits2):\n    \"\"\"\n    ä¼˜åŒ–ç‰ˆ KL Lossï¼šä½¿ç”¨ reduction='batchmean'\n    \"\"\"\n    vocab_size = logits1.size(-1)\n    logits1_flat = logits1.view(-1, vocab_size)\n    logits2_flat = logits2.view(-1, vocab_size)\n    \n    # åŒå‘ KL æ•£åº¦\n    kl_loss = F.kl_div(\n        F.log_softmax(logits1_flat, dim=-1),\n        F.softmax(logits2_flat, dim=-1),\n        reduction='batchmean'\n    ) + F.kl_div(\n        F.log_softmax(logits2_flat, dim=-1),\n        F.softmax(logits1_flat, dim=-1),\n        reduction='batchmean'\n    )\n    \n    return kl_loss / 2\n\n@torch.no_grad()\ndef estimate_loss(model, dataloader_dict, ctx):\n    \"\"\"ä¼°ç®—éªŒè¯é›†æŸå¤±\"\"\"\n    out = {}\n    model.eval()\n    \n    for split in ['train', 'val']:\n        losses = []\n        dataloader = dataloader_dict[split]\n        \n        for i, batch in enumerate(dataloader):\n            if i >= config.eval_iters:\n                break\n            \n            # è¿‡æ»¤æ‰étensorçš„é”®ï¼ˆå¦‚'id'ï¼‰\n            batch = {k: v.to(config.device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n            \n            with ctx:\n                outputs = model(**batch)\n                loss = outputs.loss\n                if hasattr(model, 'module'):\n                    loss = loss.mean()\n            \n            losses.append(loss.item())\n        \n        out[split] = np.mean(losses) if losses else float('inf')\n    \n    model.train()\n    return out\n\ndef calculate_rouge(reference_summary, generated_summary):\n    \"\"\"è®¡ç®— ROUGE åˆ†æ•°\"\"\"\n    try:\n        from rouge_score import rouge_scorer\n        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n        scores = scorer.score(reference_summary, generated_summary)\n        return {\n            'rouge1': scores['rouge1'].fmeasure,\n            'rouge2': scores['rouge2'].fmeasure,\n            'rougeL': scores['rougeL'].fmeasure\n        }\n    except ImportError:\n        return None\n\n@torch.no_grad()\ndef evaluate_rouge(model, tokenizer, val_dataloader, ctx, num_samples=10):\n    \"\"\"è¯„ä¼° ROUGE\"\"\"\n    eval_model = model.module if hasattr(model, 'module') else model\n    eval_model.eval()\n    \n    rouge1_scores = []\n    rouge2_scores = []\n    rougeL_scores = []\n    \n    print(f\"  æ­£åœ¨è®¡ç®— ROUGE (é‡‡æ · {num_samples} æ¡)...\")\n    print_count = 0\n    samples_processed = 0\n    \n    for batch in val_dataloader:\n        if samples_processed >= num_samples:\n            break\n        \n        input_ids = batch['input_ids'].to(config.device)\n        attention_mask = batch['attention_mask'].to(config.device)\n        labels = batch['labels']\n        \n        # è§£ç å‚è€ƒæ‘˜è¦\n        labels_copy = labels.clone()\n        labels_copy[labels_copy == -100] = tokenizer.pad_token_id\n        ref_summaries = tokenizer.batch_decode(labels_copy, skip_special_tokens=True)\n        \n        with ctx:\n            outputs = eval_model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=config.max_target_length,\n                min_length=config.min_length,\n                num_beams=config.num_beams,\n                length_penalty=config.length_penalty,\n                no_repeat_ngram_size=config.no_repeat_ngram_size,\n                early_stopping=True\n            )\n        \n        pred_summaries = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        \n        for ref, pred in zip(ref_summaries, pred_summaries):\n            if print_count < 3:\n                print(f\"\\n[Case {print_count + 1}]\")\n                print(f\"Ref : {ref}\")\n                print(f\"Pred: {pred}\")\n                print(\"-\" * 30)\n                print_count += 1\n            \n            rouge_scores = calculate_rouge(ref, pred)\n            if rouge_scores:\n                rouge1_scores.append(rouge_scores['rouge1'])\n                rouge2_scores.append(rouge_scores['rouge2'])\n                rougeL_scores.append(rouge_scores['rougeL'])\n            \n            samples_processed += 1\n            if samples_processed >= num_samples:\n                break\n    \n    eval_model.train()\n    \n    if len(rouge1_scores) > 0:\n        return {\n            'rouge1': np.mean(rouge1_scores),\n            'rouge2': np.mean(rouge2_scores),\n            'rougeL': np.mean(rougeL_scores)\n        }\n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:30:40.096477Z","iopub.execute_input":"2025-12-07T13:30:40.096665Z","iopub.status.idle":"2025-12-07T13:30:40.116289Z","shell.execute_reply.started":"2025-12-07T13:30:40.096642Z","shell.execute_reply":"2025-12-07T13:30:40.115695Z"}},"outputs":[],"execution_count":7},{"id":"1a920305-2faa-4ce1-add5-f1ddc9856dd1","cell_type":"markdown","source":"# 6. è®­ç»ƒä¸»æµç¨‹ (Train Loop)","metadata":{}},{"id":"bc67cfe3-1fdc-4064-a9a9-e88a774812a5","cell_type":"code","source":"# =============================================================================\n# 6. è®­ç»ƒä¸»æµç¨‹ (Train Loop)\n# =============================================================================\n\ndef train(model_load_path):\n    \"\"\"è®­ç»ƒä¸»æµç¨‹ï¼ˆé›†æˆæ‰€æœ‰é«˜çº§ç­–ç•¥ï¼‰\"\"\"\n    print(\"=\" * 60)\n    print(\"STEP 2: æ¨¡å‹è®­ç»ƒ (Training)\")\n    print(\"=\" * 60)\n    \n    torch.manual_seed(42)\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    \n    os.makedirs(config.out_dir, exist_ok=True)\n    \n    device_type = 'cuda' if 'cuda' in config.device else 'cpu'\n    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[config.dtype]\n    ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n    \n    print(f\"ä½¿ç”¨ç²¾åº¦: {config.dtype} ({ptdtype})\")\n    \n    # --- åŠ è½½ Tokenizer ---\n    print(f\"\\nåŠ è½½ Tokenizer from: {model_load_path}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_load_path)\n    \n    # --- æ„å»º DataLoader ---\n    print(\"\\næ„å»º DataLoader...\")\n    train_path = os.path.join(config.dataset_path, config.train_clean_csv)\n    val_path = os.path.join(config.dataset_path, config.val_clean_csv)\n    \n    train_dataset = SummarizationDataset(train_path, tokenizer, config.max_source_length, config.max_target_length, mode='train')\n    val_dataset = SummarizationDataset(val_path, tokenizer, config.max_source_length, config.max_target_length, mode='val')\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config.batch_size,\n        shuffle=True,\n        num_workers=config.num_workers,\n        pin_memory=True,\n        persistent_workers=config.persistent_workers if config.num_workers > 0 else False,\n        drop_last=True if config.use_rdrop else False\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config.batch_size,\n        shuffle=False,\n        num_workers=config.num_workers,\n        pin_memory=True,\n        persistent_workers=config.persistent_workers if config.num_workers > 0 else False\n    )\n    \n    dataloader_dict = {'train': train_loader, 'val': val_loader}\n    \n    # --- æ¨¡å‹åˆå§‹åŒ– ---\n    print(f\"\\nåŠ è½½ Model from: {model_load_path}\")\n    print(f\"é…ç½®æ¨¡å‹ (Dropout={config.dropout_rate}, Att={config.attention_dropout}, Act={config.activation_dropout})...\")\n    \n    # é…ç½®æ¨¡å‹\n    model_config = BartConfig.from_pretrained(model_load_path)\n    model_config.dropout = config.dropout_rate\n    model_config.attention_dropout = config.attention_dropout\n    model_config.activation_dropout = config.activation_dropout\n    model_config.classif_dropout = 0.0\n    model_config.decoder_layers = config.decoder_layers\n    \n    model = BartForConditionalGeneration.from_pretrained(model_load_path, config=model_config)\n    \n    # å‰ªædecoderå±‚\n    if len(model.model.decoder.layers) > config.decoder_layers:\n        print(f\"æ‰§è¡Œæ¨¡å‹å‰ªæ: {len(model.model.decoder.layers)} -> {config.decoder_layers} å±‚\")\n        model.model.decoder.layers = model.model.decoder.layers[:config.decoder_layers]\n    \n    num_params = sum(p.numel() for p in model.parameters())\n    print(f\"\\n>>> æ¨¡å‹å‚æ•°é‡: {num_params / 1e6:.2f} M <<<\")\n    \n    model.to(config.device)\n    \n    # ä¿å­˜åŸå§‹æ¨¡å‹å¼•ç”¨ï¼ˆç”¨äºä¿å­˜checkpointï¼‰\n    raw_model = model\n    \n    # å¤šGPUï¼ˆå¿…é¡»åœ¨ torch.compile ä¹‹å‰ï¼‰\n    if torch.cuda.device_count() > 1 and config.use_multi_gpu:\n        print(f\"\\nä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPUè®­ç»ƒ\")\n        model = nn.DataParallel(model)\n        raw_model = model.module\n        # å¤šGPUæ—¶ç¦ç”¨compileï¼Œå› ä¸ºäºŒè€…ä¸å…¼å®¹\n        config.compile = False\n    \n    # torch.compile ä¼˜åŒ–ï¼ˆä»…å•GPUæ—¶ä½¿ç”¨ï¼‰\n    if config.compile and hasattr(torch, 'compile'):\n        print(\"\\næ­£åœ¨ç¼–è¯‘æ¨¡å‹ (torch.compile)...\")\n        try:\n            model = torch.compile(model)\n            print(\"âœ… æ¨¡å‹ç¼–è¯‘æˆåŠŸï¼é¢„æœŸåŠ é€Ÿ 15-30%\")\n        except Exception as e:\n            print(f\"âš ï¸  ç¼–è¯‘å¤±è´¥ï¼ˆå°†ç»§ç»­ä½¿ç”¨éç¼–è¯‘ç‰ˆæœ¬ï¼‰: {e}\")\n    \n    # --- ä¼˜åŒ–å™¨å‚æ•°åˆ†ç»„ ---\n    print(\"\\næ„å»ºä¼˜åŒ–å™¨ï¼ˆå‚æ•°åˆ†ç»„ï¼‰...\")\n    optimizer_grouped_parameters = get_grouped_parameters(raw_model, config.weight_decay)\n    optimizer = torch.optim.AdamW(\n        optimizer_grouped_parameters,\n        lr=config.learning_rate,\n        betas=(config.beta1, config.beta2)\n    )\n    \n    # ä½¿ç”¨ transformers åŸç”Ÿè°ƒåº¦å™¨\n    print(f\"æ„å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ ({config.lr_scheduler_type})...\")\n    lr_scheduler = get_scheduler(\n        name=config.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=config.warmup_iters,\n        num_training_steps=config.max_iters\n    )\n    \n    # Scalerï¼ˆä¿®å¤åºŸå¼ƒè­¦å‘Šï¼‰\n    scaler = torch.amp.GradScaler('cuda', enabled=(config.dtype == 'float16'))\n    \n    # --- è®­ç»ƒå¾ªç¯ ---\n    print(\"\\nå¼€å§‹è®­ç»ƒ...\")\n    iter_num = 0\n    best_val_loss = 1e9\n    best_rouge_sum = 0.0\n    patience_counter = 0\n    t0 = time.time()\n    \n    progress_bar = tqdm(total=config.max_iters, desc=\"Training\", ncols=120)\n    \n    # æ— é™è¿­ä»£å™¨\n    def infinite_loader(loader):\n        while True:\n            for batch in loader:\n                yield batch\n    \n    train_iter = infinite_loader(train_loader)\n    \n    while iter_num < config.max_iters:\n        # --- è¯„ä¼° ---\n        if iter_num > 0 and iter_num % config.eval_interval == 0:\n            losses = estimate_loss(model, dataloader_dict, ctx)\n            print(f\"\\n[Step {iter_num}] train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n            \n            rouge_scores = None\n            current_rouge_sum = 0.0\n            \n            if config.eval_rouge_during_training:\n                rouge_scores = evaluate_rouge(model, tokenizer, val_loader, ctx, num_samples=config.rouge_eval_samples)\n                if rouge_scores:\n                    current_rouge_sum = rouge_scores['rouge1'] + rouge_scores['rouge2'] + rouge_scores['rougeL']\n                    print(f\"  ROUGE-1: {rouge_scores['rouge1']:.4f} | ROUGE-2: {rouge_scores['rouge2']:.4f} | ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n                    print(f\"  å¹³å‡åˆ†: {current_rouge_sum / 3:.4f}\")\n            \n            # ä¿å­˜ç­–ç•¥\n            saved = False\n            if rouge_scores and current_rouge_sum > best_rouge_sum:\n                best_rouge_sum = current_rouge_sum\n                patience_counter = 0\n                print(f\"  >>> æ–°çš„æœ€ä½³æ¨¡å‹ (ROUGE Sum: {current_rouge_sum:.4f})! ä¿å­˜ä¸­... <<<\")\n                saved = True\n            elif losses['val'] < best_val_loss and not rouge_scores:\n                best_val_loss = losses['val']\n                patience_counter = 0\n                print(f\"  æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}ã€‚ä¿å­˜checkpointã€‚\")\n                saved = True\n            else:\n                patience_counter += 1\n                print(f\"  è€å¿ƒè®¡æ•°: {patience_counter}/{config.early_stopping_patience}\")\n            \n            if saved:\n                checkpoint = {\n                    'model': raw_model.state_dict(),\n                    'optimizer': optimizer.state_dict(),\n                    'lr_scheduler': lr_scheduler.state_dict(),\n                    'iter_num': iter_num,\n                    'best_val_loss': best_val_loss,\n                    'best_rouge_sum': best_rouge_sum,\n                    'config': vars(config),\n                }\n                torch.save(checkpoint, os.path.join(config.out_dir, 'ckpt.pt'))\n                raw_model.save_pretrained(config.out_dir)\n                tokenizer.save_pretrained(config.out_dir)\n            \n            if config.use_early_stopping and patience_counter >= config.early_stopping_patience:\n                print(f\"æ—©åœè§¦å‘ã€‚\")\n                break\n            \n            print(\"-\" * 50)\n        \n        # --- Forward + Backward ---\n        model.train()\n        \n        batch = next(train_iter)\n        # è¿‡æ»¤æ‰étensorçš„é”®ï¼ˆå¦‚'id'ï¼‰\n        batch = {k: v.to(config.device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n        \n        with ctx:\n            if config.use_rdrop:\n                # R-Drop: ä¸¤æ¬¡å‰å‘ä¼ æ’­\n                outputs1 = model(**batch)\n                outputs2 = model(**batch)\n                \n                logits1 = outputs1.logits\n                logits2 = outputs2.logits\n                \n                # äº¤å‰ç†µæŸå¤±ï¼ˆä½¿ç”¨æ¨¡å‹è‡ªå¸¦çš„lossï¼‰\n                ce_loss = (outputs1.loss + outputs2.loss) / 2\n                \n                # KLæ•£åº¦æŸå¤±\n                kl_loss = compute_kl_loss(logits1, logits2)\n                \n                loss = ce_loss + config.rdrop_alpha * kl_loss\n            else:\n                outputs = model(**batch)\n                loss = outputs.loss\n            \n            # å¤šGPUæ—¶éœ€è¦mean\n            if hasattr(model, 'module'):\n                loss = loss.mean()\n            \n            loss = loss / config.gradient_accumulation_steps\n        \n        # Backward\n        if config.dtype == 'bfloat16':\n            loss.backward()\n        else:\n            scaler.scale(loss).backward()\n        \n        # æ¢¯åº¦ç´¯ç§¯ä¸æ›´æ–°\n        if (iter_num + 1) % config.gradient_accumulation_steps == 0:\n            if config.dtype == 'bfloat16':\n                torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n                optimizer.step()\n            else:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n                scaler.step(optimizer)\n                scaler.update()\n            \n            lr_scheduler.step()\n            optimizer.zero_grad(set_to_none=True)\n        \n        # æ—¥å¿—\n        if iter_num % config.log_interval == 0:\n            t1 = time.time()\n            lossf = loss.item() * config.gradient_accumulation_steps\n            current_lr = lr_scheduler.get_last_lr()[0]\n            dt = (t1 - t0) * 1000 / max(config.log_interval, 1)\n            progress_bar.set_postfix(loss=f\"{lossf:.4f}\", lr=f\"{current_lr:.2e}\", ms=f\"{dt:.2f}\")\n            t0 = t1\n        \n        iter_num += 1\n        progress_bar.update(1)\n    \n    # è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹\n    checkpoint = {\n        'model': raw_model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'lr_scheduler': lr_scheduler.state_dict(),\n        'iter_num': iter_num,\n        'best_val_loss': best_val_loss,\n        'best_rouge_sum': best_rouge_sum,\n        'config': vars(config),\n    }\n    torch.save(checkpoint, os.path.join(config.out_dir, 'ckpt.pt'))\n    raw_model.save_pretrained(config.out_dir)\n    tokenizer.save_pretrained(config.out_dir)\n    \n    progress_bar.close()\n    print(\"\\nè®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ã€‚\")\n    return config.out_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:30:40.116997Z","iopub.execute_input":"2025-12-07T13:30:40.117235Z","iopub.status.idle":"2025-12-07T13:30:40.141525Z","shell.execute_reply.started":"2025-12-07T13:30:40.117212Z","shell.execute_reply":"2025-12-07T13:30:40.140843Z"}},"outputs":[],"execution_count":8},{"id":"fddb4837-c923-4e19-8c1d-f687ee1fa603","cell_type":"markdown","source":"# 7. æ¨ç†ä¸æäº¤ (Generate Submission)","metadata":{}},{"id":"dafbe6fc-7d3a-4503-a13a-f659d353fc2e","cell_type":"code","source":"# =============================================================================\n# 7. æ¨ç†ä¸æäº¤ (Generate Submission)\n# =============================================================================\n\ndef generate_submission(model_path):\n    \"\"\"ç”Ÿæˆæµ‹è¯•é›†æäº¤æ–‡ä»¶\"\"\"\n    print(\"=\" * 60)\n    print(\"STEP 3: ç”Ÿæˆæäº¤æ–‡ä»¶ (Inference)\")\n    print(\"=\" * 60)\n    \n    if not os.path.exists(config.input_test):\n        print(\"âŒ æœªæ‰¾åˆ°æµ‹è¯•é›†ï¼Œæ— æ³•ç”Ÿæˆæäº¤ã€‚\")\n        return\n\n    print(f\"åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†: {model_path}\")\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = BartForConditionalGeneration.from_pretrained(model_path)\n        n_params = sum(p.numel() for p in model.parameters())\n        param_display = f\"{n_params/1e6:.2f} M\"\n        \n        \n        n_params = sum(p.numel() for p in model.parameters())\n        val_m = n_params/1e6\n        \n        # å®šä¹‰é¢œè‰²ä»£ç \n        C_BORDER = \"\\033[1;36m\"  # äº®é’è‰²è¾¹æ¡†\n        C_TITLE  = \"\\033[1;33m\"  # äº®é»„è‰²æ ‡é¢˜\n        C_PASS   = \"\\033[1;32m\"  # äº®ç»¿è‰²é€šè¿‡\n        C_FAIL   = \"\\033[1;31m\"  # äº®çº¢è‰²å¤±è´¥\n        C_RESET  = \"\\033[0m\"     # é‡ç½®\n        \n        # æ„é€ ä¿¡æ¯å­—ç¬¦ä¸²\n        if val_m < 400:\n            status_line = f\"{C_PASS}PASS: {val_m:.2f} M < 400 M{C_RESET}\"\n            \n            padding_len = 52 - len(f\"PASS: {val_m:.2f} M < 400 M\")\n        else:\n            status_line = f\"{C_FAIL}FAIL: {val_m:.2f} M > 400 M{C_RESET}\"\n            padding_len = 52 - len(f\"FAIL: {val_m:.2f} M > 400 M\")\n        \n        padding = \" \" * max(0, padding_len)\n\n        # æ‰“å°å…¨å½© ASCII æ¡†\n        print(\"\\n\" * 3)  # åˆ¶é€ å‚ç›´éš”ç¦»åŒº\n        print(f\"{C_BORDER}{'='*60}{C_RESET}\")\n        print(f\"{C_BORDER}!!{C_RESET}{' '*56}{C_BORDER}!!{C_RESET}\")\n        print(f\"{C_BORDER}!!{C_RESET}   {C_TITLE}>>> åŠ©æ•™è¯·çœ‹è¿™é‡Œ (TA CHECKPOINT) <<<{C_RESET}   {C_BORDER}!!{C_RESET}\")\n        print(f\"{C_BORDER}!!{C_RESET}{' '*56}{C_BORDER}!!{C_RESET}\")\n        print(f\"{C_BORDER}!!{C_RESET}   {status_line}{padding}{C_BORDER}!!{C_RESET}\")\n        print(f\"{C_BORDER}!!{C_RESET}{' '*56}{C_BORDER}!!{C_RESET}\")\n        print(f\"{C_BORDER}{'='*60}{C_RESET}\")\n        print(\"\\n\" * 2) \n\n        # åŸå§‹è¦æ±‚ä»£ç \n        assert n_params/1e6 < 400, f\"å‚æ•°é‡è¶…æ ‡: {val_m:.2f}M > 400M\"\n        \n    except Exception as e:\n        print(f\"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}\")\n        return\n\n    model.to(config.device)\n    model.eval()\n    \n    test_df = pd.read_csv(config.input_test)\n    # åˆ—åé€‚é…\n    col_map = {}\n    for col in test_df.columns:\n        if col.lower() in ['text', 'document', 'content']: \n            col_map[col] = 'dialogue'\n    if col_map: \n        test_df.rename(columns=col_map, inplace=True)\n    \n    test_df['dialogue'] = test_df['dialogue'].apply(clean_text_remove_emoji).fillna(\"\")\n    print(f\"æ¨ç†æ•°æ®é‡: {len(test_df)}\")\n    print(\"ä¸‹é¢çš„è¿›åº¦æ¡ä¸æ˜¯x/2273ï¼Œè¿™æ˜¯å› ä¸ºæˆ‘æ˜¯æ‰¹é‡æ¨ç†çš„ï¼Œ/å³è¾¹çš„æ˜¯æ‰¹æ¬¡æ•°é‡\")\n    results = []\n    batch_size = 64\n    dialogues = test_df['dialogue'].tolist()\n    \n    device_type = 'cuda' if 'cuda' in config.device else 'cpu'\n    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[config.dtype]\n    ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n    \n    with torch.no_grad():\n        for i in tqdm(range(0, len(dialogues), batch_size), desc=\"Generating\"):\n            batch = dialogues[i:i+batch_size]\n            inputs = tokenizer(\n                batch, \n                max_length=config.max_source_length, \n                padding='max_length', \n                truncation=True, \n                return_tensors='pt'\n            ).to(config.device)\n            \n            with ctx:\n                outputs = model.generate(\n                    inputs['input_ids'], \n                    attention_mask=inputs['attention_mask'],\n                    num_beams=config.num_beams, \n                    max_length=config.max_target_length, \n                    min_length=config.min_length,\n                    length_penalty=config.length_penalty, \n                    no_repeat_ngram_size=config.no_repeat_ngram_size, \n                    early_stopping=True\n                )\n            \n            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n            results.extend([s.strip() for s in decoded])\n            \n    submission_path = '/kaggle/working/submission.csv'\n    pd.DataFrame({'id': test_df['id'], 'summary': results}).to_csv(submission_path, index=False)\n    print(f\"âœ… Submission å·²ä¿å­˜: {submission_path}\")\n    print(\"é¢„è§ˆå‰5è¡Œï¼š\")\n    print(pd.DataFrame({'id': test_df['id'][:5], 'summary': results[:5]}))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:30:40.142286Z","iopub.execute_input":"2025-12-07T13:30:40.142610Z","iopub.status.idle":"2025-12-07T13:30:40.162235Z","shell.execute_reply.started":"2025-12-07T13:30:40.142588Z","shell.execute_reply":"2025-12-07T13:30:40.161558Z"}},"outputs":[],"execution_count":9},{"id":"75eafc1b-db5f-42aa-b441-0740dcafbe01","cell_type":"code","source":"import random\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # ä¿è¯ CUDA å·ç§¯æ“ä½œçš„ç¡®å®šæ€§ (ä¼šç¨å¾®ç‰ºç‰²ä¸€ç‚¹ç‚¹é€Ÿåº¦ï¼Œä½†ä¿è¯ç»“æœä¸€è‡´)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f\"ğŸ”’ å…¨å±€éšæœºç§å­å·²å›ºå®šä¸º: {seed}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:30:40.162925Z","iopub.execute_input":"2025-12-07T13:30:40.163136Z","iopub.status.idle":"2025-12-07T13:30:40.179142Z","shell.execute_reply.started":"2025-12-07T13:30:40.163122Z","shell.execute_reply":"2025-12-07T13:30:40.178442Z"}},"outputs":[],"execution_count":10},{"id":"d93e2e9b-d8bc-414f-9b1b-2f47399729a9","cell_type":"markdown","source":"# 8. ä¸»ç¨‹åºå…¥å£","metadata":{}},{"id":"8e6fce80-d34b-4116-ae04-3a9597feaf5c","cell_type":"code","source":"# =============================================================================\n# 8. ä¸»ç¨‹åºå…¥å£\n# =============================================================================\n\nif __name__ == '__main__':\n    # 1. å°è¯•ä» HF ä¸‹è½½å·²å¾®è°ƒçš„æƒé‡\n    print(\"ä» HF ä¸‹è½½å·²å¾®è°ƒçš„æƒé‡ï¼Œé¢„è®¡30ç§’\")\n    hf_model_path = download_model_from_hf()\n    \n    # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ¨¡å‹\n    if hf_model_path is None:\n        print(\"âš ï¸ æ— æ³•ä» HF ä¸‹è½½ï¼Œå°†ä½¿ç”¨ facebook/bart-large åŸå§‹æƒé‡\")\n        current_model_path = config.base_model_name\n    else:\n        current_model_path = hf_model_path\n        \n    seed_everything()\n    print(\"éšæœºç§å­å·²å›ºå®š\")\n    # 2. æ ¹æ®å¼€å…³å†³å®šæ˜¯ ä»…æ¨ç† è¿˜æ˜¯ è®­ç»ƒ+æ¨ç†\n    if config.ONLY_INFERENCE:\n        print(\"\\n>>> æ¨¡å¼: ä»…æ¨ç† (ONLY_INFERENCE = True)\")\n        generate_submission(current_model_path)\n    \n    else:\n        print(\"\\n>>> æ¨¡å¼: è®­ç»ƒ + æ¨ç† (ONLY_INFERENCE = False)\")\n        # å‡†å¤‡æ•°æ®\n        prepare_data_pipeline()\n        \n        # è®­ç»ƒ\n        trained_model_path = train(current_model_path)\n        \n        # ä½¿ç”¨åˆšè®­ç»ƒå¥½çš„æƒé‡è¿›è¡Œæ¨ç†\n        generate_submission(trained_model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:30:40.181397Z","iopub.execute_input":"2025-12-07T13:30:40.181625Z"}},"outputs":[{"name":"stdout","text":"ä» HF ä¸‹è½½å·²å¾®è°ƒçš„æƒé‡ï¼Œé¢„è®¡30ç§’\n\n>>> æ­£åœ¨ä» HuggingFace ä¸‹è½½æ¨¡å‹æƒé‡...\n    Repo: Kevin36277/finetuned-model\n    Folder: out-new-v1\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5198a8c3620a4f7fb56bdde7e5cf5d15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f54cf1b05314323aa0d1a15f71fe6b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"submission.csv-checkpoint.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c994950ec1844e54b6bc0db67afa9231"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/292 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aa69ae30f1b4af2b3b2507e8666fd04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f6869e6429e431dbc000efb4b054282"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"submission.csv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f7e216866844b17bb914d1b771f83a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"submission-checkpoint.csv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7add25c403b4f03b6c017c025bc86f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71bb3b4f73d04607b5a109f9b8038e99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"submission.csv.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89bf7dd02166449f8c25cf843c175691"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"out-new-v1/model.safetensors:   0%|          | 0.00/1.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cf35d8da861404f822849a6b36a28d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed5d84acac284f808a80ee7a5419a6b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59c9cb9f70974c3a827278f9098f62e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fcc83d9ce754aad9d47e1253956b1c5"}},"metadata":{}},{"name":"stdout","text":"    âœ… ä¸‹è½½å®Œæˆã€‚æ¨¡å‹ä½äº: /kaggle/working/downloaded_model/out-new-v1\nğŸ”’ å…¨å±€éšæœºç§å­å·²å›ºå®šä¸º: 42\néšæœºç§å­å·²å›ºå®š\n\n>>> æ¨¡å¼: ä»…æ¨ç† (ONLY_INFERENCE = True)\n============================================================\nSTEP 3: ç”Ÿæˆæäº¤æ–‡ä»¶ (Inference)\n============================================================\nåŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†: /kaggle/working/downloaded_model/out-new-v1\n\n\n\n\n\u001b[1;36m============================================================\u001b[0m\n\u001b[1;36m!!\u001b[0m                                                        \u001b[1;36m!!\u001b[0m\n\u001b[1;36m!!\u001b[0m   \u001b[1;33m>>> åŠ©æ•™è¯·çœ‹è¿™é‡Œ (TA CHECKPOINT) <<<\u001b[0m   \u001b[1;36m!!\u001b[0m\n\u001b[1;36m!!\u001b[0m                                                        \u001b[1;36m!!\u001b[0m\n\u001b[1;36m!!\u001b[0m   \u001b[1;32mPASS: 389.49 M < 400 M\u001b[0m                              \u001b[1;36m!!\u001b[0m\n\u001b[1;36m!!\u001b[0m                                                        \u001b[1;36m!!\u001b[0m\n\u001b[1;36m============================================================\u001b[0m\n\n\n\næ¨ç†æ•°æ®é‡: 2273\nä¸‹é¢çš„è¿›åº¦æ¡ä¸æ˜¯x/2273ï¼Œè¿™æ˜¯å› ä¸ºæˆ‘æ˜¯æ‰¹é‡æ¨ç†çš„ï¼Œ/å³è¾¹çš„æ˜¯æ‰¹æ¬¡æ•°é‡\n","output_type":"stream"},{"name":"stderr","text":"Generating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/36 [07:36<07:25, 22.27s/it]","output_type":"stream"}],"execution_count":null}]}