{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d650f98",
   "metadata": {
    "papermill": {
     "duration": 0.006923,
     "end_time": "2025-11-12T07:25:00.875226",
     "exception": false,
     "start_time": "2025-11-12T07:25:00.868303",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# 处理数据\n",
    "右侧点击Add Input，找到我们的比赛，然后添加\n",
    "\n",
    "从Kaggle输入的train.csv读取数据，随机划分为训练集(95%)和验证集(5%)\n",
    "\n",
    "保存到/kaggle/working/data/samsum目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0595bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:25:00.887440Z",
     "iopub.status.busy": "2025-11-12T07:25:00.887186Z",
     "iopub.status.idle": "2025-11-12T07:25:09.330299Z",
     "shell.execute_reply": "2025-11-12T07:25:09.329358Z"
    },
    "papermill": {
     "duration": 8.450689,
     "end_time": "2025-11-12T07:25:09.331670",
     "exception": false,
     "start_time": "2025-11-12T07:25:00.880981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\r\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\r\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\r\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.3.0)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2025.9.18)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge-score) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge-score) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge-score) (2024.2.0)\r\n",
      "Building wheels for collected packages: rouge-score\r\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=ce9eadb94e58afad861a4b0124f500fb35473e17f1f5c5601c06d10df5ad4781\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\r\n",
      "Successfully built rouge-score\r\n",
      "Installing collected packages: rouge-score\r\n",
      "Successfully installed rouge-score-0.1.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6750b02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:25:09.345182Z",
     "iopub.status.busy": "2025-11-12T07:25:09.344641Z",
     "iopub.status.idle": "2025-11-12T07:25:12.417686Z",
     "shell.execute_reply": "2025-11-12T07:25:12.416813Z"
    },
    "papermill": {
     "duration": 3.080845,
     "end_time": "2025-11-12T07:25:12.418863",
     "exception": false,
     "start_time": "2025-11-12T07:25:09.338018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "准备SAMSum数据集划分\n",
      "================================================================================\n",
      "\n",
      "读取数据: /kaggle/input/nanogpt-fudannlp-cs-30040/train.csv\n",
      "总样本数: 26141\n",
      "训练集样本数: 24834 (95.0%)\n",
      "验证集样本数: 1307 (5.0%)\n",
      "\n",
      "训练集已保存: /kaggle/working/data/samsum/train.csv\n",
      "验证集已保存: /kaggle/working/data/samsum/validation.csv\n",
      "\n",
      "数据集划分完成！\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import tiktoken\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"准备SAMSum数据集划分\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 读取原始CSV文件\n",
    "input_csv = '/kaggle/input/nanogpt-fudannlp-cs-30040/train.csv'\n",
    "print(f\"\\n读取数据: {input_csv}\")\n",
    "\n",
    "df = pd.read_csv(input_csv)\n",
    "total_samples = len(df)\n",
    "print(f\"总样本数: {total_samples}\")\n",
    "\n",
    "# 随机打乱数据\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 计算划分点（5%作为验证集）\n",
    "val_size = int(total_samples * 0.05)\n",
    "train_size = total_samples - val_size\n",
    "\n",
    "print(f\"训练集样本数: {train_size} ({train_size/total_samples*100:.1f}%)\")\n",
    "print(f\"验证集样本数: {val_size} ({val_size/total_samples*100:.1f}%)\")\n",
    "\n",
    "# 划分数据\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 创建输出目录\n",
    "output_dir = '/kaggle/working/data/samsum'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 保存训练集\n",
    "train_csv_path = os.path.join(output_dir, 'train.csv')\n",
    "train_df.to_csv(train_csv_path, index=False)\n",
    "print(f\"\\n训练集已保存: {train_csv_path}\")\n",
    "\n",
    "# 保存验证集\n",
    "val_csv_path = os.path.join(output_dir, 'validation.csv')\n",
    "val_df.to_csv(val_csv_path, index=False)\n",
    "print(f\"验证集已保存: {val_csv_path}\")\n",
    "\n",
    "print(\"\\n数据集划分完成！\")\n",
    "print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f246d559",
   "metadata": {
    "papermill": {
     "duration": 0.005643,
     "end_time": "2025-11-12T07:25:12.430925",
     "exception": false,
     "start_time": "2025-11-12T07:25:12.425282",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "065c4ef1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-12T07:25:12.443449Z",
     "iopub.status.busy": "2025-11-12T07:25:12.443207Z",
     "iopub.status.idle": "2025-11-12T07:25:18.874224Z",
     "shell.execute_reply": "2025-11-12T07:25:18.873584Z"
    },
    "papermill": {
     "duration": 6.438779,
     "end_time": "2025-11-12T07:25:18.875531",
     "exception": false,
     "start_time": "2025-11-12T07:25:12.436752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "===================================================================================\n",
    "GPT-2 文本摘要微调 - 完整教学脚本\n",
    "===================================================================================\n",
    "\n",
    "本脚本整合了完整的训练和评估流程，适合用于教学和学习。\n",
    "\n",
    "主要内容：\n",
    "1. GPT模型定义（完整的Transformer架构）\n",
    "2. 数据准备和加载\n",
    "3. 模型训练\n",
    "4. ROUGE评估\n",
    "5. 生成和测试\n",
    "\n",
    "学习建议：\n",
    "- 初学者：重点关注Config配置部分，了解各参数的作用\n",
    "- 进阶者：深入理解模型结构、训练循环和数据处理\n",
    "- 实践者：修改参数进行实验，观察结果变化\n",
    "\n",
    "===================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb29c7",
   "metadata": {
    "papermill": {
     "duration": 0.005737,
     "end_time": "2025-11-12T07:25:18.887518",
     "exception": false,
     "start_time": "2025-11-12T07:25:18.881781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CausalSelfAttentio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "693653b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:25:18.900700Z",
     "iopub.status.busy": "2025-11-12T07:25:18.900358Z",
     "iopub.status.idle": "2025-11-12T07:25:18.910056Z",
     "shell.execute_reply": "2025-11-12T07:25:18.909334Z"
    },
    "papermill": {
     "duration": 0.017658,
     "end_time": "2025-11-12T07:25:18.911233",
     "exception": false,
     "start_time": "2025-11-12T07:25:18.893575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x, past_kv=None, use_cache=False):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # 如果有past_kv，则拼接\n",
    "        if past_kv is not None:\n",
    "            past_k, past_v = past_kv\n",
    "            k = torch.cat([past_k, k], dim=2)  # 在序列维度拼接\n",
    "            v = torch.cat([past_v, v], dim=2)\n",
    "        \n",
    "        # 如果需要cache，保存当前的k, v\n",
    "        present_kv = (k, v) if use_cache else None\n",
    "        \n",
    "        # 更新T为完整的序列长度\n",
    "        T_full = k.size(2)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T_full] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y, present_kv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9cc4da",
   "metadata": {
    "papermill": {
     "duration": 0.005799,
     "end_time": "2025-11-12T07:25:18.923312",
     "exception": false,
     "start_time": "2025-11-12T07:25:18.917513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f781b21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:25:18.936407Z",
     "iopub.status.busy": "2025-11-12T07:25:18.935958Z",
     "iopub.status.idle": "2025-11-12T07:25:18.942299Z",
     "shell.execute_reply": "2025-11-12T07:25:18.941596Z"
    },
    "papermill": {
     "duration": 0.014322,
     "end_time": "2025-11-12T07:25:18.943391",
     "exception": false,
     "start_time": "2025-11-12T07:25:18.929069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, past_kv=None, use_cache=False):\n",
    "        attn_out, present_kv = self.attn(self.ln_1(x), past_kv=past_kv, use_cache=use_cache)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x, present_kv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126033bf",
   "metadata": {
    "papermill": {
     "duration": 0.005609,
     "end_time": "2025-11-12T07:25:18.955008",
     "exception": false,
     "start_time": "2025-11-12T07:25:18.949399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5e1e187",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:25:18.968635Z",
     "iopub.status.busy": "2025-11-12T07:25:18.967990Z",
     "iopub.status.idle": "2025-11-12T07:25:18.999949Z",
     "shell.execute_reply": "2025-11-12T07:25:18.999216Z"
    },
    "papermill": {
     "duration": 0.040142,
     "end_time": "2025-11-12T07:25:19.001012",
     "exception": false,
     "start_time": "2025-11-12T07:25:18.960870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None, past_key_values=None, use_cache=False):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        \n",
    "        # 计算position IDs\n",
    "        if past_key_values is None:\n",
    "            pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "        else:\n",
    "            # 如果使用cache，position从past的长度开始\n",
    "            past_length = past_key_values[0][0].size(2) if past_key_values is not None else 0\n",
    "            pos = torch.arange(past_length, past_length + t, dtype=torch.long, device=device)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        # 通过所有block，收集KV cache\n",
    "        present_key_values = [] if use_cache else None\n",
    "        for i, block in enumerate(self.transformer.h):\n",
    "            past_kv = past_key_values[i] if past_key_values is not None else None\n",
    "            x, present_kv = block(x, past_kv=past_kv, use_cache=use_cache)\n",
    "            if use_cache:\n",
    "                present_key_values.append(present_kv)\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        if use_cache:\n",
    "            return logits, loss, present_key_values\n",
    "        return logits, loss\n",
    "\n",
    "    def crop_block_size(self, block_size):\n",
    "        # model surgery to decrease the block size if necessary\n",
    "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
    "        # but want to use a smaller block size for some smaller, simpler model\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, override_args=None):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        override_args = override_args or {} # default to empty dict\n",
    "        # only dropout can be overridden see more notes below\n",
    "        assert all(k == 'dropout' for k in override_args)\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        config_args['bias'] = True # always True for GPT model checkpoints\n",
    "        # we can override the dropout rate, if desired\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
    "        # first estimate the number of flops we do per iteration.\n",
    "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = self.get_num_params()\n",
    "        cfg = self.config\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
    "        flops_promised = 106e9 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, eos_token_id=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \n",
    "        Args:\n",
    "            idx: 输入序列 (b, t)\n",
    "            max_new_tokens: 最大生成token数\n",
    "            temperature: 采样温度\n",
    "            top_k: top-k采样\n",
    "            eos_token_id: 结束token ID，遇到时提前停止（可以是单个ID或ID列表）\n",
    "        \"\"\"\n",
    "        # 将eos_token_id转换为列表\n",
    "        if eos_token_id is not None:\n",
    "            if isinstance(eos_token_id, int):\n",
    "                eos_token_id = [eos_token_id]\n",
    "            eos_token_id = set(eos_token_id)\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "            \n",
    "            # 检查是否生成了结束token\n",
    "            if eos_token_id is not None:\n",
    "                # 检查batch中所有序列是否都遇到了结束token\n",
    "                if idx_next[0, 0].item() in eos_token_id:\n",
    "                    break\n",
    "\n",
    "        return idx\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_with_kv_cache(self, idx, max_new_tokens, temperature=1.0, top_k=None, eos_token_id=None):\n",
    "        \"\"\"\n",
    "        使用KV cache加速的生成方法\n",
    "        \n",
    "        KV cache原理：\n",
    "        - 在自回归生成中，每一步只需要计算新token的attention\n",
    "        - 之前token的key和value可以缓存，避免重复计算\n",
    "        - 这样可以显著加速生成过程（特别是长序列）\n",
    "        \n",
    "        Args:\n",
    "            idx: 输入序列 (b, t)\n",
    "            max_new_tokens: 最大生成token数\n",
    "            temperature: 采样温度\n",
    "            top_k: top-k采样\n",
    "            eos_token_id: 结束token ID，遇到时提前停止（可以是单个ID或ID列表）\n",
    "        \"\"\"\n",
    "        # 将eos_token_id转换为集合\n",
    "        if eos_token_id is not None:\n",
    "            if isinstance(eos_token_id, int):\n",
    "                eos_token_id = [eos_token_id]\n",
    "            eos_token_id = set(eos_token_id)\n",
    "        \n",
    "        # 第一步：处理整个prompt，获取初始的KV cache\n",
    "        idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "        logits, _, past_key_values = self(idx_cond, use_cache=True)\n",
    "        \n",
    "        # 对第一个token进行采样\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        # 检查是否立即遇到结束token\n",
    "        if eos_token_id is not None and idx_next[0, 0].item() in eos_token_id:\n",
    "            return idx\n",
    "        \n",
    "        # 后续生成步骤：每次只处理一个新token，使用KV cache\n",
    "        for _ in range(max_new_tokens - 1):\n",
    "            # 只输入最后一个token，使用past_key_values\n",
    "            logits, _, past_key_values = self(\n",
    "                idx[:, [-1]], \n",
    "                past_key_values=past_key_values, \n",
    "                use_cache=True\n",
    "            )\n",
    "            \n",
    "            # 采样\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # 拼接新token\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "            \n",
    "            # 提前停止检查\n",
    "            if eos_token_id is not None and idx_next[0, 0].item() in eos_token_id:\n",
    "                break\n",
    "        \n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ca258d",
   "metadata": {
    "papermill": {
     "duration": 0.00608,
     "end_time": "2025-11-12T07:25:19.013094",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.007014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62b39096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:25:19.025890Z",
     "iopub.status.busy": "2025-11-12T07:25:19.025655Z",
     "iopub.status.idle": "2025-11-12T07:25:19.115400Z",
     "shell.execute_reply": "2025-11-12T07:25:19.114552Z"
    },
    "papermill": {
     "duration": 0.097725,
     "end_time": "2025-11-12T07:25:19.116613",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.018888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import csv\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "\n",
    "# 导入模型定义\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 第一部分：配置参数\n",
    "# 这些参数可以由学生根据需要进行调整\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    配置类：包含所有可调参数\n",
    "    参数组织方式便于学生理解，参数值与nanoGPT原始配置保持一致\n",
    "    \"\"\"\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 数据集配置\n",
    "    # =========================================================================\n",
    "    dataset_path = '/kaggle/working/data/samsum'  # 原始数据集路径\n",
    "    dataset = 'samsum'          # 数据集名称（处理后的数据会保存在data/{dataset}/目录）\n",
    "    \n",
    "    # 特殊token定义（用于分隔对话和摘要）\n",
    "    dialogue_start = \"\\n\\n### DIALOGUE:\\n\"  # 对话开始标记\n",
    "    summary_start = \"\\n\\n### SUMMARY:\\n\"     # 摘要开始标记\n",
    "    summary_end = \"<|endoftext|>\"            # 摘要结束标记（GPT-2的EOS token）\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 训练配置（建议学生重点关注这部分）\n",
    "    # =========================================================================\n",
    "    # 模型初始化\n",
    "    init_from = 'gpt2'       # 'scratch'(从头训练) 或 'resume'(继续训练) 或 'gpt2'/'gpt2-xl'(从预训练模型微调)\n",
    "    \n",
    "    # 批次配置\n",
    "    batch_size = 8              # 每个GPU的批次大小（micro-batch size）\n",
    "    gradient_accumulation_steps = 16  # 梯度累积步数，有效批次 = batch_size * gradient_accumulation_steps\n",
    "    block_size = 1024           # 上下文窗口大小（最大序列长度）\n",
    "    \n",
    "    # 训练步数\n",
    "    max_iters = 500              # 总训练迭代次数\n",
    "    \n",
    "    # 优化器配置（AdamW）\n",
    "    learning_rate = 6e-5        # 学习率（微调时使用较小的学习率）\n",
    "    weight_decay = 1e-2         # 权重衰减系数\n",
    "    beta1 = 0.9                 # Adam的beta1参数\n",
    "    beta2 = 0.999                # Adam的beta2参数\n",
    "    grad_clip = 1.0             # 梯度裁剪阈值（0.0表示不裁剪）\n",
    "    \n",
    "    # 学习率调度\n",
    "    decay_lr = True#False            # 是否使用学习率衰减（微调时通常用常数学习率）\n",
    "    warmup_iters = 100         # 学习率预热步数\n",
    "    lr_decay_iters = 2000     # 学习率衰减的总步数\n",
    "    min_lr = 1e-5               # 最小学习率\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 模型配置（从头训练时需要设置，从预训练模型加载时会被覆盖）\n",
    "    # =========================================================================\n",
    "    n_layer = 12                # Transformer层数\n",
    "    n_head = 12                 # 注意力头数\n",
    "    n_embd = 768                # 嵌入维度\n",
    "    dropout = 0.1               # Dropout率（预训练0.0，微调可尝试0.1+）\n",
    "    bias = False                # LayerNorm和Linear层是否使用bias\n",
    "    \n",
    "    # =========================================================================\n",
    "    # I/O配置\n",
    "    # =========================================================================\n",
    "    out_dir = 'out-summarization' # checkpoint保存目录\n",
    "    eval_interval = 10           # 每多少步评估一次\n",
    "    log_interval = 5            # 每多少步打印日志\n",
    "    eval_iters = 40             # 评估时的迭代次数\n",
    "    eval_only = False           # 是否只评估不训练\n",
    "    always_save_checkpoint = True  # 是否每次评估都保存checkpoint（False表示只保存最佳模型）\n",
    "    \n",
    "    # ROUGE评估配置（训练过程中）\n",
    "    eval_rouge_during_training = True  # 是否在训练时评估ROUGE分数\n",
    "    rouge_eval_samples = 5      # 训练时ROUGE评估的样本数（较少避免太慢）\n",
    "    \n",
    "    # =========================================================================\n",
    "    # wandb日志配置（可选）\n",
    "    # =========================================================================\n",
    "    wandb_log = False           # 是否启用wandb日志\n",
    "    wandb_project = 'owt'       # wandb项目名\n",
    "    wandb_run_name = 'gpt2'     # wandb运行名称\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 系统配置\n",
    "    # =========================================================================\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'  # 训练设备\n",
    "    dtype = 'float16'#'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "    # dtype = 'float32'\n",
    "    compile = True#False             # 是否使用PyTorch 2.0编译（需要CUDA Capability >= 7.0，P100不支持）\n",
    "    backend = 'nccl'            # DDP后端（'nccl'用于GPU，'gloo'用于CPU）\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 测试/生成配置\n",
    "    # =========================================================================\n",
    "    num_test_samples = 10       # 测试时评估的样本数量\n",
    "    max_new_tokens = 100        # 生成时的最大token数\n",
    "    temperature = 0.7           # 生成温度（1.0=无变化，<1.0=更确定，>1.0=更随机）\n",
    "    top_k = 50                 # Top-K采样（保留概率最高的K个token）\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037d7d08",
   "metadata": {
    "papermill": {
     "duration": 0.005799,
     "end_time": "2025-11-12T07:25:19.128723",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.122924",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## def prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ccf9d29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:25:19.141669Z",
     "iopub.status.busy": "2025-11-12T07:25:19.141417Z",
     "iopub.status.idle": "2025-11-12T07:25:19.152418Z",
     "shell.execute_reply": "2025-11-12T07:25:19.151654Z"
    },
    "papermill": {
     "duration": 0.018947,
     "end_time": "2025-11-12T07:25:19.153476",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.134529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 第二部分：数据准备\n",
    "# 读取samsum数据集，格式化为训练格式，并进行tokenization\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_data():\n",
    "    \"\"\"\n",
    "    准备摘要数据集\n",
    "    \n",
    "    数据格式设计：\n",
    "    每条训练样本格式为：\n",
    "    \\n\\n### DIALOGUE:\\n{对话内容}\\n\\n### SUMMARY:\\n{摘要内容}<|endoftext|>\n",
    "    \n",
    "    重要：每个样本独立保存，不连接成长序列\n",
    "    \n",
    "    这样模型能学习到：\n",
    "    - 看到 DIALOGUE 标记后，理解后面是对话内容\n",
    "    - 看到 SUMMARY 标记后，开始生成摘要\n",
    "    - 看到 <|endoftext|> 表示摘要结束\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"准备数据集...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 创建数据目录\n",
    "    data_dir = os.path.join('data', config.dataset)\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # 初始化tokenizer\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    # 处理训练集和验证集\n",
    "    for split in ['train', 'validation']:\n",
    "        print(f\"\\n处理 {split} 数据集...\")\n",
    "        csv_file = os.path.join(config.dataset_path, f'{split}.csv')\n",
    "        \n",
    "        # 读取CSV文件\n",
    "        dialogues = []\n",
    "        summaries = []\n",
    "        with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                dialogues.append(row['dialogue'])\n",
    "                summaries.append(row['summary'])\n",
    "        \n",
    "        print(f\"  读取了 {len(dialogues)} 条数据\")\n",
    "        \n",
    "        # 格式化并tokenize每条数据，每个样本单独保存\n",
    "        samples = []  # 存储所有样本的token列表\n",
    "        valid_count = 0\n",
    "        skipped_count = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for dialogue, summary in zip(dialogues, summaries):\n",
    "            # 构建完整的训练样本\n",
    "            formatted_text = (\n",
    "                config.dialogue_start + dialogue +\n",
    "                config.summary_start + summary +\n",
    "                config.summary_end\n",
    "            )\n",
    "            \n",
    "            # Tokenize\n",
    "            tokens = enc.encode(formatted_text, allowed_special={config.summary_end})\n",
    "            \n",
    "            # 检查长度是否超过block_size\n",
    "            if len(tokens) <= config.block_size:\n",
    "                samples.append(tokens)\n",
    "                valid_count += 1\n",
    "                total_tokens += len(tokens)\n",
    "            else:\n",
    "                # 如果太长，进行截断（保留对话开始和摘要部分）\n",
    "                # 找到SUMMARY标记的位置\n",
    "                summary_tokens = enc.encode(config.summary_start, allowed_special={config.summary_end})\n",
    "                summary_pos = None\n",
    "                for i in range(len(tokens) - len(summary_tokens)):\n",
    "                    if tokens[i:i+len(summary_tokens)] == summary_tokens:\n",
    "                        summary_pos = i\n",
    "                        break\n",
    "                \n",
    "                if summary_pos and (len(tokens) - summary_pos) < config.block_size * 0.3:\n",
    "                    # 如果能找到摘要位置，且摘要部分不太长，则截断对话部分\n",
    "                    dialogue_tokens = enc.encode(config.dialogue_start, allowed_special={config.summary_end})\n",
    "                    available_space = config.block_size - (len(tokens) - summary_pos) - len(dialogue_tokens)\n",
    "                    \n",
    "                    if available_space > 0:\n",
    "                        # 截断对话内容\n",
    "                        truncated_tokens = (\n",
    "                            dialogue_tokens +\n",
    "                            tokens[len(dialogue_tokens):len(dialogue_tokens)+available_space] +\n",
    "                            tokens[summary_pos:]\n",
    "                        )\n",
    "                        samples.append(truncated_tokens)\n",
    "                        valid_count += 1\n",
    "                        total_tokens += len(truncated_tokens)\n",
    "                    else:\n",
    "                        skipped_count += 1\n",
    "                else:\n",
    "                    skipped_count += 1\n",
    "        \n",
    "        print(f\"  有效样本数: {valid_count}\")\n",
    "        print(f\"  跳过样本数: {skipped_count}\")\n",
    "        print(f\"  总token数: {total_tokens:,}\")\n",
    "        print(f\"  平均token数: {total_tokens // valid_count if valid_count > 0 else 0}\")\n",
    "        \n",
    "        # 保存为pickle文件（每个样本单独保存）\n",
    "        output_file = 'train.pkl' if split == 'train' else 'val.pkl'\n",
    "        output_path = os.path.join(data_dir, output_file)\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(samples, f)\n",
    "        print(f\"  保存到: {output_path}\")\n",
    "    \n",
    "    # 保存meta信息（词表大小）\n",
    "    meta = {\n",
    "        'vocab_size': enc.n_vocab,\n",
    "        'dialogue_start': config.dialogue_start,\n",
    "        'summary_start': config.summary_start,\n",
    "        'summary_end': config.summary_end,\n",
    "    }\n",
    "    with open(os.path.join(data_dir, 'meta.pkl'), 'wb') as f:\n",
    "        pickle.dump(meta, f)\n",
    "    \n",
    "    print(\"\\n数据准备完成！\")\n",
    "    print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca41cc",
   "metadata": {
    "papermill": {
     "duration": 0.005676,
     "end_time": "2025-11-12T07:25:19.164983",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.159307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## class SummarizationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd23e1d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:25:19.177708Z",
     "iopub.status.busy": "2025-11-12T07:25:19.177486Z",
     "iopub.status.idle": "2025-11-12T07:25:19.185565Z",
     "shell.execute_reply": "2025-11-12T07:25:19.185022Z"
    },
    "papermill": {
     "duration": 0.015832,
     "end_time": "2025-11-12T07:25:19.186653",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.170821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 第三部分：数据集类和数据加载\n",
    "# =============================================================================\n",
    "\n",
    "class SummarizationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    摘要任务的Dataset类\n",
    "    \n",
    "    每个样本是一个完整的\"对话+摘要\"序列，包含：\n",
    "    - dialogue_start + 对话内容 + summary_start + 摘要内容 + summary_end\n",
    "    \n",
    "    这个类负责：\n",
    "    1. 加载tokenized的样本\n",
    "    2. Padding/截断到固定长度\n",
    "    3. 构建输入(x)和目标(y)序列\n",
    "    \"\"\"\n",
    "    \n",
    "    # def __init__(self, data_path, block_size):\n",
    "    #     \"\"\"\n",
    "    #     参数:\n",
    "    #         data_path: pickle文件路径，包含样本的token列表\n",
    "    #         block_size: 序列的最大长度\n",
    "    #     \"\"\"\n",
    "        # with open(data_path, 'rb') as f:\n",
    "        #     self.samples = pickle.load(f)\n",
    "        # self.block_size = block_size\n",
    "        # print(f\"  加载了 {len(self.samples)} 个样本\")\n",
    "    def __init__(self, data_path, block_size):\n",
    "        with open(data_path, 'rb') as f:\n",
    "            self.samples = pickle.load(f)\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # 为了找到摘要开始的位置，我们需要提前tokenize摘要开始的标记\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.summary_token_ids = enc.encode(config.summary_start)\n",
    "        \n",
    "        print(f\"  加载了 {len(self.samples)} 个样本\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    # def __getitem__(self, idx):\n",
    "        # \"\"\"\n",
    "        # 返回一个样本的(x, y)对\n",
    "        \n",
    "        # x: 输入序列 [block_size]\n",
    "        # y: 目标序列 [block_size]，即x向右移动一位\n",
    "        \n",
    "        # 重要：x中不能有-1（会导致embedding错误），只有y中可以有-1用于padding\n",
    "        # \"\"\"\n",
    "        # sample_tokens = self.samples[idx]\n",
    "        # sample_len = len(sample_tokens)\n",
    "        \n",
    "        # if sample_len >= self.block_size + 1:\n",
    "        #     # 样本足够长，直接截断\n",
    "        #     x = torch.tensor(sample_tokens[:self.block_size], dtype=torch.long)\n",
    "        #     y = torch.tensor(sample_tokens[1:self.block_size + 1], dtype=torch.long)\n",
    "        # else:\n",
    "        #     # 样本较短，需要padding\n",
    "        #     # x: 用0 padding（GPT-2的<|endoftext|> token ID是50256，但0通常是安全的padding）\n",
    "        #     # y: 用-1 padding（会在loss计算时被ignore）\n",
    "        #     x_tokens = sample_tokens[:sample_len]\n",
    "        #     y_tokens = sample_tokens[1:sample_len] if sample_len > 0 else []\n",
    "            \n",
    "        #     # Padding到block_size\n",
    "        #     x_padding_length = self.block_size - len(x_tokens)\n",
    "        #     y_padding_length = self.block_size - len(y_tokens)\n",
    "            \n",
    "        #     # 使用50256（<|endoftext|>）作为x的padding，-1作为y的padding\n",
    "        #     x_tokens = x_tokens + [50256] * x_padding_length\n",
    "        #     y_tokens = y_tokens + [-1] * y_padding_length\n",
    "            \n",
    "        #     x = torch.tensor(x_tokens, dtype=torch.long)\n",
    "        #     y = torch.tensor(y_tokens, dtype=torch.long)\n",
    "        \n",
    "        # return x, y\n",
    "    def __getitem__(self, idx):\n",
    "        sample_tokens = self.samples[idx]\n",
    "        \n",
    "        # 1. 创建 x 和 y (与之前逻辑相同)\n",
    "        # x 是输入, y 是目标 (x 向右移动一位)\n",
    "        x_tokens = sample_tokens[:-1]\n",
    "        y_tokens = sample_tokens[1:]\n",
    "        \n",
    "        # 2. 应用损失掩码到 y 上\n",
    "        # 找到摘要开始的位置\n",
    "        summary_start_pos = -1\n",
    "        for i in range(len(x_tokens) - len(self.summary_token_ids)):\n",
    "            if x_tokens[i:i+len(self.summary_token_ids)] == self.summary_token_ids:\n",
    "                summary_start_pos = i\n",
    "                break\n",
    "        \n",
    "        # 如果找到了摘要标记，将它之前的所有目标 token 设置为 -1\n",
    "        if summary_start_pos != -1:\n",
    "            # 我们希望从 \"### SUMMARY:\\n\" 的最后一个 token 开始预测第一个摘要词\n",
    "            # 所以，掩码应该应用到这个位置之前的所有 token\n",
    "            mask_end_index = summary_start_pos + len(self.summary_token_ids)\n",
    "            for i in range(mask_end_index):\n",
    "                y_tokens[i] = -1\n",
    "        \n",
    "        # 3. Padding (与之前逻辑相同)\n",
    "        x_padding_len = self.block_size - len(x_tokens)\n",
    "        y_padding_len = self.block_size - len(y_tokens)\n",
    "        \n",
    "        x_padded = x_tokens + [50256] * x_padding_len\n",
    "        y_padded = y_tokens + [-1] * y_padding_len\n",
    "        \n",
    "        # 截断以防万一\n",
    "        x = torch.tensor(x_padded[:self.block_size], dtype=torch.long)\n",
    "        y = torch.tensor(y_padded[:self.block_size], dtype=torch.long)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "\n",
    "# 全局变量：缓存DataLoader\n",
    "_dataloaders = {'train': None, 'val': None}\n",
    "_data_iters = {'train': None, 'val': None}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f67bf7f",
   "metadata": {
    "papermill": {
     "duration": 0.005738,
     "end_time": "2025-11-12T07:25:19.198360",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.192622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## getbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "931055be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:25:19.210662Z",
     "iopub.status.busy": "2025-11-12T07:25:19.210459Z",
     "iopub.status.idle": "2025-11-12T07:25:19.216281Z",
     "shell.execute_reply": "2025-11-12T07:25:19.215575Z"
    },
    "papermill": {
     "duration": 0.013199,
     "end_time": "2025-11-12T07:25:19.217348",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.204149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(split, data_dir):\n",
    "    \"\"\"\n",
    "    获取一个训练批次\n",
    "    \n",
    "    使用DataLoader实现，支持：\n",
    "    1. 自动batch处理\n",
    "    2. 可选的shuffle（训练集shuffle，验证集不shuffle）\n",
    "    3. 自动循环迭代（epoch结束后自动重新开始）\n",
    "    \n",
    "    参数:\n",
    "        split: 'train' 或 'val'\n",
    "        data_dir: 数据目录\n",
    "    \n",
    "    返回:\n",
    "        x: 输入序列 [batch_size, block_size]\n",
    "        y: 目标序列 [batch_size, block_size]\n",
    "    \"\"\"\n",
    "    global _dataloaders, _data_iters\n",
    "    \n",
    "    # 首次调用：创建DataLoader\n",
    "    if _dataloaders[split] is None:\n",
    "        data_path = os.path.join(data_dir, f'{split}.pkl')\n",
    "        dataset = SummarizationDataset(data_path, config.block_size)\n",
    "        \n",
    "        # 训练集shuffle，验证集不shuffle\n",
    "        shuffle = (split == 'train')\n",
    "        \n",
    "        _dataloaders[split] = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=4,  # 主线程加载数据（简单场景足够）\n",
    "            pin_memory=True if 'cuda' in config.device else False,\n",
    "        )\n",
    "        _data_iters[split] = iter(_dataloaders[split])\n",
    "    \n",
    "    # 获取下一个batch\n",
    "    try:\n",
    "        x, y = next(_data_iters[split])\n",
    "    except StopIteration:\n",
    "        # 当前epoch结束，重新开始\n",
    "        _data_iters[split] = iter(_dataloaders[split])\n",
    "        x, y = next(_data_iters[split])\n",
    "    \n",
    "    # 移动到设备\n",
    "    if 'cuda' in config.device:\n",
    "        x = x.to(config.device, non_blocking=True)\n",
    "        y = y.to(config.device, non_blocking=True)\n",
    "    else:\n",
    "        x = x.to(config.device)\n",
    "        y = y.to(config.device)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cced037b",
   "metadata": {
    "papermill": {
     "duration": 0.005852,
     "end_time": "2025-11-12T07:25:19.229265",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.223413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## estimate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a798c4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:25:19.242100Z",
     "iopub.status.busy": "2025-11-12T07:25:19.241723Z",
     "iopub.status.idle": "2025-11-12T07:25:19.246202Z",
     "shell.execute_reply": "2025-11-12T07:25:19.245498Z"
    },
    "papermill": {
     "duration": 0.012083,
     "end_time": "2025-11-12T07:25:19.247206",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.235123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, ctx, data_dir):\n",
    "    \"\"\"\n",
    "    估计训练集和验证集上的损失\n",
    "    \n",
    "    通过多次迭代求平均，得到更准确的损失估计\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(config.eval_iters)\n",
    "        for k in range(config.eval_iters):\n",
    "            X, Y = get_batch(split, data_dir)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0625effa",
   "metadata": {
    "papermill": {
     "duration": 0.005712,
     "end_time": "2025-11-12T07:25:19.258661",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.252949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## evaluate rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d9eeddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:25:19.271627Z",
     "iopub.status.busy": "2025-11-12T07:25:19.271416Z",
     "iopub.status.idle": "2025-11-12T07:25:19.280199Z",
     "shell.execute_reply": "2025-11-12T07:25:19.279692Z"
    },
    "papermill": {
     "duration": 0.016644,
     "end_time": "2025-11-12T07:25:19.281259",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.264615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_rouge_during_training(model, ctx, data_dir, num_samples=3):\n",
    "    \"\"\"\n",
    "    在训练过程中评估ROUGE分数\n",
    "    \n",
    "    从验证集中随机选择几个样本，生成摘要并计算ROUGE分数\n",
    "    这可以帮助我们实时监控模型的摘要质量\n",
    "    \n",
    "    参数:\n",
    "        model: 模型\n",
    "        ctx: autocast上下文\n",
    "        data_dir: 数据目录（未使用，保留兼容性）\n",
    "        num_samples: 评估的样本数量（默认3个，避免评估时间过长）\n",
    "    \n",
    "    返回:\n",
    "        rouge_scores: 包含平均ROUGE分数的字典\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 初始化tokenizer\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    # 从验证集CSV文件中读取样本\n",
    "    val_csv = os.path.join(config.dataset_path, 'validation.csv')\n",
    "    if not os.path.exists(val_csv):\n",
    "        print(\"  (跳过ROUGE评估：未找到验证集)\")\n",
    "        model.train()\n",
    "        return None\n",
    "    \n",
    "    # 读取验证集\n",
    "    dialogues = []\n",
    "    summaries = []\n",
    "    with open(val_csv, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            dialogues.append(row['dialogue'])\n",
    "            summaries.append(row['summary'])\n",
    "    \n",
    "    # 随机选择num_samples个样本\n",
    "    import random\n",
    "    indices = random.sample(range(len(dialogues)), min(num_samples, len(dialogues)))\n",
    "    \n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    # 获取<|endoftext|>的token ID作为停止符\n",
    "    eos_token_id = enc.encode(config.summary_end, allowed_special={config.summary_end})[0]\n",
    "    \n",
    "    # 临时保存原始max_new_tokens，训练时使用较少的tokens以加快速度\n",
    "    original_max_new_tokens = config.max_new_tokens\n",
    "    config.max_new_tokens = 100  # 训练时用较少的tokens\n",
    "    \n",
    "    for idx in tqdm(indices):\n",
    "        dialogue = dialogues[idx]\n",
    "        reference_summary = summaries[idx]\n",
    "        \n",
    "        # 构建prompt\n",
    "        prompt = config.dialogue_start + dialogue + config.summary_start\n",
    "        prompt_tokens = enc.encode(prompt, allowed_special={config.summary_end})\n",
    "        \n",
    "        # 如果prompt太长，跳过\n",
    "        if len(prompt_tokens) > config.block_size - config.max_new_tokens:\n",
    "            print(f\"len(prompt_tokens) > config.block_size - config.max_new_tokens: {len(prompt_tokens)} > {config.block_size - config.max_new_tokens}\")\n",
    "            continue\n",
    "        \n",
    "        # 使用KV cache加速生成摘要（但训练时为了速度，使用原始的generate方法）\n",
    "        # 注意：训练时模型可能还没有完全训练好，所以使用简单的generate方法\n",
    "        x = torch.tensor(prompt_tokens, dtype=torch.long, device=config.device)[None, ...]\n",
    "        \n",
    "        with ctx:\n",
    "            y = model.generate(\n",
    "                x,\n",
    "                max_new_tokens=config.max_new_tokens,\n",
    "                temperature=0.8,\n",
    "                top_k=200,\n",
    "                eos_token_id=eos_token_id\n",
    "            )\n",
    "        \n",
    "        # 解码\n",
    "        generated_tokens = y[0].tolist()\n",
    "        generated_text = enc.decode(generated_tokens)\n",
    "        \n",
    "        # 提取摘要（使用公共函数）\n",
    "        generated_summary = extract_summary(generated_text, prompt, enc)\n",
    "        \n",
    "        # 计算ROUGE分数（使用公共函数）\n",
    "        if generated_summary:  # 确保生成了内容\n",
    "            rouge_scores = calculate_rouge(reference_summary, generated_summary)\n",
    "            if rouge_scores:\n",
    "                rouge1_scores.append(rouge_scores['rouge1'])\n",
    "                rouge2_scores.append(rouge_scores['rouge2'])\n",
    "                rougeL_scores.append(rouge_scores['rougeL'])\n",
    "    \n",
    "    # 恢复原始max_new_tokens\n",
    "    config.max_new_tokens = original_max_new_tokens\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # 返回平均分数\n",
    "    if len(rouge1_scores) > 0:\n",
    "        return {\n",
    "            'rouge1': np.mean(rouge1_scores),\n",
    "            'rouge2': np.mean(rouge2_scores),\n",
    "            'rougeL': np.mean(rougeL_scores)\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1af2f5",
   "metadata": {
    "papermill": {
     "duration": 0.005764,
     "end_time": "2025-11-12T07:25:19.292945",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.287181",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## get lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "199d1dc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:25:19.306854Z",
     "iopub.status.busy": "2025-11-12T07:25:19.306448Z",
     "iopub.status.idle": "2025-11-12T07:25:19.324974Z",
     "shell.execute_reply": "2025-11-12T07:25:19.324434Z"
    },
    "papermill": {
     "duration": 0.027377,
     "end_time": "2025-11-12T07:25:19.326032",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.298655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lr(iter_num):\n",
    "    \"\"\"\n",
    "    学习率调度：带预热的余弦衰减\n",
    "    \n",
    "    1. 前warmup_iters步：线性增加\n",
    "    2. 之后：余弦衰减到min_lr\n",
    "    \"\"\"\n",
    "    # 线性预热\n",
    "    if iter_num < config.warmup_iters:\n",
    "        return config.learning_rate * (iter_num + 1) / (config.warmup_iters + 1)\n",
    "    \n",
    "    # 如果超过衰减步数，返回最小学习率\n",
    "    if iter_num > config.lr_decay_iters:\n",
    "        return config.min_lr\n",
    "    \n",
    "    # 余弦衰减\n",
    "    decay_ratio = (iter_num - config.warmup_iters) / (config.lr_decay_iters - config.warmup_iters)\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return config.min_lr + coeff * (config.learning_rate - config.min_lr)\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"训练主函数\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"开始训练...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(1337)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # 创建输出目录\n",
    "    os.makedirs(config.out_dir, exist_ok=True)\n",
    "    \n",
    "    # 设置设备和精度\n",
    "    device_type = 'cuda' if 'cuda' in config.device else 'cpu'\n",
    "    ptdtype = {\n",
    "        'float32': torch.float32,\n",
    "        'bfloat16': torch.bfloat16,\n",
    "        'float16': torch.float16\n",
    "    }[config.dtype]\n",
    "    ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(\n",
    "        device_type=device_type, dtype=ptdtype\n",
    "    )\n",
    "    \n",
    "    # 数据目录\n",
    "    data_dir = os.path.join('data', config.dataset)\n",
    "    \n",
    "    # 加载词表信息\n",
    "    meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "    meta_vocab_size = None\n",
    "    if os.path.exists(meta_path):\n",
    "        with open(meta_path, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "        meta_vocab_size = meta['vocab_size']\n",
    "        print(f\"从 {meta_path} 加载词表大小: {meta_vocab_size}\")\n",
    "    \n",
    "    # 初始化模型\n",
    "    print(f\"\\n模型初始化方式: {config.init_from}\")\n",
    "    model_args = dict(\n",
    "        n_layer=config.n_layer,\n",
    "        n_head=config.n_head,\n",
    "        n_embd=config.n_embd,\n",
    "        block_size=config.block_size,\n",
    "        bias=config.bias,\n",
    "        vocab_size=None,\n",
    "        dropout=config.dropout\n",
    "    )\n",
    "    \n",
    "    if config.init_from == 'scratch':\n",
    "        # 从头训练\n",
    "        print(\"从头开始训练新模型\")\n",
    "        model_args['vocab_size'] = meta_vocab_size if meta_vocab_size else 50304\n",
    "        gptconf = GPTConfig(**model_args)\n",
    "        model = GPT(gptconf)\n",
    "        iter_num = 0\n",
    "        best_val_loss = 1e9\n",
    "        \n",
    "    elif config.init_from == 'resume':\n",
    "        # 从checkpoint恢复\n",
    "        print(f\"从 {config.out_dir} 恢复训练\")\n",
    "        ckpt_path = os.path.join(config.out_dir, 'ckpt.pt')\n",
    "        checkpoint = torch.load(ckpt_path, map_location=config.device)\n",
    "        checkpoint_model_args = checkpoint['model_args']\n",
    "        \n",
    "        for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "            model_args[k] = checkpoint_model_args[k]\n",
    "        \n",
    "        gptconf = GPTConfig(**model_args)\n",
    "        model = GPT(gptconf)\n",
    "        \n",
    "        state_dict = checkpoint['model']\n",
    "        unwanted_prefix = '_orig_mod.'\n",
    "        for k, v in list(state_dict.items()):\n",
    "            if k.startswith(unwanted_prefix):\n",
    "                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "        \n",
    "        model.load_state_dict(state_dict)\n",
    "        iter_num = checkpoint['iter_num']\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        \n",
    "    elif config.init_from.startswith('gpt2'):\n",
    "        # 从预训练GPT-2加载\n",
    "        print(f\"从OpenAI GPT-2加载: {config.init_from}\")\n",
    "        override_args = dict(dropout=config.dropout)\n",
    "        model = GPT.from_pretrained(config.init_from, override_args)\n",
    "        \n",
    "        for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "            model_args[k] = getattr(model.config, k)\n",
    "        \n",
    "        iter_num = 0\n",
    "        best_val_loss = 1e9\n",
    "    \n",
    "    # 调整block_size（如果需要）\n",
    "    if config.block_size < model.config.block_size:\n",
    "        model.crop_block_size(config.block_size)\n",
    "        model_args['block_size'] = config.block_size\n",
    "    \n",
    "    model.to(config.device)\n",
    "    \n",
    "    # 初始化优化器\n",
    "    optimizer = model.configure_optimizers(\n",
    "        config.weight_decay,\n",
    "        config.learning_rate,\n",
    "        (config.beta1, config.beta2),\n",
    "        device_type\n",
    "    )\n",
    "    \n",
    "    if config.init_from == 'resume' and 'optimizer' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    checkpoint = None  # 释放内存\n",
    "    \n",
    "    # 编译模型（可选，PyTorch 2.0+）\n",
    "    if config.compile:\n",
    "        print(\"编译模型（首次会比较慢）...\")\n",
    "        unoptimized_model = model\n",
    "        model = torch.compile(model)\n",
    "    \n",
    "    # 初始化GradScaler（用于混合精度训练）\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(config.dtype == 'float16'))\n",
    "    \n",
    "    # 训练循环\n",
    "    print(\"\\n开始训练循环...\")\n",
    "    print(f\"总迭代次数: {config.max_iters}\")\n",
    "    print(f\"批次大小: {config.batch_size}\")\n",
    "    print(f\"梯度累积步数: {config.gradient_accumulation_steps}\")\n",
    "    print(f\"有效批次大小: {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    X, Y = get_batch('train', data_dir)\n",
    "    t0 = time.time()\n",
    "    local_iter_num = 0\n",
    "    raw_model = model\n",
    "    running_mfu = -1.0\n",
    "    \n",
    "    while True:\n",
    "        # 设置学习率\n",
    "        lr = get_lr(iter_num) if config.decay_lr else config.learning_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        # 评估和保存checkpoint\n",
    "        if iter_num % config.eval_interval == 0:\n",
    "            losses = estimate_loss(model, ctx, data_dir)\n",
    "            print(f\"\\nStep {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            \n",
    "            # 计算ROUGE分数（从第一次评估后开始，避免初始化时模型输出不稳定）\n",
    "            if iter_num > 0 and config.eval_rouge_during_training:\n",
    "                print(\"  评估ROUGE分数...\")\n",
    "                rouge_scores = evaluate_rouge_during_training(\n",
    "                    model, ctx, data_dir, num_samples=config.rouge_eval_samples\n",
    "                )\n",
    "                if rouge_scores:\n",
    "                    print(f\"  ROUGE-1: {rouge_scores['rouge1']:.4f}, \"\n",
    "                          f\"ROUGE-2: {rouge_scores['rouge2']:.4f}, \"\n",
    "                          f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "            \n",
    "            # 保存checkpoint\n",
    "            if losses['val'] < best_val_loss or config.always_save_checkpoint:\n",
    "                best_val_loss = losses['val']\n",
    "                if iter_num > 0:\n",
    "                    checkpoint = {\n",
    "                        'model': raw_model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'model_args': model_args,\n",
    "                        'iter_num': iter_num,\n",
    "                        'best_val_loss': best_val_loss,\n",
    "                        'config': vars(config),\n",
    "                    }\n",
    "                    print(f\"  保存checkpoint到 {config.out_dir}\")\n",
    "                    torch.save(checkpoint, os.path.join(config.out_dir, 'ckpt.pt'))\n",
    "        \n",
    "        if iter_num == 0 and config.eval_only:\n",
    "            break\n",
    "        \n",
    "        # 前向-反向传播（带梯度累积）\n",
    "        for micro_step in range(config.gradient_accumulation_steps):\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / config.gradient_accumulation_steps\n",
    "            \n",
    "            # 异步预取下一个batch\n",
    "            X, Y = get_batch('train', data_dir)\n",
    "            \n",
    "            # 反向传播\n",
    "            scaler.scale(loss).backward()\n",
    "        \n",
    "        # 梯度裁剪\n",
    "        if config.grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "        \n",
    "        # 更新参数\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # 记录日志\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        \n",
    "        if iter_num % config.log_interval == 0:\n",
    "            lossf = loss.item() * config.gradient_accumulation_steps\n",
    "            if local_iter_num >= 5:\n",
    "                mfu = raw_model.estimate_mfu(\n",
    "                    config.batch_size * config.gradient_accumulation_steps, dt\n",
    "                )\n",
    "                running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n",
    "            print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "        \n",
    "        iter_num += 1\n",
    "        local_iter_num += 1\n",
    "        \n",
    "        # 终止条件\n",
    "        if iter_num > config.max_iters:\n",
    "            break\n",
    "    \n",
    "    print(\"\\n训练完成！\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd37db93",
   "metadata": {
    "papermill": {
     "duration": 0.005659,
     "end_time": "2025-11-12T07:25:19.337684",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.332025",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77580877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:25:19.350033Z",
     "iopub.status.busy": "2025-11-12T07:25:19.349824Z",
     "iopub.status.idle": "2025-11-12T07:25:19.356028Z",
     "shell.execute_reply": "2025-11-12T07:25:19.355304Z"
    },
    "papermill": {
     "duration": 0.013742,
     "end_time": "2025-11-12T07:25:19.357177",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.343435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 第四部分：测试和评估\n",
    "# =============================================================================\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    加载训练好的模型\n",
    "    \n",
    "    返回:\n",
    "        model: 加载的模型\n",
    "        enc: tokenizer\n",
    "        ctx: autocast上下文\n",
    "    \"\"\"\n",
    "    # 设置设备和精度\n",
    "    device_type = 'cuda' if 'cuda' in config.device else 'cpu'\n",
    "    ptdtype = {\n",
    "        'float32': torch.float32,\n",
    "        'bfloat16': torch.bfloat16,\n",
    "        'float16': torch.float16\n",
    "    }[config.dtype]\n",
    "    ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(\n",
    "        device_type=device_type, dtype=ptdtype\n",
    "    )\n",
    "    \n",
    "    # 加载模型\n",
    "    print(f\"\\n从 {config.out_dir} 加载模型...\")\n",
    "    ckpt_path = os.path.join(config.out_dir, 'ckpt.pt')\n",
    "\n",
    "    print(f\"path {ckpt_path}\")\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        print(f\"错误: 找不到checkpoint文件 {ckpt_path}\")\n",
    "        print(\"请先运行训练！\")\n",
    "        return None, None, None\n",
    "    \n",
    "    checkpoint = torch.load(ckpt_path, map_location=config.device)\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    \n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    \n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "    \n",
    "    if config.compile:\n",
    "        print(\"编译模型...\")\n",
    "        model = torch.compile(model)\n",
    "    \n",
    "    # 初始化tokenizer\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    return model, enc, ctx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73445667",
   "metadata": {
    "papermill": {
     "duration": 0.005544,
     "end_time": "2025-11-12T07:25:19.368479",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.362935",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## generate summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "537f9864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:25:19.381116Z",
     "iopub.status.busy": "2025-11-12T07:25:19.380792Z",
     "iopub.status.idle": "2025-11-12T07:25:19.389988Z",
     "shell.execute_reply": "2025-11-12T07:25:19.389270Z"
    },
    "papermill": {
     "duration": 0.016785,
     "end_time": "2025-11-12T07:25:19.391053",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.374268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_summary(model, prompt_tokens, enc, ctx, eos_token_id=None):\n",
    "    \"\"\"\n",
    "    使用KV cache加速生成摘要\n",
    "    \n",
    "    参数:\n",
    "        model: GPT模型\n",
    "        prompt_tokens: prompt的token列表\n",
    "        enc: tokenizer\n",
    "        ctx: autocast上下文\n",
    "        eos_token_id: 结束token ID，用于提前停止\n",
    "    \n",
    "    返回:\n",
    "        generated_text: 生成的完整文本（包含prompt和摘要）\n",
    "    \"\"\"\n",
    "    # 检查长度，如果太长则截断\n",
    "    if len(prompt_tokens) > config.block_size - config.max_new_tokens:\n",
    "        dialogue_start_tokens = enc.encode(config.dialogue_start)\n",
    "        summary_start_tokens = enc.encode(config.summary_start)\n",
    "        available_space = config.block_size - config.max_new_tokens - len(dialogue_start_tokens) - len(summary_start_tokens)\n",
    "        \n",
    "        if available_space > 0:\n",
    "            # 找到对话部分的tokens\n",
    "            # 解码prompt找到对话部分\n",
    "            prompt_text = enc.decode(prompt_tokens)\n",
    "            dialogue_start_pos = prompt_text.find(config.dialogue_start)\n",
    "            summary_start_pos = prompt_text.find(config.summary_start)\n",
    "            \n",
    "            if dialogue_start_pos >= 0 and summary_start_pos > dialogue_start_pos:\n",
    "                dialogue_text = prompt_text[dialogue_start_pos + len(config.dialogue_start):summary_start_pos]\n",
    "                dialogue_tokens = enc.encode(dialogue_text)\n",
    "                truncated_dialogue_tokens = dialogue_tokens[:available_space]\n",
    "                prompt_tokens = dialogue_start_tokens + truncated_dialogue_tokens + summary_start_tokens\n",
    "            else:\n",
    "                # 如果找不到标记，直接截断\n",
    "                prompt_tokens = prompt_tokens[:config.block_size - config.max_new_tokens]\n",
    "        else:\n",
    "            # 如果空间不足，只保留必要的标记\n",
    "            prompt_tokens = dialogue_start_tokens + summary_start_tokens\n",
    "    \n",
    "    # 转换为tensor\n",
    "    x = torch.tensor(prompt_tokens, dtype=torch.long, device=config.device)[None, ...]\n",
    "    \n",
    "    # 使用KV cache加速生成摘要\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            y = model.generate_with_kv_cache(\n",
    "                x,\n",
    "                max_new_tokens=config.max_new_tokens,\n",
    "                temperature=config.temperature,\n",
    "                top_k=config.top_k,\n",
    "                eos_token_id=eos_token_id\n",
    "            )\n",
    "    \n",
    "    # 解码生成的token\n",
    "    generated_tokens = y[0].tolist()\n",
    "    generated_text = enc.decode(generated_tokens)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "def extract_summary(generated_text, prompt_text, enc):\n",
    "    \"\"\"\n",
    "    从生成的文本中提取摘要部分\n",
    "    \n",
    "    参数:\n",
    "        generated_text: 生成的完整文本\n",
    "        prompt_text: 原始prompt文本\n",
    "        enc: tokenizer\n",
    "    \n",
    "    返回:\n",
    "        generated_summary: 提取的摘要文本\n",
    "    \"\"\"\n",
    "    # 提取生成的摘要（去除prompt部分）\n",
    "    if config.summary_start in generated_text:\n",
    "        generated_summary = generated_text.split(config.summary_start)[-1]\n",
    "        \n",
    "        # 去除结束标记\n",
    "        if config.summary_end in generated_summary:\n",
    "            generated_summary = generated_summary.split(config.summary_end)[0]\n",
    "    else:\n",
    "        # 如果没找到标记，就从prompt长度之后开始提取\n",
    "        if len(generated_text) > len(prompt_text):\n",
    "            generated_summary = generated_text[len(prompt_text):]\n",
    "        else:\n",
    "            generated_summary = \"\"\n",
    "    \n",
    "    # 清理生成的摘要\n",
    "    generated_summary = generated_summary.strip()\n",
    "    \n",
    "    # 如果生成的摘要为空，使用空字符串\n",
    "    if not generated_summary:\n",
    "        generated_summary = \"\"\n",
    "    \n",
    "    return generated_summary\n",
    "\n",
    "\n",
    "def calculate_rouge(reference_summary, generated_summary):\n",
    "    \"\"\"\n",
    "    计算ROUGE分数\n",
    "    \n",
    "    参数:\n",
    "        reference_summary: 参考摘要\n",
    "        generated_summary: 生成的摘要\n",
    "    \n",
    "    返回:\n",
    "        dict: 包含rouge1, rouge2, rougeL的字典，如果失败则返回None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from rouge_score import rouge_scorer\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        scores = scorer.score(reference_summary, generated_summary)\n",
    "        return {\n",
    "            'rouge1': scores['rouge1'].fmeasure,\n",
    "            'rouge2': scores['rouge2'].fmeasure,\n",
    "            'rougeL': scores['rougeL'].fmeasure\n",
    "        }\n",
    "    except ImportError:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc46909",
   "metadata": {
    "papermill": {
     "duration": 0.005672,
     "end_time": "2025-11-12T07:25:19.402530",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.396858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6009dc77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:25:19.415919Z",
     "iopub.status.busy": "2025-11-12T07:25:19.415282Z",
     "iopub.status.idle": "2025-11-12T08:41:38.231053Z",
     "shell.execute_reply": "2025-11-12T08:41:38.230048Z"
    },
    "papermill": {
     "duration": 4578.823901,
     "end_time": "2025-11-12T08:41:38.232384",
     "exception": false,
     "start_time": "2025-11-12T07:25:19.408483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "                                 GPT-2 摘要微调教学脚本                                 \n",
      "================================================================================\n",
      "\n",
      "当前配置:\n",
      "  数据集: /kaggle/working/data/samsum\n",
      "  模型初始化: gpt2\n",
      "  设备: cuda\n",
      "  批次大小: 8\n",
      "  最大迭代次数: 500\n",
      "  学习率: 6e-05\n",
      "expect: train_pkl: data/samsum/train.pkl\n",
      "\n",
      "未找到处理后的数据文件，开始准备数据...\n",
      "================================================================================\n",
      "准备数据集...\n",
      "================================================================================\n",
      "\n",
      "处理 train 数据集...\n",
      "  读取了 24834 条数据\n",
      "  有效样本数: 24834\n",
      "  跳过样本数: 0\n",
      "  总token数: 4,393,055\n",
      "  平均token数: 176\n",
      "  保存到: data/samsum/train.pkl\n",
      "\n",
      "处理 validation 数据集...\n",
      "  读取了 1307 条数据\n",
      "  有效样本数: 1307\n",
      "  跳过样本数: 0\n",
      "  总token数: 233,808\n",
      "  平均token数: 178\n",
      "  保存到: data/samsum/val.pkl\n",
      "\n",
      "数据准备完成！\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "开始训练...\n",
      "================================================================================\n",
      "从 data/samsum/meta.pkl 加载词表大小: 50257\n",
      "\n",
      "模型初始化方式: gpt2\n",
      "从OpenAI GPT-2加载: gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 07:25:39.961253: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762932340.374887      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762932340.477476      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.1\n",
      "number of parameters: 123.65M\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36555e67d184c4cad8274fdb1720361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd929fec79694a84849f18ba619c49eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7391e7372f9541448b3d5262523179f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "编译模型（首次会比较慢）...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/530882074.py:142: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(config.dtype == 'float16'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始训练循环...\n",
      "总迭代次数: 500\n",
      "批次大小: 8\n",
      "梯度累积步数: 16\n",
      "有效批次大小: 128\n",
      "--------------------------------------------------------------------------------\n",
      "  加载了 24834 个样本\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1112 07:26:15.761000 19 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  加载了 1307 个样本\n",
      "\n",
      "Step 0: train loss 3.3021, val loss 3.2740\n",
      "iter 0: loss 3.4652, time 75553.08ms, mfu -100.00%\n",
      "iter 5: loss 3.5836, time 6021.23ms, mfu 17561.81%\n",
      "\n",
      "Step 10: train loss 3.0686, val loss 3.0097\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.0881, ROUGE-2: 0.0000, ROUGE-L: 0.0646\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 10: loss 3.4327, time 22848.23ms, mfu 16268.43%\n",
      "iter 15: loss 3.0161, time 6234.15ms, mfu 16337.79%\n",
      "\n",
      "Step 20: train loss 2.6465, val loss 2.6622\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.2451, ROUGE-2: 0.0276, ROUGE-L: 0.1588\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 20: loss 2.8038, time 21223.65ms, mfu 15202.25%\n",
      "iter 25: loss 2.8658, time 6392.60ms, mfu 15336.18%\n",
      "\n",
      "Step 30: train loss 2.4580, val loss 2.4617\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.2507, ROUGE-2: 0.0412, ROUGE-L: 0.1765\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 30: loss 2.8526, time 21148.66ms, mfu 14302.57%\n",
      "iter 35: loss 2.2516, time 6489.98ms, mfu 14501.65%\n",
      "\n",
      "Step 40: train loss 2.2717, val loss 2.3140\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.2803, ROUGE-2: 0.0534, ROUGE-L: 0.2239\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 40: loss 2.3523, time 39786.15ms, mfu 13317.26%\n",
      "iter 45: loss 2.2710, time 6412.15ms, mfu 13634.65%\n",
      "\n",
      "Step 50: train loss 2.1793, val loss 2.2266\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4469, ROUGE-2: 0.0918, ROUGE-L: 0.2738\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 50: loss 2.4303, time 23591.25ms, mfu 12719.42%\n",
      "iter 55: loss 2.6398, time 6424.36ms, mfu 13093.46%\n",
      "\n",
      "Step 60: train loss 2.1286, val loss 2.2119\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3496, ROUGE-2: 0.0744, ROUGE-L: 0.2489\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 60: loss 2.4259, time 23989.74ms, mfu 12224.90%\n",
      "iter 65: loss 2.5138, time 6480.67ms, mfu 12634.09%\n",
      "\n",
      "Step 70: train loss 2.1386, val loss 2.1032\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3466, ROUGE-2: 0.0790, ROUGE-L: 0.2522\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 70: loss 2.1472, time 23876.02ms, mfu 11813.57%\n",
      "iter 75: loss 2.2644, time 6485.10ms, mfu 12262.78%\n",
      "\n",
      "Step 80: train loss 2.0604, val loss 2.0941\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.2852, ROUGE-2: 0.0655, ROUGE-L: 0.2226\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 80: loss 2.2152, time 24036.83ms, mfu 11476.42%\n",
      "iter 85: loss 2.2050, time 6484.00ms, mfu 11959.62%\n",
      "\n",
      "Step 90: train loss 2.0260, val loss 2.0374\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3883, ROUGE-2: 0.1037, ROUGE-L: 0.3046\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 90: loss 2.2227, time 24039.61ms, mfu 11203.53%\n",
      "iter 95: loss 2.3024, time 6485.41ms, mfu 11713.67%\n",
      "\n",
      "Step 100: train loss 2.0944, val loss 2.0577\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3866, ROUGE-2: 0.1328, ROUGE-L: 0.2823\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 100: loss 1.9537, time 24529.42ms, mfu 10973.39%\n",
      "iter 105: loss 2.1132, time 6488.41ms, mfu 11505.78%\n",
      "\n",
      "Step 110: train loss 2.0020, val loss 1.9590\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4025, ROUGE-2: 0.1322, ROUGE-L: 0.3107\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 110: loss 1.8591, time 23893.83ms, mfu 10797.76%\n",
      "iter 115: loss 1.9795, time 6472.46ms, mfu 11351.74%\n",
      "\n",
      "Step 120: train loss 1.9136, val loss 1.9944\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3153, ROUGE-2: 0.0480, ROUGE-L: 0.2190\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 120: loss 1.8358, time 24482.38ms, mfu 10648.48%\n",
      "iter 125: loss 1.9190, time 6486.08ms, mfu 11213.95%\n",
      "\n",
      "Step 130: train loss 1.9349, val loss 1.9453\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3262, ROUGE-2: 0.0593, ROUGE-L: 0.2167\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 130: loss 2.1538, time 24304.80ms, mfu 10527.63%\n",
      "iter 135: loss 2.0350, time 6500.58ms, mfu 11101.55%\n",
      "\n",
      "Step 140: train loss 1.9103, val loss 1.9435\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4440, ROUGE-2: 0.1653, ROUGE-L: 0.3311\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 140: loss 2.4251, time 23938.43ms, mfu 10433.13%\n",
      "iter 145: loss 2.1395, time 6481.04ms, mfu 11021.40%\n",
      "\n",
      "Step 150: train loss 1.9553, val loss 1.8722\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3768, ROUGE-2: 0.0721, ROUGE-L: 0.2682\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 150: loss 2.1807, time 23784.63ms, mfu 10363.85%\n",
      "iter 155: loss 1.9899, time 7117.61ms, mfu 10813.13%\n",
      "\n",
      "Step 160: train loss 1.8252, val loss 1.9175\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3338, ROUGE-2: 0.0584, ROUGE-L: 0.2082\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 160: loss 1.9594, time 24511.14ms, mfu 10163.22%\n",
      "iter 165: loss 1.9966, time 7307.79ms, mfu 10593.90%\n",
      "\n",
      "Step 170: train loss 1.8065, val loss 1.8824\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4468, ROUGE-2: 0.1457, ROUGE-L: 0.2906\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 170: loss 1.5929, time 24241.09ms, mfu 9970.73%\n",
      "iter 175: loss 1.9759, time 7365.66ms, mfu 10409.29%\n",
      "\n",
      "Step 180: train loss 1.7988, val loss 1.8766\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4189, ROUGE-2: 0.1381, ROUGE-L: 0.3393\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 180: loss 1.9539, time 24468.16ms, mfu 9800.53%\n",
      "iter 185: loss 2.3479, time 7374.79ms, mfu 10254.33%\n",
      "\n",
      "Step 190: train loss 1.8328, val loss 1.8321\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3378, ROUGE-2: 0.1130, ROUGE-L: 0.3028\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 190: loss 2.1527, time 24480.28ms, mfu 9660.85%\n",
      "iter 195: loss 1.7869, time 7376.63ms, mfu 10128.26%\n",
      "\n",
      "Step 200: train loss 1.7965, val loss 1.8753\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3976, ROUGE-2: 0.2114, ROUGE-L: 0.3722\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 200: loss 2.0999, time 24606.92ms, mfu 9545.17%\n",
      "iter 205: loss 1.8203, time 7365.37ms, mfu 10026.34%\n",
      "\n",
      "Step 210: train loss 1.8101, val loss 1.8281\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3815, ROUGE-2: 0.0616, ROUGE-L: 0.2506\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 210: loss 1.6702, time 24593.63ms, mfu 9453.67%\n",
      "iter 215: loss 2.2657, time 7394.81ms, mfu 9938.28%\n",
      "\n",
      "Step 220: train loss 1.7780, val loss 1.8357\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4221, ROUGE-2: 0.0817, ROUGE-L: 0.2791\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 220: loss 1.8764, time 24296.07ms, mfu 9379.68%\n",
      "iter 225: loss 2.0662, time 7374.46ms, mfu 9875.63%\n",
      "\n",
      "Step 230: train loss 1.7345, val loss 1.8091\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4109, ROUGE-2: 0.0592, ROUGE-L: 0.2111\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 230: loss 2.1935, time 24289.34ms, mfu 9323.42%\n",
      "iter 235: loss 2.0316, time 7364.71ms, mfu 9826.89%\n",
      "\n",
      "Step 240: train loss 1.7903, val loss 1.8155\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3813, ROUGE-2: 0.0812, ROUGE-L: 0.2619\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 240: loss 1.9831, time 24756.68ms, mfu 9271.33%\n",
      "iter 245: loss 1.7086, time 7375.35ms, mfu 9777.95%\n",
      "\n",
      "Step 250: train loss 1.7949, val loss 1.8070\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3385, ROUGE-2: 0.0876, ROUGE-L: 0.2631\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 250: loss 2.0193, time 24482.20ms, mfu 9232.07%\n",
      "iter 255: loss 1.8974, time 7374.85ms, mfu 9742.71%\n",
      "\n",
      "Step 260: train loss 1.7338, val loss 1.7812\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.5219, ROUGE-2: 0.2276, ROUGE-L: 0.4124\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 260: loss 1.8782, time 24374.24ms, mfu 9202.27%\n",
      "iter 265: loss 1.7335, time 7379.78ms, mfu 9714.93%\n",
      "\n",
      "Step 270: train loss 1.7672, val loss 1.7828\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.5057, ROUGE-2: 0.2323, ROUGE-L: 0.3891\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 270: loss 1.9966, time 24499.60ms, mfu 9175.05%\n",
      "iter 275: loss 1.8845, time 7378.44ms, mfu 9690.69%\n",
      "\n",
      "Step 280: train loss 1.7121, val loss 1.7730\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4190, ROUGE-2: 0.1292, ROUGE-L: 0.2782\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 280: loss 1.9334, time 25031.46ms, mfu 9144.07%\n",
      "iter 285: loss 1.9665, time 7388.34ms, mfu 9660.88%\n",
      "\n",
      "Step 290: train loss 1.6744, val loss 1.7851\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3751, ROUGE-2: 0.0997, ROUGE-L: 0.2550\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 290: loss 1.7398, time 24476.15ms, mfu 9126.82%\n",
      "iter 295: loss 1.7987, time 7391.60ms, mfu 9644.73%\n",
      "\n",
      "Step 300: train loss 1.7443, val loss 1.7422\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3526, ROUGE-2: 0.0896, ROUGE-L: 0.2544\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 300: loss 1.7705, time 24475.44ms, mfu 9112.30%\n",
      "iter 305: loss 1.8636, time 7377.22ms, mfu 9634.45%\n",
      "\n",
      "Step 310: train loss 1.5972, val loss 1.7474\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3735, ROUGE-2: 0.0906, ROUGE-L: 0.2571\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 310: loss 1.6977, time 24333.68ms, mfu 9105.56%\n",
      "iter 315: loss 1.6547, time 7406.55ms, mfu 9622.71%\n",
      "\n",
      "Step 320: train loss 1.7111, val loss 1.7520\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3621, ROUGE-2: 0.0903, ROUGE-L: 0.2932\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 320: loss 1.8477, time 24374.93ms, mfu 9094.26%\n",
      "iter 325: loss 1.7067, time 7380.53ms, mfu 9617.58%\n",
      "\n",
      "Step 330: train loss 1.5996, val loss 1.7576\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4619, ROUGE-2: 0.2635, ROUGE-L: 0.4150\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 330: loss 1.4207, time 24340.44ms, mfu 9090.26%\n",
      "iter 335: loss 1.5225, time 7364.28ms, mfu 9617.13%\n",
      "\n",
      "Step 340: train loss 1.6122, val loss 1.7211\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4894, ROUGE-2: 0.2275, ROUGE-L: 0.3956\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 340: loss 1.5570, time 24564.24ms, mfu 9085.90%\n",
      "iter 345: loss 1.3511, time 7373.00ms, mfu 9611.51%\n",
      "\n",
      "Step 350: train loss 1.6068, val loss 1.7394\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4326, ROUGE-2: 0.2017, ROUGE-L: 0.3527\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 350: loss 1.7384, time 24559.50ms, mfu 9080.92%\n",
      "iter 355: loss 1.8448, time 7373.71ms, mfu 9606.89%\n",
      "\n",
      "Step 360: train loss 1.6369, val loss 1.6979\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3820, ROUGE-2: 0.1302, ROUGE-L: 0.3011\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 360: loss 1.6652, time 24701.82ms, mfu 9074.29%\n",
      "iter 365: loss 1.7095, time 7372.18ms, mfu 9601.22%\n",
      "\n",
      "Step 370: train loss 1.6437, val loss 1.7597\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4070, ROUGE-2: 0.1215, ROUGE-L: 0.3092\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 370: loss 1.7962, time 24525.95ms, mfu 9072.25%\n",
      "iter 375: loss 1.4467, time 7384.53ms, mfu 9596.99%\n",
      "\n",
      "Step 380: train loss 1.6118, val loss 1.7021\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4724, ROUGE-2: 0.1798, ROUGE-L: 0.3733\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 380: loss 1.5524, time 24398.55ms, mfu 9070.69%\n",
      "iter 385: loss 1.5743, time 7388.19ms, mfu 9594.87%\n",
      "\n",
      "Step 390: train loss 1.6126, val loss 1.7194\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.5058, ROUGE-2: 0.2349, ROUGE-L: 0.3843\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 390: loss 1.9333, time 24681.38ms, mfu 9063.82%\n",
      "iter 395: loss 1.5113, time 7378.66ms, mfu 9590.54%\n",
      "\n",
      "Step 400: train loss 1.6240, val loss 1.6791\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4329, ROUGE-2: 0.1503, ROUGE-L: 0.3308\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 400: loss 1.8798, time 24339.96ms, mfu 9065.93%\n",
      "iter 405: loss 1.5571, time 7379.49ms, mfu 9592.28%\n",
      "\n",
      "Step 410: train loss 1.5449, val loss 1.7283\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4808, ROUGE-2: 0.1548, ROUGE-L: 0.3384\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 410: loss 1.8320, time 24667.21ms, mfu 9061.73%\n",
      "iter 415: loss 1.8104, time 7386.60ms, mfu 9587.12%\n",
      "\n",
      "Step 420: train loss 1.5500, val loss 1.6788\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4080, ROUGE-2: 0.0834, ROUGE-L: 0.2718\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 420: loss 1.7789, time 24510.74ms, mfu 9059.83%\n",
      "iter 425: loss 1.8587, time 7388.02ms, mfu 9585.13%\n",
      "\n",
      "Step 430: train loss 1.6060, val loss 1.7227\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3863, ROUGE-2: 0.0939, ROUGE-L: 0.2809\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 430: loss 1.9642, time 24529.82ms, mfu 9057.70%\n",
      "iter 435: loss 1.6410, time 7386.61ms, mfu 9583.49%\n",
      "\n",
      "Step 440: train loss 1.5729, val loss 1.6665\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4145, ROUGE-2: 0.0641, ROUGE-L: 0.2733\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 440: loss 1.5966, time 24572.24ms, mfu 9055.48%\n",
      "iter 445: loss 1.4857, time 7380.35ms, mfu 9582.71%\n",
      "\n",
      "Step 450: train loss 1.6017, val loss 1.6977\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3707, ROUGE-2: 0.0592, ROUGE-L: 0.2299\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 450: loss 1.6256, time 24527.44ms, mfu 9055.56%\n",
      "iter 455: loss 1.5680, time 7384.16ms, mfu 9582.04%\n",
      "\n",
      "Step 460: train loss 1.5730, val loss 1.6601\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4609, ROUGE-2: 0.1280, ROUGE-L: 0.3371\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 460: loss 1.9352, time 24453.78ms, mfu 9056.26%\n",
      "iter 465: loss 1.4193, time 7388.00ms, mfu 9581.92%\n",
      "\n",
      "Step 470: train loss 1.5163, val loss 1.7286\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4407, ROUGE-2: 0.2325, ROUGE-L: 0.3767\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 470: loss 1.7895, time 24662.36ms, mfu 9052.50%\n",
      "iter 475: loss 1.6976, time 7408.57ms, mfu 9574.56%\n",
      "\n",
      "Step 480: train loss 1.5016, val loss 1.6229\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4181, ROUGE-2: 0.1919, ROUGE-L: 0.2850\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 480: loss 1.5511, time 24254.31ms, mfu 9053.09%\n",
      "iter 485: loss 1.5919, time 7394.76ms, mfu 9577.76%\n",
      "\n",
      "Step 490: train loss 1.5243, val loss 1.7047\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.3534, ROUGE-2: 0.0783, ROUGE-L: 0.2341\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 490: loss 1.4730, time 24842.26ms, mfu 9045.64%\n",
      "iter 495: loss 1.8658, time 7387.38ms, mfu 9572.49%\n",
      "\n",
      "Step 500: train loss 1.4812, val loss 1.6530\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.4001, ROUGE-2: 0.0340, ROUGE-L: 0.2311\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 500: loss 1.4263, time 24847.50ms, mfu 9040.81%\n",
      "\n",
      "训练完成！\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 第五部分：主函数\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数：整合数据准备、训练和评估\n",
    "    \n",
    "    执行流程：\n",
    "    1. 准备数据（如果数据文件不存在）\n",
    "    2. 训练模型\n",
    "    3. 评估模型\n",
    "    \"\"\"\n",
    "    print(\"\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"GPT-2 摘要微调教学脚本\".center(80))\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n当前配置:\")\n",
    "    print(f\"  数据集: {config.dataset_path}\")\n",
    "    print(f\"  模型初始化: {config.init_from}\")\n",
    "    print(f\"  设备: {config.device}\")\n",
    "    print(f\"  批次大小: {config.batch_size}\")\n",
    "    print(f\"  最大迭代次数: {config.max_iters}\")\n",
    "    print(f\"  学习率: {config.learning_rate}\")\n",
    "    \n",
    "    # 步骤1: 准备数据\n",
    "    data_dir = os.path.join('data', config.dataset)\n",
    "    train_pkl = os.path.join(data_dir, 'train.pkl')\n",
    "    \n",
    "    print(\"expect: train_pkl:\",train_pkl)\n",
    "    \n",
    "    if not os.path.exists(train_pkl):\n",
    "        print(\"\\n未找到处理后的数据文件，开始准备数据...\")\n",
    "        prepare_data()\n",
    "    else:\n",
    "        print(\"\\n找到已处理的数据文件，跳过数据准备步骤\")\n",
    "        print(f\"如需重新准备数据，请删除 {data_dir} 目录\")\n",
    "    \n",
    "    # 步骤2: 训练模型\n",
    "    if not config.eval_only:\n",
    "        train()\n",
    "    else:\n",
    "        print(\"\\neval_only=True，跳过训练\")\n",
    "    \n",
    "    \n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19708dea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T08:41:38.294306Z",
     "iopub.status.busy": "2025-11-12T08:41:38.293015Z",
     "iopub.status.idle": "2025-11-12T08:41:38.298310Z",
     "shell.execute_reply": "2025-11-12T08:41:38.297704Z"
    },
    "papermill": {
     "duration": 0.036571,
     "end_time": "2025-11-12T08:41:38.299468",
     "exception": false,
     "start_time": "2025-11-12T08:41:38.262897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 在你的代码文件底部，或者一个新的 cell 中运行\n",
    "# from tqdm import tqdm\n",
    "# import time\n",
    "\n",
    "# # 假设你的数据集已经准备好了\n",
    "# data_path = 'data/samsum/train.pkl' \n",
    "# block_size = 1024\n",
    "# dataset = SummarizationDataset(data_path, block_size)\n",
    "\n",
    "# # 测试前1000个样本的加载时间\n",
    "# total_time = 0\n",
    "# num_samples_to_test = 1000\n",
    "\n",
    "# start_time = time.time()\n",
    "# for i in tqdm(range(num_samples_to_test)):\n",
    "#     x, y = dataset[i]\n",
    "# end_time = time.time()\n",
    "\n",
    "# avg_time = (end_time - start_time) / num_samples_to_test * 1000  # 转换为毫秒\n",
    "# print(f\"\\n平均每个样本的 __getitem__ 耗时: {avg_time:.4f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9bede8",
   "metadata": {
    "papermill": {
     "duration": 0.028134,
     "end_time": "2025-11-12T08:41:38.356561",
     "exception": false,
     "start_time": "2025-11-12T08:41:38.328427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "743331b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T08:41:38.415200Z",
     "iopub.status.busy": "2025-11-12T08:41:38.414900Z",
     "iopub.status.idle": "2025-11-12T08:41:38.440996Z",
     "shell.execute_reply": "2025-11-12T08:41:38.440400Z"
    },
    "papermill": {
     "duration": 0.057088,
     "end_time": "2025-11-12T08:41:38.442130",
     "exception": false,
     "start_time": "2025-11-12T08:41:38.385042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate():\n",
    "    \"\"\"\n",
    "    评估模型性能\n",
    "    \n",
    "    测试流程：\n",
    "    1. 加载训练好的模型\n",
    "    2. 从测试集中读取样本\n",
    "    3. 给定对话，让模型生成摘要（使用KV cache加速）\n",
    "    4. 计算生成摘要与真实摘要的ROUGE分数\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"开始评估...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 加载模型\n",
    "    model, enc, ctx = load_model()\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    # 获取<|endoftext|>的token ID作为停止符\n",
    "    eos_token_id = enc.encode(config.summary_end, allowed_special={config.summary_end})[0]\n",
    "    \n",
    "    # 加载测试数据\n",
    "    print(\"\\n加载测试数据...\")\n",
    "    test_file = os.path.join(config.dataset_path, 'validation.csv')\n",
    "    \n",
    "    dialogues = []\n",
    "    summaries = []\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            if i >= config.num_test_samples:\n",
    "                break\n",
    "            dialogues.append(row['dialogue'])\n",
    "            summaries.append(row['summary'])\n",
    "    \n",
    "    print(f\"加载了 {len(dialogues)} 条测试样本\")\n",
    "    \n",
    "    # 检查是否可以计算ROUGE\n",
    "    rouge_result = calculate_rouge(\"test\", \"test\")\n",
    "    use_rouge = rouge_result is not None\n",
    "    if not use_rouge:\n",
    "        print(\"\\n警告: 未安装rouge_score库，将跳过ROUGE评分\")\n",
    "        print(\"安装命令: pip install rouge-score\")\n",
    "    \n",
    "    # 评估每个样本\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"开始生成和评估...\")\n",
    "    print(\"优化: 使用KV cache + 提前停止\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    all_rouge1_f = []\n",
    "    all_rouge2_f = []\n",
    "    all_rougeL_f = []\n",
    "    \n",
    "    for idx, (dialogue, reference_summary) in enumerate(zip(dialogues, summaries)):\n",
    "        print(f\"\\n[样本 {idx+1}/{len(dialogues)}]\")\n",
    "        print(f\"对话: {dialogue[:100]}...\" if len(dialogue) > 100 else f\"对话: {dialogue}\")\n",
    "        \n",
    "        # 构建prompt（对话 + 摘要开始标记）\n",
    "        prompt = config.dialogue_start + dialogue + config.summary_start\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        prompt_tokens = enc.encode(prompt, allowed_special={config.summary_end})\n",
    "        \n",
    "        # 检查长度\n",
    "        if len(prompt_tokens) > config.block_size - config.max_new_tokens:\n",
    "            print(\"  警告: prompt太长，进行截断\")\n",
    "        \n",
    "        # 使用KV cache加速生成摘要\n",
    "        generated_text = generate_summary(model, prompt_tokens, enc, ctx, eos_token_id=eos_token_id)\n",
    "        \n",
    "        # 提取摘要\n",
    "        generated_summary = extract_summary(generated_text, prompt, enc)\n",
    "        \n",
    "        print(f\"真实摘要: {reference_summary}\")\n",
    "        print(f\"生成摘要: {generated_summary}\")\n",
    "        \n",
    "        # 计算ROUGE分数\n",
    "        if use_rouge:\n",
    "            rouge_scores = calculate_rouge(reference_summary, generated_summary)\n",
    "            if rouge_scores:\n",
    "                rouge1_f = rouge_scores['rouge1']\n",
    "                rouge2_f = rouge_scores['rouge2']\n",
    "                rougeL_f = rouge_scores['rougeL']\n",
    "                \n",
    "                print(f\"ROUGE-1: {rouge1_f:.4f}\")\n",
    "                print(f\"ROUGE-2: {rouge2_f:.4f}\")\n",
    "                print(f\"ROUGE-L: {rougeL_f:.4f}\")\n",
    "                \n",
    "                all_rouge1_f.append(rouge1_f)\n",
    "                all_rouge2_f.append(rouge2_f)\n",
    "                all_rougeL_f.append(rougeL_f)\n",
    "    \n",
    "    # 打印平均分数\n",
    "    if use_rouge and len(all_rouge1_f) > 0:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"平均ROUGE分数:\")\n",
    "        print(f\"  ROUGE-1: {np.mean(all_rouge1_f):.4f}\")\n",
    "        print(f\"  ROUGE-2: {np.mean(all_rouge2_f):.4f}\")\n",
    "        print(f\"  ROUGE-L: {np.mean(all_rougeL_f):.4f}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def predict_test_set_fast():\n",
    "    \"\"\"\n",
    "    使用KV cache加速的测试集推理\n",
    "    \n",
    "    优势：\n",
    "    1. 使用KV cache，避免重复计算，大幅提升速度\n",
    "    2. 设置提前停止符，遇到结束token立即停止\n",
    "    3. 使用我们训练的模型，确保兼容性\n",
    "    \n",
    "    流程：\n",
    "    1. 加载训练好的模型\n",
    "    2. 读取测试集数据\n",
    "    3. 使用KV cache逐样本生成摘要\n",
    "    4. 保存为提交格式\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"开始对测试集进行KV cache加速推理...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 加载模型\n",
    "    model, enc, ctx = load_model()\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    # 获取<|endoftext|>的token ID作为停止符\n",
    "    eos_token_id = enc.encode(config.summary_end, allowed_special={config.summary_end})[0]\n",
    "    \n",
    "    # 读取测试数据\n",
    "    print(\"\\n加载测试数据...\")\n",
    "    test_file = '/kaggle/input/nanogpt-fudannlp-cs-30040/test.csv'\n",
    "    \n",
    "    if not os.path.exists(test_file):\n",
    "        print(f\"错误: 找不到测试文件 {test_file}\")\n",
    "        return\n",
    "    \n",
    "    test_data = []\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            test_data.append({\n",
    "                'id': row['id'],\n",
    "                'dialogue': row['dialogue']\n",
    "            })\n",
    "    \n",
    "    print(f\"加载了 {len(test_data)} 条测试样本\")\n",
    "    \n",
    "    # 准备保存结果\n",
    "    results = []\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"开始生成摘要...\")\n",
    "    print(f\"优化: 使用KV cache + 提前停止 (遇到 '{config.summary_end}' 立即停止)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # 对每个样本生成摘要\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, sample in enumerate(test_data):\n",
    "        sample_id = sample['id']\n",
    "        dialogue = sample['dialogue']\n",
    "        \n",
    "        if (idx + 1) % 10 == 0 or idx == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            if idx > 0:\n",
    "                avg_time = elapsed / idx\n",
    "                remaining = avg_time * (len(test_data) - idx)\n",
    "                print(f\"处理进度: {idx+1}/{len(test_data)} | 平均耗时: {avg_time:.2f}秒/样本 | 预计剩余: {remaining/60:.1f}分钟\")\n",
    "            else:\n",
    "                print(f\"处理进度: {idx+1}/{len(test_data)}\")\n",
    "        \n",
    "        # 构建prompt（对话 + 摘要开始标记）\n",
    "        prompt = config.dialogue_start + dialogue + config.summary_start\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        prompt_tokens = enc.encode(prompt, allowed_special={config.summary_end})\n",
    "        \n",
    "        # 使用KV cache加速生成摘要\n",
    "        generated_text = generate_summary(model, prompt_tokens, enc, ctx, eos_token_id=eos_token_id)\n",
    "        \n",
    "        # 提取摘要\n",
    "        generated_summary = extract_summary(generated_text, prompt, enc)\n",
    "        \n",
    "        results.append({\n",
    "            'id': sample_id,\n",
    "            'summary': generated_summary\n",
    "        })\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n生成完成！总耗时: {total_time/60:.1f}分钟 | 平均: {total_time/len(test_data):.2f}秒/样本\")\n",
    "    \n",
    "    # 保存结果到CSV文件\n",
    "    output_file = 'submission.csv'\n",
    "    output_path = os.path.join(config.out_dir, output_file)\n",
    "    \n",
    "    print(f\"\\n保存结果到 {output_path}\")\n",
    "    with open(output_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['id', 'summary'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "    \n",
    "    print(f\"完成！生成了 {len(results)} 条摘要\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c12e93",
   "metadata": {
    "papermill": {
     "duration": 0.0282,
     "end_time": "2025-11-12T08:41:38.499107",
     "exception": false,
     "start_time": "2025-11-12T08:41:38.470907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 测试evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "345de246",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T08:41:38.557298Z",
     "iopub.status.busy": "2025-11-12T08:41:38.556732Z",
     "iopub.status.idle": "2025-11-12T08:41:52.048085Z",
     "shell.execute_reply": "2025-11-12T08:41:52.047108Z"
    },
    "papermill": {
     "duration": 13.521755,
     "end_time": "2025-11-12T08:41:52.049377",
     "exception": false,
     "start_time": "2025-11-12T08:41:38.527622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\r\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\r\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.3.0)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2025.9.18)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge-score) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge-score) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge-score) (2024.2.0)\r\n",
      "\n",
      "================================================================================\n",
      "开始评估...\n",
      "================================================================================\n",
      "\n",
      "从 out-summarization 加载模型...\n",
      "path out-summarization/ckpt.pt\n",
      "number of parameters: 123.65M\n",
      "编译模型...\n",
      "\n",
      "加载测试数据...\n",
      "加载了 10 条测试样本\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "开始生成和评估...\n",
      "优化: 使用KV cache + 提前停止\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[样本 1/10]\n",
      "对话: Liam: hey do you think you could water my plants while i'm gone this weekend?\n",
      "Sophie: sure, no probl...\n",
      "真实摘要: Liam asked Sophie to water his plants while he was away for the weekend. Sophie agreed and teased him about past plant care mistakes. Liam offered her leftover lasagna as a thank-you.\n",
      "生成摘要: ※亙伔��scertainly moved away and I'm not treated for him for the case\"The person who was challenging for the person※-creefe .\n",
      "ROUGE-1: 0.2182\n",
      "ROUGE-2: 0.0377\n",
      "ROUGE-L: 0.1091\n",
      "\n",
      "[样本 2/10]\n",
      "对话: Zoe: Did you hear about the party this weekend?\n",
      "Luca: Yeah! You going?\n",
      "Zoe: Maybe... you?\n",
      "Luca: Only...\n",
      "真实摘要: Zoe and Luca discuss attending a weekend party together and make plans to go.\n",
      "生成摘要: The crisis※伻伅・劎���企��兌�伙�了7“�id“���> -person-E24-s-based, but and the law was a group of the first-year-censassum※-8※�伻оро��伙�lot of the country, and the most important and their rights of the public opinion of\n",
      "ROUGE-1: 0.1176\n",
      "ROUGE-2: 0.0000\n",
      "ROUGE-L: 0.1176\n",
      "\n",
      "[样本 3/10]\n",
      "对话: Manager: The client presentation is missing the Q3 projections. Where are we with that?\n",
      "Raj: I sent ...\n",
      "真实摘要: Raj discovers he failed to send the Q3 projections to Alicia for a client presentation happening in 3 hours, and his manager steps in to help with formatting the data.\n",
      "生成摘要: Levin’�强弅D, it's sensitive to the crisis“癆-Lase“�_MantRippid-0-Pens, but it's for the wrong, the person's a group of a group of the place of it's passionate for the group of the movie-election-old\n",
      "4-Lively, but for the Lucky-Sare\"I was not for the group for people.\n",
      "ROUGE-1: 0.1163\n",
      "ROUGE-2: 0.0000\n",
      "ROUGE-L: 0.1163\n",
      "\n",
      "[样本 4/10]\n",
      "对话: Nurse: Dr. Reynolds, did you speak with the patient in room 304?\n",
      "Dr. Reynolds: Just finished. Family...\n",
      "真实摘要: Dr. Reynolds missed the family of a patient in room 304 and needs to deliver bad medical news when they return.\n",
      "生成摘要: You're excited about the special-only for the wrong-controlled for the people who were excited-sensitive※-3-based-※�伻о��伆・伆-induced person of the group of their community of the country-group �伆-3: the case“�癨-5-Ruck.\n",
      "The general-based-made it's not only and the media-F2_E-Famous\n",
      "ROUGE-1: 0.0833\n",
      "ROUGE-2: 0.0000\n",
      "ROUGE-L: 0.0833\n",
      "\n",
      "[样本 5/10]\n",
      "对话: Tasha: okay so the wifi is out again\n",
      "Dev: seriously?? it just went down yesterday too\n",
      "Tasha: i know,...\n",
      "真实摘要: Tasha and Dev deal with repeated internet outages and decide to escalate the issue to their landlord if not resolved quickly.\n",
      "生成摘要: The group by the law of the group of the majority was prepared in the group of the group of the group and the crisis‧�※-year-R※-older-government and for the first-TV-old’����伙�吙����伆的�old_S.\n",
      "ROUGE-1: 0.1034\n",
      "ROUGE-2: 0.0000\n",
      "ROUGE-L: 0.1034\n",
      "\n",
      "[样本 6/10]\n",
      "对话: Dad: hey sweetie u eat yet\n",
      "Lena: not yet been swamped w homework\n",
      "Dad: i made some soup… save u some?...\n",
      "真实摘要: Lena had not eaten because she was doing homework. Her dad offered her leftover soup he had made, and promised to save it for her. Lena appreciated his kindness.\n",
      "生成摘要: The person in the wrong from the movie“克朜����伆叙伆-scally of the place for the following by the first is to be sensitive※�-minded about the important to be a crisis��伔�弙�������伆-7※�伆伆-3-old and other youngness-Lived-censased by the new living-censa-\n",
      "ROUGE-1: 0.0833\n",
      "ROUGE-2: 0.0000\n",
      "ROUGE-L: 0.0278\n",
      "\n",
      "[样本 7/10]\n",
      "对话: Nina: OMG guess who just texted me?\n",
      "Leo: Who?\n",
      "Nina: Jason from high school!\n",
      "Leo: The guy you had a c...\n",
      "真实摘要: Nina tells Leo her high school crush Jason contacted her and asked if she’s single. She’s nervous but considering meeting him.\n",
      "生成摘要: ‡-E-�§��伆、晙伙�-government-old group are concerned�old group of the new-personated by the number of the group's “劙��五�_Fidelmied and the person. They're important group for the exception with a series‌少н伙伻�Ridelic-Loyal※�old„企�伎伊�\n",
      "ROUGE-1: 0.0678\n",
      "ROUGE-2: 0.0000\n",
      "ROUGE-L: 0.0339\n",
      "\n",
      "[样本 8/10]\n",
      "对话: Priya: are you free this weekend? i need help moving\n",
      "Aiden: ugh… how much stuff?\n",
      "Priya: just a couch...\n",
      "真实摘要: Priya recruits Aiden to help her move in exchange for pizza. They settle on Saturday morning.\n",
      "生成摘要: Fumad“��伻杨传企�lotreadad‎天兙癆�Lääää※���伙�-Focating an interesting to be an active-Loyally-country-TVL_category-E, and the law's a person—��会�‏�伎伨・虆尙����old※�\n",
      "ROUGE-1: 0.0526\n",
      "ROUGE-2: 0.0000\n",
      "ROUGE-L: 0.0526\n",
      "\n",
      "[样本 9/10]\n",
      "对话: Boss: Did you finish the client report?\n",
      "Sophia: Almost done! Just need to proofread.\n",
      "Boss: It was du...\n",
      "真实摘要: Sophia apologized for missing the client report deadline due to a family emergency and promised to submit it by 3pm. Her boss granted an extension.\n",
      "生成摘要: The person who was a long-person-18-judied for better-R-old※��伙伆-Massing the majority for the other matters that's※伎�噆・伆 of the country for a person who was important for the newness-scoded for the public decision to be responsible for the view�‮����伊圻-old-the group\"The most important organizations, there\n",
      "ROUGE-1: 0.1026\n",
      "ROUGE-2: 0.0000\n",
      "ROUGE-L: 0.1026\n",
      "\n",
      "[样本 10/10]\n",
      "对话: Zoe: Did you just eat the last of my yogurt?\n",
      "Noah: Maybe.\n",
      "Zoe: Noah.\n",
      "Noah: I was hungry! I’ll buy yo...\n",
      "真实摘要: Noah ate Zoe’s last yogurt, prompting her to demand replacement fancy yogurt and a muffin as compensation.\n",
      "生成摘要: D-Laded in the wrong.\n",
      "ROUGE-1: 0.0000\n",
      "ROUGE-2: 0.0000\n",
      "ROUGE-L: 0.0000\n",
      "\n",
      "================================================================================\n",
      "平均ROUGE分数:\n",
      "  ROUGE-1: 0.0945\n",
      "  ROUGE-2: 0.0038\n",
      "  ROUGE-L: 0.0747\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28bd428a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T08:41:52.194392Z",
     "iopub.status.busy": "2025-11-12T08:41:52.194061Z",
     "iopub.status.idle": "2025-11-12T09:05:28.165643Z",
     "shell.execute_reply": "2025-11-12T09:05:28.164965Z"
    },
    "papermill": {
     "duration": 1416.003113,
     "end_time": "2025-11-12T09:05:28.166819",
     "exception": false,
     "start_time": "2025-11-12T08:41:52.163706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "开始对测试集进行KV cache加速推理...\n",
      "================================================================================\n",
      "\n",
      "从 out-summarization 加载模型...\n",
      "path out-summarization/ckpt.pt\n",
      "number of parameters: 123.65M\n",
      "编译模型...\n",
      "\n",
      "加载测试数据...\n",
      "加载了 2273 条测试样本\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "开始生成摘要...\n",
      "优化: 使用KV cache + 提前停止 (遇到 '<|endoftext|>' 立即停止)\n",
      "--------------------------------------------------------------------------------\n",
      "处理进度: 1/2273\n",
      "处理进度: 10/2273 | 平均耗时: 0.69秒/样本 | 预计剩余: 26.0分钟\n",
      "处理进度: 20/2273 | 平均耗时: 0.65秒/样本 | 预计剩余: 24.5分钟\n",
      "处理进度: 30/2273 | 平均耗时: 0.63秒/样本 | 预计剩余: 23.5分钟\n",
      "处理进度: 40/2273 | 平均耗时: 0.60秒/样本 | 预计剩余: 22.4分钟\n",
      "处理进度: 50/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 23.0分钟\n",
      "处理进度: 60/2273 | 平均耗时: 0.63秒/样本 | 预计剩余: 23.1分钟\n",
      "处理进度: 70/2273 | 平均耗时: 0.64秒/样本 | 预计剩余: 23.4分钟\n",
      "处理进度: 80/2273 | 平均耗时: 0.63秒/样本 | 预计剩余: 23.2分钟\n",
      "处理进度: 90/2273 | 平均耗时: 0.63秒/样本 | 预计剩余: 22.9分钟\n",
      "处理进度: 100/2273 | 平均耗时: 0.63秒/样本 | 预计剩余: 22.8分钟\n",
      "处理进度: 110/2273 | 平均耗时: 0.63秒/样本 | 预计剩余: 22.7分钟\n",
      "处理进度: 120/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 22.4分钟\n",
      "处理进度: 130/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 22.1分钟\n",
      "处理进度: 140/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 22.2分钟\n",
      "处理进度: 150/2273 | 平均耗时: 0.63秒/样本 | 预计剩余: 22.3分钟\n",
      "处理进度: 160/2273 | 平均耗时: 0.63秒/样本 | 预计剩余: 22.2分钟\n",
      "处理进度: 170/2273 | 平均耗时: 0.63秒/样本 | 预计剩余: 22.0分钟\n",
      "处理进度: 180/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 21.7分钟\n",
      "处理进度: 190/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 21.7分钟\n",
      "处理进度: 200/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 21.6分钟\n",
      "处理进度: 210/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 21.3分钟\n",
      "处理进度: 220/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 21.2分钟\n",
      "处理进度: 230/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 21.0分钟\n",
      "处理进度: 240/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 21.0分钟\n",
      "处理进度: 250/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 20.9分钟\n",
      "处理进度: 260/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 20.7分钟\n",
      "处理进度: 270/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 20.7分钟\n",
      "处理进度: 280/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 20.6分钟\n",
      "处理进度: 290/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 20.5分钟\n",
      "处理进度: 300/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 20.3分钟\n",
      "处理进度: 310/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 20.2分钟\n",
      "处理进度: 320/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 20.0分钟\n",
      "处理进度: 330/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 19.9分钟\n",
      "处理进度: 340/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 19.9分钟\n",
      "处理进度: 350/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 19.7分钟\n",
      "处理进度: 360/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 19.5分钟\n",
      "处理进度: 370/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 19.5分钟\n",
      "处理进度: 380/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 19.4分钟\n",
      "处理进度: 390/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 19.3分钟\n",
      "处理进度: 400/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 19.1分钟\n",
      "处理进度: 410/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 19.0分钟\n",
      "处理进度: 420/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 18.9分钟\n",
      "处理进度: 430/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 18.8分钟\n",
      "处理进度: 440/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 18.6分钟\n",
      "处理进度: 450/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 18.6分钟\n",
      "处理进度: 460/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 18.5分钟\n",
      "处理进度: 470/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 18.3分钟\n",
      "处理进度: 480/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 18.2分钟\n",
      "处理进度: 490/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 18.0分钟\n",
      "处理进度: 500/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 17.9分钟\n",
      "处理进度: 510/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 17.8分钟\n",
      "处理进度: 520/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 17.7分钟\n",
      "处理进度: 530/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 17.6分钟\n",
      "处理进度: 540/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 17.5分钟\n",
      "处理进度: 550/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 17.4分钟\n",
      "处理进度: 560/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 17.4分钟\n",
      "处理进度: 570/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 17.3分钟\n",
      "处理进度: 580/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 17.2分钟\n",
      "处理进度: 590/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 17.1分钟\n",
      "处理进度: 600/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 17.1分钟\n",
      "处理进度: 610/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 17.0分钟\n",
      "处理进度: 620/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 16.8分钟\n",
      "处理进度: 630/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 16.8分钟\n",
      "处理进度: 640/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 16.7分钟\n",
      "处理进度: 650/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 16.6分钟\n",
      "处理进度: 660/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 16.5分钟\n",
      "处理进度: 670/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 16.4分钟\n",
      "处理进度: 680/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 16.3分钟\n",
      "处理进度: 690/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 16.2分钟\n",
      "处理进度: 700/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 16.1分钟\n",
      "处理进度: 710/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 16.0分钟\n",
      "处理进度: 720/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 15.9分钟\n",
      "处理进度: 730/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 15.8分钟\n",
      "处理进度: 740/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 15.7分钟\n",
      "处理进度: 750/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 15.6分钟\n",
      "处理进度: 760/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 15.5分钟\n",
      "处理进度: 770/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 15.4分钟\n",
      "处理进度: 780/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 15.3分钟\n",
      "处理进度: 790/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 15.1分钟\n",
      "处理进度: 800/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 15.0分钟\n",
      "处理进度: 810/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 15.0分钟\n",
      "处理进度: 820/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 14.8分钟\n",
      "处理进度: 830/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 14.8分钟\n",
      "处理进度: 840/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 14.7分钟\n",
      "处理进度: 850/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 14.6分钟\n",
      "处理进度: 860/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 14.5分钟\n",
      "处理进度: 870/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 14.4分钟\n",
      "处理进度: 880/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 14.2分钟\n",
      "处理进度: 890/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 14.1分钟\n",
      "处理进度: 900/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 14.1分钟\n",
      "处理进度: 910/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 14.0分钟\n",
      "处理进度: 920/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 13.8分钟\n",
      "处理进度: 930/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 13.7分钟\n",
      "处理进度: 940/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 13.6分钟\n",
      "处理进度: 950/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 13.6分钟\n",
      "处理进度: 960/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 13.4分钟\n",
      "处理进度: 970/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 13.3分钟\n",
      "处理进度: 980/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 13.2分钟\n",
      "处理进度: 990/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 13.1分钟\n",
      "处理进度: 1000/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 13.0分钟\n",
      "处理进度: 1010/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 13.0分钟\n",
      "处理进度: 1020/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 12.9分钟\n",
      "处理进度: 1030/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 12.8分钟\n",
      "处理进度: 1040/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 12.7分钟\n",
      "处理进度: 1050/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 12.6分钟\n",
      "处理进度: 1060/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 12.5分钟\n",
      "处理进度: 1070/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 12.4分钟\n",
      "处理进度: 1080/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 12.3分钟\n",
      "处理进度: 1090/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 12.2分钟\n",
      "处理进度: 1100/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 12.1分钟\n",
      "处理进度: 1110/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 11.9分钟\n",
      "处理进度: 1120/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 11.8分钟\n",
      "处理进度: 1130/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 11.7分钟\n",
      "处理进度: 1140/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 11.6分钟\n",
      "处理进度: 1150/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 11.5分钟\n",
      "处理进度: 1160/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 11.4分钟\n",
      "处理进度: 1170/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 11.3分钟\n",
      "处理进度: 1180/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 11.2分钟\n",
      "处理进度: 1190/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 11.1分钟\n",
      "处理进度: 1200/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 11.0分钟\n",
      "处理进度: 1210/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 10.9分钟\n",
      "处理进度: 1220/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 10.8分钟\n",
      "处理进度: 1230/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 10.7分钟\n",
      "处理进度: 1240/2273 | 平均耗时: 0.61秒/样本 | 预计剩余: 10.6分钟\n",
      "处理进度: 1250/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 10.5分钟\n",
      "处理进度: 1260/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 10.4分钟\n",
      "处理进度: 1270/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 10.3分钟\n",
      "处理进度: 1280/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 10.2分钟\n",
      "处理进度: 1290/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 10.1分钟\n",
      "处理进度: 1300/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 10.0分钟\n",
      "处理进度: 1310/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 9.9分钟\n",
      "处理进度: 1320/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 9.8分钟\n",
      "处理进度: 1330/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 9.7分钟\n",
      "处理进度: 1340/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 9.6分钟\n",
      "处理进度: 1350/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 9.5分钟\n",
      "处理进度: 1360/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 9.4分钟\n",
      "处理进度: 1370/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 9.3分钟\n",
      "处理进度: 1380/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 9.2分钟\n",
      "处理进度: 1390/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 9.1分钟\n",
      "处理进度: 1400/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 9.0分钟\n",
      "处理进度: 1410/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 8.9分钟\n",
      "处理进度: 1420/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 8.8分钟\n",
      "处理进度: 1430/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 8.7分钟\n",
      "处理进度: 1440/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 8.6分钟\n",
      "处理进度: 1450/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 8.5分钟\n",
      "处理进度: 1460/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 8.4分钟\n",
      "处理进度: 1470/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 8.3分钟\n",
      "处理进度: 1480/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 8.1分钟\n",
      "处理进度: 1490/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 8.1分钟\n",
      "处理进度: 1500/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 8.0分钟\n",
      "处理进度: 1510/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 7.9分钟\n",
      "处理进度: 1520/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 7.8分钟\n",
      "处理进度: 1530/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 7.6分钟\n",
      "处理进度: 1540/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 7.5分钟\n",
      "处理进度: 1550/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 7.4分钟\n",
      "处理进度: 1560/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 7.3分钟\n",
      "处理进度: 1570/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 7.2分钟\n",
      "处理进度: 1580/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 7.1分钟\n",
      "处理进度: 1590/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 7.0分钟\n",
      "处理进度: 1600/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 6.9分钟\n",
      "处理进度: 1610/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 6.8分钟\n",
      "处理进度: 1620/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 6.7分钟\n",
      "处理进度: 1630/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 6.6分钟\n",
      "处理进度: 1640/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 6.5分钟\n",
      "处理进度: 1650/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 6.4分钟\n",
      "处理进度: 1660/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 6.3分钟\n",
      "处理进度: 1670/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 6.2分钟\n",
      "处理进度: 1680/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 6.1分钟\n",
      "处理进度: 1690/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 6.0分钟\n",
      "处理进度: 1700/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 5.9分钟\n",
      "处理进度: 1710/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 5.8分钟\n",
      "处理进度: 1720/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 5.7分钟\n",
      "处理进度: 1730/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 5.6分钟\n",
      "处理进度: 1740/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 5.5分钟\n",
      "处理进度: 1750/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 5.4分钟\n",
      "处理进度: 1760/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 5.3分钟\n",
      "处理进度: 1770/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 5.2分钟\n",
      "处理进度: 1780/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 5.1分钟\n",
      "处理进度: 1790/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 5.0分钟\n",
      "处理进度: 1800/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 4.9分钟\n",
      "处理进度: 1810/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 4.8分钟\n",
      "处理进度: 1820/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 4.7分钟\n",
      "处理进度: 1830/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 4.6分钟\n",
      "处理进度: 1840/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 4.5分钟\n",
      "处理进度: 1850/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 4.4分钟\n",
      "处理进度: 1860/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 4.3分钟\n",
      "处理进度: 1870/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 4.2分钟\n",
      "处理进度: 1880/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 4.1分钟\n",
      "处理进度: 1890/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 4.0分钟\n",
      "处理进度: 1900/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 3.9分钟\n",
      "处理进度: 1910/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 3.8分钟\n",
      "处理进度: 1920/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 3.7分钟\n",
      "处理进度: 1930/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 3.5分钟\n",
      "处理进度: 1940/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 3.4分钟\n",
      "处理进度: 1950/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 3.3分钟\n",
      "处理进度: 1960/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 3.2分钟\n",
      "处理进度: 1970/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 3.1分钟\n",
      "处理进度: 1980/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 3.0分钟\n",
      "处理进度: 1990/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 2.9分钟\n",
      "处理进度: 2000/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 2.8分钟\n",
      "处理进度: 2010/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 2.7分钟\n",
      "处理进度: 2020/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 2.6分钟\n",
      "处理进度: 2030/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 2.5分钟\n",
      "处理进度: 2040/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 2.4分钟\n",
      "处理进度: 2050/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 2.3分钟\n",
      "处理进度: 2060/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 2.2分钟\n",
      "处理进度: 2070/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 2.1分钟\n",
      "处理进度: 2080/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 2.0分钟\n",
      "处理进度: 2090/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 1.9分钟\n",
      "处理进度: 2100/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 1.8分钟\n",
      "处理进度: 2110/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 1.7分钟\n",
      "处理进度: 2120/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 1.6分钟\n",
      "处理进度: 2130/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 1.5分钟\n",
      "处理进度: 2140/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 1.4分钟\n",
      "处理进度: 2150/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 1.3分钟\n",
      "处理进度: 2160/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 1.2分钟\n",
      "处理进度: 2170/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 1.1分钟\n",
      "处理进度: 2180/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 1.0分钟\n",
      "处理进度: 2190/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 0.9分钟\n",
      "处理进度: 2200/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 0.8分钟\n",
      "处理进度: 2210/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 0.7分钟\n",
      "处理进度: 2220/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 0.6分钟\n",
      "处理进度: 2230/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 0.5分钟\n",
      "处理进度: 2240/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 0.4分钟\n",
      "处理进度: 2250/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 0.2分钟\n",
      "处理进度: 2260/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 0.1分钟\n",
      "处理进度: 2270/2273 | 平均耗时: 0.62秒/样本 | 预计剩余: 0.0分钟\n",
      "\n",
      "生成完成！总耗时: 23.5分钟 | 平均: 0.62秒/样本\n",
      "\n",
      "保存结果到 out-summarization/submission.csv\n",
      "完成！生成了 2273 条摘要\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'out-summarization/submission.csv'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test_set_fast()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14409389,
     "isSourceIdPinned": false,
     "sourceId": 120001,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6035.267577,
   "end_time": "2025-11-12T09:05:31.124300",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-12T07:24:55.856723",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00be215c662b4391b41b45ab45477b87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4f72b353f82c4c5e9147fb32c14bbb7a",
       "max": 665.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_36303814a327490ebf364afe09dab41d",
       "tabbable": null,
       "tooltip": null,
       "value": 665.0
      }
     },
     "011d2cf9737c45dbafbdf62af16e6cc2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7258e74c3ac84d549670b6365daf3a52",
       "placeholder": "​",
       "style": "IPY_MODEL_e9b379710bfa4d328b798f8c3b239523",
       "tabbable": null,
       "tooltip": null,
       "value": "generation_config.json: 100%"
      }
     },
     "080d1546a1b74f0ebcbf484ae4cc314c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "11271a6f4a3749abb5e158cd0f5fab5f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1132837d40554fb5bbdcd579ccf547d1",
       "placeholder": "​",
       "style": "IPY_MODEL_aff8c2f6b91d4b8a85bd0244ac17390d",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "1132837d40554fb5bbdcd579ccf547d1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "13d03e9308a64b6c867d6f0ad6893788": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1777b19a2dc24e809542ddca04238f84": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "36303814a327490ebf364afe09dab41d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4ab9e739da64477aa40de5a86e662ab0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6a7a478ea196476982f7e62c4480da07",
       "placeholder": "​",
       "style": "IPY_MODEL_ab4bcb15a9834638b0b61516583e266b",
       "tabbable": null,
       "tooltip": null,
       "value": " 124/124 [00:00&lt;00:00, 17.2kB/s]"
      }
     },
     "4f72b353f82c4c5e9147fb32c14bbb7a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "566a553019b043df88d467c23bedb312": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6a7a478ea196476982f7e62c4480da07": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6af89e477eca4b50861aca82c2697321": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1777b19a2dc24e809542ddca04238f84",
       "max": 124.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_768c6fd280df44af91afabb89d03bb9d",
       "tabbable": null,
       "tooltip": null,
       "value": 124.0
      }
     },
     "7258e74c3ac84d549670b6365daf3a52": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7391e7372f9541448b3d5262523179f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_011d2cf9737c45dbafbdf62af16e6cc2",
        "IPY_MODEL_6af89e477eca4b50861aca82c2697321",
        "IPY_MODEL_4ab9e739da64477aa40de5a86e662ab0"
       ],
       "layout": "IPY_MODEL_d47d0fb4369149a1b14070d11c3f0466",
       "tabbable": null,
       "tooltip": null
      }
     },
     "759798c1b6c94fdfb34910b05396c7b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "768c6fd280df44af91afabb89d03bb9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "97224c2b30b246a6a0257147d39508d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "98312a34b3ca4c8fbb664dd9dd79add0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_566a553019b043df88d467c23bedb312",
       "max": 548105171.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_97224c2b30b246a6a0257147d39508d7",
       "tabbable": null,
       "tooltip": null,
       "value": 548105171.0
      }
     },
     "a23fb1d371f444caa70c80f805b97221": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a3700a0206e64b72b60bdff8e6b448aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ab4bcb15a9834638b0b61516583e266b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "aff8c2f6b91d4b8a85bd0244ac17390d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b7977fa897fb457d9e33580ab238769d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a23fb1d371f444caa70c80f805b97221",
       "placeholder": "​",
       "style": "IPY_MODEL_da55734b1e5447bf8a4440034fed264a",
       "tabbable": null,
       "tooltip": null,
       "value": " 548M/548M [00:01&lt;00:00, 389MB/s]"
      }
     },
     "bb05841e4e054e1bb94dfe85bab12629": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d36555e67d184c4cad8274fdb1720361": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f54cfe2da1a24e7589bf18a55a64343a",
        "IPY_MODEL_00be215c662b4391b41b45ab45477b87",
        "IPY_MODEL_df22fb7d90844442ada9531ae0850082"
       ],
       "layout": "IPY_MODEL_bb05841e4e054e1bb94dfe85bab12629",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d47d0fb4369149a1b14070d11c3f0466": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "da55734b1e5447bf8a4440034fed264a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dba2cf07c66141498acaa94eb4f0ad69": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dd929fec79694a84849f18ba619c49eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_11271a6f4a3749abb5e158cd0f5fab5f",
        "IPY_MODEL_98312a34b3ca4c8fbb664dd9dd79add0",
        "IPY_MODEL_b7977fa897fb457d9e33580ab238769d"
       ],
       "layout": "IPY_MODEL_759798c1b6c94fdfb34910b05396c7b8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "df22fb7d90844442ada9531ae0850082": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_13d03e9308a64b6c867d6f0ad6893788",
       "placeholder": "​",
       "style": "IPY_MODEL_a3700a0206e64b72b60bdff8e6b448aa",
       "tabbable": null,
       "tooltip": null,
       "value": " 665/665 [00:00&lt;00:00, 75.5kB/s]"
      }
     },
     "e9b379710bfa4d328b798f8c3b239523": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f54cfe2da1a24e7589bf18a55a64343a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_dba2cf07c66141498acaa94eb4f0ad69",
       "placeholder": "​",
       "style": "IPY_MODEL_080d1546a1b74f0ebcbf484ae4cc314c",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
