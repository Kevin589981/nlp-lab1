{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 步骤 0: 环境安装\n",
    "\n",
    " 首先，安装运行此项目所需的库。\n",
    "\n",
    " 在 Kaggle TPU VM 环境中，torch_xla 通常是预装的。\n",
    "\n",
    " transformers, datasets, 和 rouge-score 是我们完成任务所需要的核心库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.35.2\" \"datasets==2.15.0\" \"accelerate==0.24.1\" \"rouge-score==0.1.2\" \"pandas\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 步骤 1: 数据准备\n",
    "\n",
    " 这部分代码与您提供的原始代码逻辑保持一致。\n",
    "\n",
    " 它会从 Kaggle 输入目录读取 `train.csv`，将其随机划分为 95% 的训练集和 5% 的验证集，\n",
    "\n",
    " 并将它们保存到工作目录中，以便后续的模型训练和评估使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"步骤 1: 准备 SAMSum 数据集划分\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 定义输入和输出路径\n",
    "# 注意：请确保在Kaggle的 \"Add data\" 中添加了相应的数据集\n",
    "INPUT_DATA_DIR = '/kaggle/input/nanogpt-fudannlp-cs-30040'\n",
    "OUTPUT_DATA_DIR = '/kaggle/working/data/samsum'\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(OUTPUT_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# 检查原始数据是否存在\n",
    "train_csv_path_orig = os.path.join(INPUT_DATA_DIR, 'train.csv')\n",
    "if not os.path.exists(train_csv_path_orig):\n",
    "    raise FileNotFoundError(f\"错误：未在 {INPUT_DATA_DIR} 中找到 train.csv。请确保已将比赛数据集添加到 notebook 中。\")\n",
    "\n",
    "print(f\"\\n读取原始数据: {train_csv_path_orig}\")\n",
    "df = pd.read_csv(train_csv_path_orig)\n",
    "\n",
    "# 随机打乱数据\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# --- (可选) 快速测试 ---\n",
    "# 如果需要快速测试，可以取消下面一行的注释，仅使用少量数据\n",
    "df = df[:200]\n",
    "# -------------------------\n",
    "\n",
    "total_samples = len(df)\n",
    "print(f\"总样本数: {total_samples}\")\n",
    "\n",
    "# 计算划分点 (5% 作为验证集)\n",
    "val_size = int(total_samples * 0.05)\n",
    "train_size = total_samples - val_size\n",
    "\n",
    "print(f\"训练集样本数: {train_size} ({train_size/total_samples*100:.1f}%)\")\n",
    "print(f\"验证集样本数: {val_size} ({val_size/total_samples*100:.1f}%)\")\n",
    "\n",
    "# 划分数据\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:]\n",
    "\n",
    "# 保存训练集\n",
    "train_csv_path = os.path.join(OUTPUT_DATA_DIR, 'train.csv')\n",
    "train_df.to_csv(train_csv_path, index=False)\n",
    "print(f\"\\n训练集已保存: {train_csv_path}\")\n",
    "\n",
    "# 保存验证集\n",
    "val_csv_path = os.path.join(OUTPUT_DATA_DIR, 'validation.csv')\n",
    "val_df.to_csv(val_csv_path, index=False)\n",
    "print(f\"验证集已保存: {val_csv_path}\")\n",
    "\n",
    "print(\"\\n数据集划分完成！\")\n",
    "print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 步骤 2: 配置训练参数\n",
    "\n",
    " 在这里，我们定义所有与模型、训练、评估相关的超参数。\n",
    "\n",
    "\n",
    "\n",
    " - **Model Arguments**: 指定要使用的模型，例如 `google/pegasus-base`。\n",
    "\n",
    " - **Data Arguments**: 定义数据路径和处理相关的参数，如最大序列长度。\n",
    "\n",
    " - **Training Arguments**: 控制训练过程的核心参数，包括学习率、批次大小、保存策略、混合精度等。\n",
    "\n",
    "   - `eval_only`: 如果设为 `True`，脚本将只加载模型并进行评估，不会进行训练。\n",
    "\n",
    "   - `resume_from_checkpoint`: 如果提供一个检查点路径，将从该断点继续训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Args:\n",
    "    # =========================================================================\n",
    "    # 核心配置 (Core Configuration)\n",
    "    # =========================================================================\n",
    "    model_name_or_path: str = 'google/pegasus-base'\n",
    "    output_dir: str = '/kaggle/working/pegasus-samsum-finetuned'\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 数据配置 (Data Configuration)\n",
    "    # =========================================================================\n",
    "    train_file: str = os.path.join(OUTPUT_DATA_DIR, 'train.csv')\n",
    "    validation_file: str = os.path.join(OUTPUT_DATA_DIR, 'validation.csv')\n",
    "    test_file: str = os.path.join(INPUT_DATA_DIR, 'test.csv') # 用于最终提交\n",
    "    max_source_length: int = 512   # 输入对话的最大长度\n",
    "    max_target_length: int = 128   # 输出摘要的最大长度\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 训练与评估配置 (Training & Evaluation Configuration)\n",
    "    # =========================================================================\n",
    "    # --- 模式控制 ---\n",
    "    do_train: bool = True\n",
    "    do_eval: bool = True\n",
    "    eval_only: bool = False # 设为 True 则只进行评估\n",
    "    resume_from_checkpoint: str = None # 例如: '/path/to/checkpoint'\n",
    "    \n",
    "    # --- 训练超参数 ---\n",
    "    num_train_epochs: int = 3\n",
    "    per_device_train_batch_size: int = 4 # 可根据显存/内存调整\n",
    "    per_device_eval_batch_size: int = 8\n",
    "    gradient_accumulation_steps: int = 8 # 有效批大小 = batch_size * a_steps * num_devices\n",
    "    learning_rate: float = 5e-5\n",
    "    weight_decay: float = 0.01\n",
    "    lr_scheduler_type: str = 'linear'\n",
    "    warmup_steps: int = 50\n",
    "    \n",
    "    # --- 混合精度 ---\n",
    "    # TPU 通常使用 bfloat16。Tesla T4 支持 bfloat16。P100 仅支持 float16。\n",
    "    # 设置为 'bf16' 或 'fp16'。设为 None 则使用 fp32。\n",
    "    mixed_precision: str = 'bf16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'fp16'\n",
    "    \n",
    "    # --- 日志与保存 ---\n",
    "    logging_steps: int = 25\n",
    "    eval_steps: int = 100 # 每N步在验证集上评估一次\n",
    "    save_steps: int = 100\n",
    "    save_total_limit: int = 2 # 最多保留几个检查点\n",
    "    \n",
    "    # --- 生成参数 (用于评估和预测) ---\n",
    "    num_beams: int = 4\n",
    "    \n",
    "    # --- 随机种子 ---\n",
    "    seed: int = 42\n",
    "\n",
    "# 实例化配置\n",
    "TRAINING_ARGS = Args()\n",
    "\n",
    "# 如果 eval_only 为 True，则禁用训练\n",
    "if TRAINING_ARGS.eval_only:\n",
    "    TRAINING_ARGS.do_train = False\n",
    "    # 如果只评估，通常需要指定一个模型断点来加载\n",
    "    if TRAINING_ARGS.resume_from_checkpoint is None:\n",
    "        print(\"警告: 'eval_only' 为 True，但未指定 'resume_from_checkpoint'。将尝试从 'output_dir' 加载模型。\")\n",
    "        TRAINING_ARGS.resume_from_checkpoint = TRAINING_ARGS.output_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 步骤 3: 模型、分词器和数据处理\n",
    "\n",
    " 在这个部分，我们：\n",
    "\n",
    " 1.  加载预训练的 PEGASUS 模型和对应的分词器 (Tokenizer)。\n",
    "\n",
    " 2.  使用 `datasets` 库加载我们之前划分好的 CSV 文件。\n",
    "\n",
    " 3.  定义一个预处理函数，它会将对话和摘要文本转换成模型所需的 `input_ids`, `attention_mask` 和 `labels` 格式。\n",
    "\n",
    "     - **重要**: padding token 在 `labels` 中会被替换为 -100，这样它们就不会对损失计算产生影响。\n",
    "\n",
    " 4.  应用这个预处理函数到我们的数据集上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# 尝试下载nltk工具包，如果失败则继续\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"步骤 3: 加载模型、分词器和处理数据\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# --- 1. 加载模型和分词器 ---\n",
    "print(f\"加载模型配置: {TRAINING_ARGS.model_name_or_path}\")\n",
    "config = AutoConfig.from_pretrained(TRAINING_ARGS.model_name_or_path)\n",
    "\n",
    "print(f\"加载分词器: {TRAINING_ARGS.model_name_or_path}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRAINING_ARGS.model_name_or_path)\n",
    "\n",
    "print(f\"加载Seq2Seq模型: {TRAINING_ARGS.model_name_or_path}\")\n",
    "# 根据 resume_from_checkpoint 参数决定是加载预训练模型还是本地断点\n",
    "model_load_path = TRAINING_ARGS.resume_from_checkpoint if TRAINING_ARGS.resume_from_checkpoint and os.path.exists(TRAINING_ARGS.resume_from_checkpoint) else TRAINING_ARGS.model_name_or_path\n",
    "print(f\"模型加载路径: {model_load_path}\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_load_path, config=config)\n",
    "\n",
    "# --- 2. 加载数据集 ---\n",
    "data_files = {}\n",
    "if TRAINING_ARGS.do_train:\n",
    "    data_files[\"train\"] = TRAINING_ARGS.train_file\n",
    "if TRAINING_ARGS.do_eval:\n",
    "    data_files[\"validation\"] = TRAINING_ARGS.validation_file\n",
    "\n",
    "raw_datasets = load_dataset(\"csv\", data_files=data_files, cache_dir='/kaggle/working/cache')\n",
    "print(f\"\\n加载的数据集: {raw_datasets}\")\n",
    "\n",
    "\n",
    "# --- 3. 定义预处理函数 ---\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"dialogue\"]\n",
    "    targets = examples[\"summary\"]\n",
    "    \n",
    "    # 对输入进行分词\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=TRAINING_ARGS.max_source_length, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # 对目标（摘要）进行分词\n",
    "    # 使用 `text_target` 参数来为解码器进行分词\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets, \n",
    "            max_length=TRAINING_ARGS.max_target_length, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    # **Loss掩码**: 将label中由padding产生的位置替换为-100，这样损失函数会忽略它们\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# --- 4. 应用预处理 ---\n",
    "if TRAINING_ARGS.do_train:\n",
    "    train_dataset = raw_datasets[\"train\"]\n",
    "    train_dataset = train_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names,\n",
    "        desc=\"处理训练数据集\",\n",
    "    )\n",
    "    print(f\"\\n处理后的训练样本 (一个): {train_dataset[0]}\")\n",
    "\n",
    "if TRAINING_ARGS.do_eval:\n",
    "    eval_dataset = raw_datasets[\"validation\"]\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=eval_dataset.column_names,\n",
    "        desc=\"处理验证数据集\",\n",
    "    )\n",
    "    print(f\"处理后的验证样本 (一个): {eval_dataset[0]}\")\n",
    "\n",
    "print(\"\\n数据处理完成!\")\n",
    "print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 步骤 4: 训练与评估\n",
    "\n",
    " 这个部分是整个流程的核心。我们执行以下操作：\n",
    "\n",
    " 1.  **检测环境**: 判断当前是在 TPU 还是 GPU 环境下运行。Kaggle TPU Notebook 会设置 `TPU_NAME` 环境变量。\n",
    "\n",
    " 2.  **设置训练参数**:\n",
    "\n",
    "     - **TPU**: 使用 `torch_xla` 要求的特殊配置，如 `tpu_num_cores`。\n",
    "\n",
    "     - **GPU**: 使用标准的 `Seq2SeqTrainingArguments`。\n",
    "\n",
    " 3.  **定义评估指标**: 创建一个函数 `compute_metrics`，用于在评估过程中计算 ROUGE 分数。这是评估模型摘要质量的关键指标。\n",
    "\n",
    " 4.  **初始化 `Trainer`**: `Seq2SeqTrainer` 是 Hugging Face 提供的一个强大的工具，它封装了训练和评估的循环。我们传入模型、参数、数据集和评估函数来初始化它。\n",
    "\n",
    " 5.  **开始训练/评估**:\n",
    "\n",
    "     - 如果 `do_train` 为 `True`，调用 `trainer.train()`。\n",
    "\n",
    "     - 如果 `do_eval` 为 `True`，调用 `trainer.evaluate()`。\n",
    "\n",
    " 6.  **保存结果**: 训练完成后，模型、分词器和训练状态会自动保存在 `output_dir` 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. 定义评估指标计算函数 ---\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # 将 -100 替换回 pad_token_id\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # 解码预测和标签\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # 使用nltk进行分词，为ROUGE计算做准备\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # 计算 ROUGE 分数\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)\n",
    "    \n",
    "    result = {\n",
    "        'rouge1': [], 'rouge2': [], 'rougeL': [], 'rougeLsum': [],\n",
    "    }\n",
    "    \n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        scores = scorer.score(label, pred)\n",
    "        result['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "        result['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "        result['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "        result['rougeLsum'].append(scores['rougeLsum'].fmeasure)\n",
    "\n",
    "    result = {key: np.mean(val) * 100 for key, val in result.items()}\n",
    "    \n",
    "    # 添加生成长度的度量\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "\n",
    "# --- 2. 设置训练器 ---\n",
    "# 检测是否在TPU环境\n",
    "is_tpu = \"TPU_NAME\" in os.environ\n",
    "if is_tpu:\n",
    "    print(\"\\n检测到 TPU 环境，将使用 torch_xla 进行训练。\")\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    # TPU 通常有 8 个核心\n",
    "    tpu_cores = 8\n",
    "else:\n",
    "    print(\"\\n未检测到 TPU 环境，将使用 GPU/CPU 进行训练。\")\n",
    "\n",
    "# 设置随机种子\n",
    "set_seed(TRAINING_ARGS.seed)\n",
    "\n",
    "# 定义数据整理器\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100 # 确保标签填充符被正确处理\n",
    ")\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=TRAINING_ARGS.output_dir,\n",
    "    do_train=TRAINING_ARGS.do_train,\n",
    "    do_eval=TRAINING_ARGS.do_eval,\n",
    "    per_device_train_batch_size=TRAINING_ARGS.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=TRAINING_ARGS.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=TRAINING_ARGS.gradient_accumulation_steps,\n",
    "    learning_rate=TRAINING_ARGS.learning_rate,\n",
    "    weight_decay=TRAINING_ARGS.weight_decay,\n",
    "    num_train_epochs=TRAINING_ARGS.num_train_epochs,\n",
    "    lr_scheduler_type=TRAINING_ARGS.lr_scheduler_type,\n",
    "    warmup_steps=TRAINING_ARGS.warmup_steps,\n",
    "    \n",
    "    # 日志和保存\n",
    "    logging_dir=f\"{TRAINING_ARGS.output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=TRAINING_ARGS.logging_steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=TRAINING_ARGS.eval_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=TRAINING_ARGS.save_steps,\n",
    "    save_total_limit=TRAINING_ARGS.save_total_limit,\n",
    "    load_best_model_at_end=True, # 训练结束后加载最佳模型\n",
    "    \n",
    "    # 评估相关\n",
    "    predict_with_generate=True,\n",
    "    generation_num_beams=TRAINING_ARGS.num_beams,\n",
    "    \n",
    "    # 混合精度\n",
    "    fp16=False if TRAINING_ARGS.mixed_precision == 'bf16' else (TRAINING_ARGS.mixed_precision == 'fp16'),\n",
    "    bf16=TRAINING_ARGS.mixed_precision == 'bf16',\n",
    "    \n",
    "    # TPU 特定参数\n",
    "    tpu_num_cores=tpu_cores if is_tpu else None,\n",
    ")\n",
    "\n",
    "# 初始化 Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if TRAINING_ARGS.do_train else None,\n",
    "    eval_dataset=eval_dataset if TRAINING_ARGS.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# --- 3. 开始训练和评估 ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"步骤 4: 开始训练与评估\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 开始训练\n",
    "if TRAINING_ARGS.do_train:\n",
    "    print(\"\\n--- 开始训练 ---\")\n",
    "    train_result = trainer.train(resume_from_checkpoint=TRAINING_ARGS.resume_from_checkpoint)\n",
    "    \n",
    "    # 保存最终的模型和训练指标\n",
    "    trainer.save_model() # 保存最终的最佳模型\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    print(\"\\n训练完成!\")\n",
    "else:\n",
    "    print(\"\\n跳过训练步骤 ('do_train' is False)。\")\n",
    "\n",
    "# 进行最终评估\n",
    "if TRAINING_ARGS.do_eval:\n",
    "    print(\"\\n--- 开始最终评估 ---\")\n",
    "    if not TRAINING_ARGS.do_train:\n",
    "        # 如果只评估，需要确保模型已经加载\n",
    "        print(f\"从 {model_load_path} 加载模型进行评估。\")\n",
    "    \n",
    "    metrics = trainer.evaluate(max_length=TRAINING_ARGS.max_target_length, num_beams=TRAINING_ARGS.num_beams, metric_key_prefix=\"eval\")\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n",
    "    print(\"\\n评估完成!\")\n",
    "\n",
    "print(\"\\n模型训练与评估流程结束。\")\n",
    "print(f\"最终模型和结果保存在: {TRAINING_ARGS.output_dir}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 步骤 5: 生成提交文件\n",
    "\n",
    " 训练和评估完成后，最后一步是使用我们微调好的模型对官方的 `test.csv` 文件进行预测，并生成符合比赛要求的 `submission.csv` 文件。\n",
    "\n",
    "\n",
    "\n",
    " 流程如下：\n",
    "\n",
    " 1.  加载测试数据集。\n",
    "\n",
    " 2.  定义一个预处理函数，只对输入的 `dialogue` 进行分词。\n",
    "\n",
    " 3.  使用 `trainer.predict()` 方法在测试集上进行推理。这个方法会自动处理所有细节，包括数据加载、模型推理和结果收集。\n",
    "\n",
    " 4.  解码模型生成的 token ID，得到最终的摘要文本。\n",
    "\n",
    " 5.  将ID和生成的摘要整理成 DataFrame，并保存为 `submission.csv`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"步骤 5: 生成提交文件\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. 加载测试数据\n",
    "if not os.path.exists(TRAINING_ARGS.test_file):\n",
    "    print(f\"警告: 未找到测试文件 {TRAINING_ARGS.test_file}，跳过提交文件生成。\")\n",
    "else:\n",
    "    test_df = pd.read_csv(TRAINING_ARGS.test_file)\n",
    "    print(f\"加载了 {len(test_df)} 条测试样本。\")\n",
    "\n",
    "    # 2. 批量推理以提高效率\n",
    "    predictions = []\n",
    "    batch_size = TRAINING_ARGS.per_device_eval_batch_size\n",
    "    \n",
    "    # 包装tqdm以显示进度条\n",
    "    for i in tqdm(range(0, len(test_df), batch_size), desc=\"生成摘要\"):\n",
    "        batch_dialogues = test_df['dialogue'][i:i+batch_size].tolist()\n",
    "        \n",
    "        # 对话分词\n",
    "        inputs = tokenizer(\n",
    "            batch_dialogues,\n",
    "            max_length=TRAINING_ARGS.max_source_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(trainer.model.device)\n",
    "        \n",
    "        # 使用模型生成摘要\n",
    "        summary_ids = trainer.model.generate(\n",
    "            **inputs,\n",
    "            num_beams=TRAINING_ARGS.num_beams,\n",
    "            max_length=TRAINING_ARGS.max_target_length,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        # 解码\n",
    "        batch_preds = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        predictions.extend(batch_preds)\n",
    "\n",
    "    # 3. 创建提交 DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'summary': predictions\n",
    "    })\n",
    "\n",
    "    # 4. 保存为 CSV\n",
    "    submission_path = '/kaggle/working/submission.csv'\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "    print(f\"\\n提交文件已生成: {submission_path}\")\n",
    "    print(\"文件内容预览:\")\n",
    "    print(submission_df.head())\n",
    "    print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
