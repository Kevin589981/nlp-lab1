{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5d16c71f-786a-42b0-ad16-7abeb0a99cfb",
    "_uuid": "5ec67a79-7dde-4a59-ab58-953eed525235",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    " # ä½¿ç”¨ FLAN-T5 å’Œ PEFT(LoRA) é«˜æ•ˆå¾®è°ƒæ‘˜è¦æ¨¡å‹\n",
    "\n",
    "\n",
    "\n",
    " æœ¬è„šæœ¬ä½¿ç”¨ Hugging Face `transformers`ã€`peft` å’Œ `datasets` åº“é‡æ„äº†åŸå§‹ä»£ç ã€‚\n",
    "\n",
    "\n",
    "\n",
    " ## ä¸»è¦æ”¹è¿›:\n",
    "\n",
    " 1.  **æ¨¡å‹æ¶æ„**: ä»è‡ªå®šä¹‰çš„ GPT-2 (Decoder-Only) æ›´æ¢ä¸º `google/flan-t5-base` (Encoder-Decoder)ï¼Œæ›´é€‚åˆæ‘˜è¦ä»»åŠ¡ã€‚\n",
    "\n",
    " 2.  **é«˜æ•ˆå¾®è°ƒ**: é›†æˆ PEFT (Parameter-Efficient Fine-Tuning) ä¸­çš„ LoRA æ–¹æ³•ï¼Œä»…è®­ç»ƒæ¨¡å‹ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œå¤§å¹…é™ä½æ˜¾å­˜å ç”¨å’Œè®­ç»ƒæ—¶é—´ã€‚\n",
    "\n",
    " 3.  **ç°ä»£åŒ–æ¡†æ¶**: å…¨é¢é‡‡ç”¨ `transformers.Trainer` APIï¼Œç®€åŒ–äº†è®­ç»ƒå¾ªç¯ã€è¯„ä¼°ã€æ—¥å¿—è®°å½•å’Œæ¨¡å‹ä¿å­˜ç­‰æµç¨‹ã€‚\n",
    "\n",
    " 4.  **æ•°æ®å¤„ç†**: ä½¿ç”¨ `datasets` åº“åŠ è½½å’Œå¤„ç†æ•°æ®ï¼Œæ›´åŠ é«˜æ•ˆå’Œè§„èŒƒã€‚\n",
    "\n",
    " 5.  **ç²¾ç¡®çš„ Loss Masking**: åˆ©ç”¨ `DataCollatorForSeq2Seq` è‡ªåŠ¨å¤„ç†æ ‡ç­¾å¡«å……ï¼ˆpaddingï¼‰ï¼Œç¡®ä¿æŸå¤±å‡½æ•°åªåœ¨æœ‰æ•ˆæ ‡ç­¾ä¸Šè®¡ç®—ã€‚\n",
    "\n",
    " 6.  **é…ç½®**: æ‰€æœ‰å…³é”®å‚æ•°ï¼ˆåŒ…æ‹¬ `eval_only` å’Œ `resume_from_checkpoint`ï¼‰éƒ½é›†ä¸­åœ¨ `TrainingConfig` ç±»ä¸­ï¼Œæ–¹ä¾¿ç®¡ç†ã€‚\n",
    "\n",
    " 7.  **æ··åˆç²¾åº¦**: é»˜è®¤å¯ç”¨ `fp16` (float16) è¿›è¡Œè®­ç»ƒï¼Œä»¥åŠ é€Ÿå¹¶å‡å°‘æ˜¾å­˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "073c0bef-41ef-4508-bb1f-f48ac376de8e",
    "_uuid": "a54a9851-12ef-4462-b063-44e967812615",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-12T17:49:11.914426Z",
     "iopub.status.busy": "2025-11-12T17:49:11.914137Z",
     "iopub.status.idle": "2025-11-12T17:49:15.371946Z",
     "shell.execute_reply": "2025-11-12T17:49:15.370972Z",
     "shell.execute_reply.started": "2025-11-12T17:49:11.914406Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.6)\n",
      "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
      "Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.11/dist-packages (3.20.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.18)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.3.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2025.11.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "# å®‰è£…å¿…è¦çš„åº“\n",
    "!pip install evaluate rouge-score protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "f7105d12-dc9e-4a5f-b92f-5f7e74e67198",
    "_uuid": "ad74aefe-0742-48e5-a907-bbae35a850e6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-12T17:49:15.373870Z",
     "iopub.status.busy": "2025-11-12T17:49:15.373560Z",
     "iopub.status.idle": "2025-11-12T17:49:15.379883Z",
     "shell.execute_reply": "2025-11-12T17:49:15.379176Z",
     "shell.execute_reply.started": "2025-11-12T17:49:15.373847Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import evaluate # Hugging Face çš„ evaluate åº“\n",
    "\n",
    "# å¿½ç•¥ä¸€äº›ä¸å¿…è¦çš„è­¦å‘Š\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\" # ç¦ç”¨ wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f3be432d-7d4f-4d26-8ff5-6819ec32b1fd",
    "_uuid": "8a4d372f-57d2-4f32-9468-a05ecf5c2f21",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    " ### ç¬¬1æ­¥ï¼šæ•°æ®é¢„å¤„ç†\n",
    "\n",
    " è¿™éƒ¨åˆ†ä»£ç ä¸åŸå§‹è„šæœ¬é€»è¾‘ç›¸åŒï¼Œè¯»å–`train.csv`å¹¶åˆ’åˆ†ä¸º95%çš„è®­ç»ƒé›†å’Œ5%çš„éªŒè¯é›†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "569d40c3-585e-4243-aa28-5259d9b2150f",
    "_uuid": "82657e54-bbe2-4313-b1a8-b960708a190e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-12T17:49:15.381118Z",
     "iopub.status.busy": "2025-11-12T17:49:15.380832Z",
     "iopub.status.idle": "2025-11-12T17:49:15.698063Z",
     "shell.execute_reply": "2025-11-12T17:49:15.697379Z",
     "shell.execute_reply.started": "2025-11-12T17:49:15.381086Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "å‡†å¤‡SAMSumæ•°æ®é›†åˆ’åˆ†\n",
      "================================================================================\n",
      "\n",
      "è¯»å–æ•°æ®: /kaggle/input/nanogpt-fudannlp-cs-30040/train.csv\n",
      "æ€»æ ·æœ¬æ•°: 200\n",
      "è®­ç»ƒé›†æ ·æœ¬æ•°: 190 (95.0%)\n",
      "éªŒè¯é›†æ ·æœ¬æ•°: 10 (5.0%)\n",
      "\n",
      "è®­ç»ƒé›†å·²ä¿å­˜: /kaggle/working/data/samsum/train.csv\n",
      "éªŒè¯é›†å·²ä¿å­˜: /kaggle/working/data/samsum/validation.csv\n",
      "\n",
      "æ•°æ®é›†åˆ’åˆ†å®Œæˆï¼\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"å‡†å¤‡SAMSumæ•°æ®é›†åˆ’åˆ†\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Kaggleç¯å¢ƒä¸‹çš„è¾“å…¥è·¯å¾„\n",
    "input_csv = '/kaggle/input/nanogpt-fudannlp-cs-30040/train.csv'\n",
    "\n",
    "# å¦‚æœè¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿæ–‡ä»¶ç”¨äºæœ¬åœ°æµ‹è¯•\n",
    "if not os.path.exists(input_csv):\n",
    "    print(f\"è­¦å‘Š: æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶ {input_csv}ã€‚å°†åˆ›å»ºä¸€ä¸ªç”¨äºæ¼”ç¤ºçš„è™šæ‹Ÿæ–‡ä»¶ã€‚\")\n",
    "    os.makedirs('/kaggle/input/nanogpt-fudannlp-cs-30040', exist_ok=True)\n",
    "    dummy_data = {\n",
    "        'id': [f'id_{i}' for i in range(100)],\n",
    "        'dialogue': [\"A: Hi! B: Hello. How are you? A: I'm fine, thanks.\" for _ in range(100)],\n",
    "        'summary': [\"A and B greeted each other.\" for _ in range(100)]\n",
    "    }\n",
    "    pd.DataFrame(dummy_data).to_csv(input_csv, index=False)\n",
    "\n",
    "\n",
    "print(f\"\\nè¯»å–æ•°æ®: {input_csv}\")\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "\n",
    "# éšæœºæ‰“ä¹±æ•°æ®\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df = df.head(200)  # åªå–å‰200æ¡æ•°æ®è¿›è¡Œæµ‹è¯•\n",
    "\n",
    "total_samples = len(df)\n",
    "print(f\"æ€»æ ·æœ¬æ•°: {total_samples}\")\n",
    "\n",
    "# è®¡ç®—åˆ’åˆ†ç‚¹\n",
    "val_size = int(total_samples * 0.05)\n",
    "train_size = total_samples - val_size\n",
    "print(f\"è®­ç»ƒé›†æ ·æœ¬æ•°: {train_size} ({train_size/total_samples*100:.1f}%)\")\n",
    "print(f\"éªŒè¯é›†æ ·æœ¬æ•°: {val_size} ({val_size/total_samples*100:.1f}%)\")\n",
    "\n",
    "# åˆ’åˆ†æ•°æ®\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:]\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "output_dir = '/kaggle/working/data/samsum'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ä¿å­˜æ–‡ä»¶\n",
    "train_csv_path = os.path.join(output_dir, 'train.csv')\n",
    "val_csv_path = os.path.join(output_dir, 'validation.csv')\n",
    "train_df.to_csv(train_csv_path, index=False)\n",
    "val_df.to_csv(val_csv_path, index=False)\n",
    "\n",
    "print(f\"\\nè®­ç»ƒé›†å·²ä¿å­˜: {train_csv_path}\")\n",
    "print(f\"éªŒè¯é›†å·²ä¿å­˜: {val_csv_path}\")\n",
    "print(\"\\næ•°æ®é›†åˆ’åˆ†å®Œæˆï¼\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "82e2c444-4b1f-4e55-988e-63f848214dbf",
    "_uuid": "685eb1d6-af07-4026-bb4d-2ec26f2c5083",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    " ### ç¬¬2æ­¥ï¼šé…ç½®å‚æ•°\n",
    "\n",
    " å°†æ‰€æœ‰è®­ç»ƒå’Œæ¨¡å‹ç›¸å…³çš„å‚æ•°é›†ä¸­åœ¨ä¸€ä¸ªé…ç½®ç±»ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "3684a603-b3e0-4330-969d-83e5e760d5d8",
    "_uuid": "c7885454-29e5-4da3-9a4c-ed6027441912",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-12T17:49:15.699709Z",
     "iopub.status.busy": "2025-11-12T17:49:15.699499Z",
     "iopub.status.idle": "2025-11-12T17:49:15.708611Z",
     "shell.execute_reply": "2025-11-12T17:49:15.707802Z",
     "shell.execute_reply.started": "2025-11-12T17:49:15.699693Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # æ¨¡å‹å’Œåˆ†è¯å™¨é…ç½®\n",
    "    model_name_or_path: str = \"google/flan-t5-base\" # T5-base çº¦250Må‚æ•°\n",
    "    tokenizer_name_or_path: str = \"google/flan-t5-base\"\n",
    "    \n",
    "    # æ•°æ®é…ç½®\n",
    "    data_path: str = output_dir\n",
    "    source_prefix: str = \"summarize: \" # T5/FLAN-T5 æ¨èä¸ºä»»åŠ¡æ·»åŠ å‰ç¼€\n",
    "    source_column: str = \"dialogue\"\n",
    "    target_column: str = \"summary\"\n",
    "    max_source_length: int = 512 # è¾“å…¥å¯¹è¯çš„æœ€å¤§é•¿åº¦\n",
    "    max_target_length: int = 128 # è¾“å‡ºæ‘˜è¦çš„æœ€å¤§é•¿åº¦\n",
    "    \n",
    "    # PEFT (LoRA) é…ç½®\n",
    "    use_peft: bool = True\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.1\n",
    "    \n",
    "    # è®­ç»ƒæ§åˆ¶å‚æ•°\n",
    "    output_dir: str = \"/kaggle/working/flan-t5-samsum-lora\" # æ¨¡å‹æƒé‡å’Œç»“æœçš„è¾“å‡ºç›®å½•\n",
    "    eval_only: bool = False # æ˜¯å¦åªè¿›è¡Œè¯„ä¼°è€Œä¸è®­ç»ƒ\n",
    "    resume_from_checkpoint: bool = False # æ˜¯å¦ä»ä¸Šä¸€ä¸ªæ–­ç‚¹ç»§ç»­è®­ç»ƒ\n",
    "    \n",
    "    # Seq2SeqTrainingArguments å‚æ•°\n",
    "    per_device_train_batch_size: int = 8\n",
    "    per_device_eval_batch_size: int = 8\n",
    "    gradient_accumulation_steps: int = 8 # æœ‰æ•ˆæ‰¹æ¬¡å¤§å° = 4 * 8 = 32\n",
    "    learning_rate: float = 3e-5 # LoRA å¾®è°ƒé€šå¸¸ä½¿ç”¨æ¯”å…¨é‡å¾®è°ƒç¨å¤§çš„å­¦ä¹ ç‡\n",
    "    num_train_epochs: int = 3\n",
    "    logging_steps: int = 10 # æ¯10æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—\n",
    "    eval_steps: int = 50 # æ¯50æ­¥è¯„ä¼°ä¸€æ¬¡\n",
    "    save_steps: int = 50 # æ¯50æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹\n",
    "    evaluation_strategy: str = \"steps\"\n",
    "    save_strategy: str = \"steps\"\n",
    "    save_total_limit: int = 2 # åªä¿ç•™æœ€æ–°çš„2ä¸ªæ£€æŸ¥ç‚¹\n",
    "    load_best_model_at_end: bool = True # è®­ç»ƒç»“æŸååŠ è½½æœ€ä½³æ¨¡å‹\n",
    "    predict_with_generate: bool = True # åœ¨è¯„ä¼°æ—¶ä½¿ç”¨generateç”Ÿæˆæ–‡æœ¬\n",
    "    metric_for_best_model: str = \"eval_rougeL\" # ä½¿ç”¨ROUGE-Lä½œä¸ºè¡¡é‡æœ€ä½³æ¨¡å‹çš„æŒ‡æ ‡\n",
    "    fp16: bool = True # å¿…é¡»ä½¿ç”¨ float16\n",
    "\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ebf6a677-2f77-4a11-9299-0df61ece6bb6",
    "_uuid": "a0c8ea3e-7fbe-43e3-9771-97fc66d5bdfe",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    " ### ç¬¬3æ­¥ï¼šåŠ è½½æ¨¡å‹ã€åˆ†è¯å™¨å’Œæ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "ca8f7cdb-973a-49cc-814e-783bab14231d",
    "_uuid": "4dae210b-651a-49e1-a0f3-ee3919680bc1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-12T17:49:15.710154Z",
     "iopub.status.busy": "2025-11-12T17:49:15.709472Z",
     "iopub.status.idle": "2025-11-12T17:49:16.919374Z",
     "shell.execute_reply": "2025-11-12T17:49:16.918804Z",
     "shell.execute_reply.started": "2025-11-12T17:49:15.710136Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PEFT (LoRA) æ¨¡å‹å·²å¯ç”¨:\n",
      "trainable params: 884,736 || all params: 248,462,592 || trainable%: 0.3561\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae8d309d2044fd39a4ced1f76a35903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a048104195f43df809d757106b4b914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ•°æ®é›†ç»“æ„:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary'],\n",
      "        num_rows: 190\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n",
      "\n",
      "è®­ç»ƒé›†æ ·æœ¬ç¤ºä¾‹:\n",
      "{'id': 23249, 'dialogue': 'Emma: So whatâ€™s the plan for Ethanâ€™s birthday?\\nConnor: Bowling maybe? He loves that.\\nEmma: True, but remember last time he said laser tag looked fun too?\\nConnor: He did? Alright, laser tag it is then.\\nEmma: Cool. Oh, and food?\\nConnor: Pizza, right? Or should we take him somewhere after?\\nEmma: Pizza sounds easier. Maybe a cake afterward? Iâ€™ll bake! ğŸ˜‰\\nConnor: Fancy. Okay, Iâ€™ll book the laser tag place tonight.\\nEmma: Perfect! Heâ€™ll be so excited.', 'summary': 'Emma and Connor brainstorm plans for Ethanâ€™s birthday and decide on laser tag with pizza and homemade cake.'}\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name_or_path)\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "# `torch_dtype=torch.float16` ç¡®ä¿æ¨¡å‹ä»¥fp16åŠ è½½\n",
    "# `device_map=\"auto\"` ä¼šè‡ªåŠ¨å°†æ¨¡å‹åˆ†é…åˆ°å¯ç”¨çš„è®¾å¤‡ä¸Šï¼ˆä¾‹å¦‚GPUï¼‰\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    config.model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# å¦‚æœé…ç½®ä½¿ç”¨PEFT (LoRA)\n",
    "if config.use_peft:\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        target_modules=[\"q\", \"v\"] # åœ¨T5çš„qå’ŒvæŠ•å½±çŸ©é˜µä¸Šåº”ç”¨LoRA\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    print(\"\\nPEFT (LoRA) æ¨¡å‹å·²å¯ç”¨:\")\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "data_files = {\n",
    "    \"train\": os.path.join(config.data_path, \"train.csv\"),\n",
    "    \"validation\": os.path.join(config.data_path, \"validation.csv\"),\n",
    "}\n",
    "# ä½¿ç”¨ datasets åº“åŠ è½½CSVæ–‡ä»¶\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "print(\"\\næ•°æ®é›†ç»“æ„:\")\n",
    "print(dataset)\n",
    "print(\"\\nè®­ç»ƒé›†æ ·æœ¬ç¤ºä¾‹:\")\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6c4a5fc0-a4e4-48dc-abb8-8dfef0a6f04e",
    "_uuid": "5a1cc757-1970-42dc-8621-08048e88efe2",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    " ### ç¬¬4æ­¥ï¼šæ•°æ®é¢„å¤„ç†å‡½æ•°å’Œè¯„ä¼°æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "146fa176-b5d8-46db-acb5-ae0fc319d098",
    "_uuid": "07c7cf00-23e6-4dbc-95ce-f4c309cece7b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-12T17:49:16.920305Z",
     "iopub.status.busy": "2025-11-12T17:49:16.920062Z",
     "iopub.status.idle": "2025-11-12T17:49:17.538836Z",
     "shell.execute_reply": "2025-11-12T17:49:17.538261Z",
     "shell.execute_reply.started": "2025-11-12T17:49:16.920288Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51e2415eea3496298de3ef543836a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/190 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331ef8c767ec41849d8bec25139c1840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "åˆ†è¯åæ•°æ®é›†çš„åˆ—: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼Œç”¨äºå¯¹æ•°æ®è¿›è¡Œåˆ†è¯\"\"\"\n",
    "    inputs = [config.source_prefix + doc for doc in examples[config.source_column]]\n",
    "    model_inputs = tokenizer(inputs, max_length=config.max_source_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # å¯¹æ ‡ç­¾è¿›è¡Œåˆ†è¯\n",
    "    labels = tokenizer(text_target=examples[config.target_column], max_length=config.max_target_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# ä½¿ç”¨ .map() æ–¹æ³•å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œé¢„å¤„ç†\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names  # <--- æ·»åŠ è¿™ä¸€è¡Œ\n",
    ")\n",
    "print(\"\\nåˆ†è¯åæ•°æ®é›†çš„åˆ—:\", tokenized_dataset[\"train\"].column_names)\n",
    "\n",
    "# åŠ è½½ ROUGE è¯„ä¼°æŒ‡æ ‡\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"è®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    # å°†æ¨¡å‹ç”Ÿæˆçš„ token ID è§£ç ä¸ºæ–‡æœ¬\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # å°†æ ‡ç­¾ä¸­çš„ -100 æ›¿æ¢ä¸º padding token IDï¼Œä»¥ä¾¿è§£ç \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # ROUGEæœŸæœ›æ¯å¥æ‘˜è¦åéƒ½æœ‰ä¸€ä¸ªæ¢è¡Œç¬¦\n",
    "    decoded_preds = [\"\\n\".join(pred.strip().split()) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.strip().split()) for label in decoded_labels]\n",
    "\n",
    "    # è®¡ç®— ROUGE åˆ†æ•°\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    # æå–ä¸»è¦çš„ f-measure åˆ†æ•°\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # æ·»åŠ ç”Ÿæˆæ–‡æœ¬çš„å¹³å‡é•¿åº¦\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "13f29e52-e09d-460a-980d-d85b4a4b3a0b",
    "_uuid": "a6ee9544-9c4a-42c6-b658-963515af4f05",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    " ### ç¬¬5æ­¥ï¼šè®¾ç½® Trainer å¹¶å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e51cf6ed-c5a9-479d-a49b-56a1722a9a2e",
    "_uuid": "3a6c6247-c2ce-42aa-8f6b-a1a72b6d47dc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-12T17:49:17.539826Z",
     "iopub.status.busy": "2025-11-12T17:49:17.539578Z",
     "iopub.status.idle": "2025-11-12T17:49:47.410612Z",
     "shell.execute_reply": "2025-11-12T17:49:47.409318Z",
     "shell.execute_reply.started": "2025-11-12T17:49:17.539808Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸš€ å¼€å§‹æ¨¡å‹å¾®è°ƒ...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: /kaggle/working/flan-t5-samsum-lora/final_model\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ˆ å¼€å§‹æœ€ç»ˆè¯„ä¼°...\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\", line 2175, in forward\n    return self.base_model(\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\", line 216, in forward\n    return self.model.forward(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 1792, in forward\n    decoder_outputs = self.decoder(\n                      ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 1105, in forward\n    layer_outputs = layer_module(\n                    ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\", line 83, in __call__\n    return super().__call__(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 702, in forward\n    cross_attention_outputs = self.layer[1](\n                              ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 632, in forward\n    attention_output = self.EncDecAttention(\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 507, in forward\n    key_states = self.k(current_states)\n                 ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48/508733148.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ğŸ“ˆ å¼€å§‹æœ€ç»ˆè¯„ä¼°...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\næœ€ç»ˆè¯„ä¼°ç»“æœ:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_48/508733148.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# ç°åœ¨ï¼Œçˆ¶ç±»çš„ evaluate æ–¹æ³•ä½¿ç”¨çš„æ˜¯æœªè¢«åŒ…è£…çš„ã€åœ¨å•ä¸ªè®¾å¤‡ä¸Šçš„æ¨¡å‹\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# è¯„ä¼°ç»“æŸåï¼Œæ— è®ºæˆåŠŸä¸å¦ï¼Œéƒ½æ¢å¤åŸå§‹çš„å¹¶è¡Œæ¨¡å‹åŒ…è£…å™¨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_seq2seq.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     def predict(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4198\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4199\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4200\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4201\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4393\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4394\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4395\u001b[0m             \u001b[0mmain_input_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"main_input_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4396\u001b[0m             inputs_decode = (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_seq2seq.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhas_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_smoother\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_smoother\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> List[Any]:\n\u001b[0;32m--> 212\u001b[0;31m         return parallel_apply(\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\", line 2175, in forward\n    return self.base_model(\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\", line 216, in forward\n    return self.model.forward(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 1792, in forward\n    decoder_outputs = self.decoder(\n                      ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 1105, in forward\n    layer_outputs = layer_module(\n                    ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\", line 83, in __call__\n    return super().__call__(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 702, in forward\n    cross_attention_outputs = self.layer[1](\n                              ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 632, in forward\n    attention_output = self.EncDecAttention(\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 507, in forward\n    key_states = self.k(current_states)\n                 ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n"
     ]
    }
   ],
   "source": [
    "# å®šä¹‰æ•°æ®æ•´ç†å™¨ï¼Œç”¨äºåŠ¨æ€å¡«å……æ‰¹æ¬¡ä¸­çš„æ•°æ®\n",
    "# `label_pad_token_id=-100` æ˜¯å…³é”®ï¼Œå®ƒèƒ½ç¡®ä¿å¡«å……çš„æ ‡ç­¾åœ¨è®¡ç®—æŸå¤±æ—¶è¢«å¿½ç•¥\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# å®šä¹‰è®­ç»ƒå‚æ•°\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    predict_with_generate=config.predict_with_generate,\n",
    "    fp16=config.fp16,\n",
    "    learning_rate=config.learning_rate,\n",
    "    num_train_epochs=config.num_train_epochs,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=config.logging_steps,\n",
    "    eval_strategy=config.evaluation_strategy,\n",
    "    eval_steps=config.eval_steps,\n",
    "    save_strategy=config.save_strategy,\n",
    "    save_steps=config.save_steps,\n",
    "    save_total_limit=config.save_total_limit,\n",
    "    load_best_model_at_end=config.load_best_model_at_end,\n",
    "    metric_for_best_model=config.metric_for_best_model,\n",
    "    report_to=\"none\", # ç¦ç”¨ wandb/tensorboard\n",
    "    # æ·»åŠ ä»¥ä¸‹å‚æ•°æ¥ç¦ç”¨DataParallel\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "# =================================================================\n",
    "# è‡ªå®šä¹‰ Trainer ç±»æ¥è§£å†³å¤šGPUè¯„ä¼°é—®é¢˜\n",
    "# =================================================================\n",
    "from transformers import Seq2SeqTrainer\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        é‡å†™evaluateæ–¹æ³•ï¼Œåœ¨è¯„ä¼°æ—¶æš‚æ—¶å°†æ¨¡å‹ç§»åˆ°å•ä¸ªè®¾å¤‡ä¸Š\n",
    "        \"\"\"\n",
    "        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¢«DataParallelåŒ…è£…\n",
    "        model_to_eval = self.model\n",
    "        \n",
    "        # å¦‚æœæ¨¡å‹è¢«DataParallelåŒ…è£…ï¼Œæš‚æ—¶è§£åŒ…\n",
    "        if isinstance(self.model, nn.DataParallel):\n",
    "            print(\"æ£€æµ‹åˆ°DataParallelï¼Œåœ¨è¯„ä¼°æ—¶ä¸´æ—¶ä½¿ç”¨å•GPU...\")\n",
    "            # è·å–åŸå§‹æ¨¡å‹ï¼ˆæœªåŒ…è£…çš„ï¼‰\n",
    "            original_model = self.model.module\n",
    "            # å°†æ¨¡å‹ç§»åˆ°ä¸»è®¾å¤‡ï¼ˆé€šå¸¸æ˜¯cuda:0ï¼‰\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            original_model = original_model.to(device)\n",
    "            # æš‚æ—¶æ›¿æ¢self.model\n",
    "            self.model = original_model\n",
    "            \n",
    "        # å¦‚æœä½¿ç”¨device_map=\"auto\"åŠ è½½çš„æ¨¡å‹ï¼Œç¡®ä¿æ­£ç¡®å¤„ç†\n",
    "        elif hasattr(self.model, \"hf_device_map\"):\n",
    "            # å¯¹äºä½¿ç”¨device_mapçš„æ¨¡å‹ï¼Œåˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„å•GPUç‰ˆæœ¬\n",
    "            print(\"æ£€æµ‹åˆ°multi-deviceæ¨¡å‹ï¼Œåœ¨è¯„ä¼°æ—¶é‡æ–°é…ç½®...\")\n",
    "            from accelerate import cpu_offload_with_hook\n",
    "            \n",
    "            # å°†æ•´ä¸ªæ¨¡å‹ç§»åˆ°å•ä¸ªGPUä¸Š\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            \n",
    "            # å¦‚æœæ˜¯PEFTæ¨¡å‹ï¼Œè·å–åŸºç¡€æ¨¡å‹\n",
    "            if hasattr(self.model, 'base_model'):\n",
    "                base_model = self.model.base_model\n",
    "                # å°†æ‰€æœ‰å‚æ•°ç§»åˆ°åŒä¸€è®¾å¤‡\n",
    "                for param in base_model.parameters():\n",
    "                    param.data = param.data.to(device)\n",
    "                self.model = self.model.to(device)\n",
    "            else:\n",
    "                # å°†æ‰€æœ‰å‚æ•°ç§»åˆ°åŒä¸€è®¾å¤‡\n",
    "                for param in self.model.parameters():\n",
    "                    param.data = param.data.to(device)\n",
    "                self.model = self.model.to(device)\n",
    "        \n",
    "        try:\n",
    "            # è°ƒç”¨çˆ¶ç±»çš„evaluateæ–¹æ³•\n",
    "            result = super().evaluate(*args, **kwargs)\n",
    "        finally:\n",
    "            # å¦‚æœä¹‹å‰æ˜¯DataParallelï¼Œæ¢å¤åŒ…è£…\n",
    "            if isinstance(model_to_eval, nn.DataParallel):\n",
    "                # é‡æ–°åŒ…è£…æ¨¡å‹\n",
    "                self.model = nn.DataParallel(original_model)\n",
    "                print(\"è¯„ä¼°å®Œæˆï¼Œå·²æ¢å¤DataParallelæ¨¡å¼\")\n",
    "                \n",
    "        return result\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None, **gen_kwargs):\n",
    "        \"\"\"\n",
    "        é‡å†™prediction_stepä»¥ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š\n",
    "        \"\"\"\n",
    "        # ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½åœ¨æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡ä¸Š\n",
    "        if hasattr(model, 'device'):\n",
    "            target_device = model.device\n",
    "        elif hasattr(model, 'module') and hasattr(model.module, 'device'):\n",
    "            target_device = model.module.device\n",
    "        else:\n",
    "            # å°è¯•ä»æ¨¡å‹å‚æ•°è·å–è®¾å¤‡\n",
    "            target_device = next(model.parameters()).device\n",
    "        \n",
    "        # å°†æ‰€æœ‰è¾“å…¥ç§»åˆ°ç›®æ ‡è®¾å¤‡\n",
    "        for key in inputs:\n",
    "            if isinstance(inputs[key], torch.Tensor):\n",
    "                inputs[key] = inputs[key].to(target_device)\n",
    "        \n",
    "        # è°ƒç”¨çˆ¶ç±»çš„prediction_step\n",
    "        return super().prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys, **gen_kwargs)\n",
    "\n",
    "\n",
    "# åˆå§‹åŒ–è‡ªå®šä¹‰Trainer\n",
    "trainer = CustomSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒ\n",
    "if not config.eval_only:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸš€ å¼€å§‹æ¨¡å‹å¾®è°ƒ...\")\n",
    "    print(\"=\" * 80)\n",
    "    # å¦‚æœ resume_from_checkpoint ä¸º Trueï¼ŒTrainer ä¼šè‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹\n",
    "    trainer.train(resume_from_checkpoint=config.resume_from_checkpoint)\n",
    "    \n",
    "    # ä¿å­˜æœ€ç»ˆçš„ LoRA é€‚é…å™¨æƒé‡\n",
    "    final_model_path = os.path.join(config.output_dir, \"final_model\")\n",
    "    trainer.save_model(final_model_path)\n",
    "    print(f\"\\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {final_model_path}\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ” eval_only=Trueï¼Œè·³è¿‡è®­ç»ƒï¼Œç›´æ¥è¿›è¡Œè¯„ä¼°ã€‚\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# è¯„ä¼°æ¨¡å‹\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“ˆ å¼€å§‹æœ€ç»ˆè¯„ä¼°...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# åœ¨è¯„ä¼°å‰ï¼Œç¡®ä¿æ¨¡å‹åœ¨å•ä¸ªè®¾å¤‡ä¸Š\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPUï¼Œè¯„ä¼°å°†åœ¨å•GPUä¸Šè¿›è¡Œ\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"\\næœ€ç»ˆè¯„ä¼°ç»“æœ:\")\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "37719641-735a-42ae-bc48-c68fd298a068",
    "_uuid": "2ce24f70-7176-444a-bbbd-2f13c914c162",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    " ### ç¬¬6æ­¥ï¼šç”Ÿæˆæµ‹è¯•é›†ç»“æœå¹¶å¯¼å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5312a6a2-56ee-41e2-945b-26e7c5fb6ddc",
    "_uuid": "2c6ecf74-a244-4309-a0b8-ef6b9fc08a0f",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-11-12T17:49:47.411251Z",
     "iopub.status.idle": "2025-11-12T17:49:47.411485Z",
     "shell.execute_reply": "2025-11-12T17:49:47.411382Z",
     "shell.execute_reply.started": "2025-11-12T17:49:47.411372Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_on_test_set(config):\n",
    "    \"\"\"\n",
    "    åŠ è½½å¾®è°ƒåçš„æ¨¡å‹ï¼Œå¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼Œå¹¶ç”Ÿæˆ submission.csv æ–‡ä»¶ã€‚\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“¦ å¼€å§‹å¯¹æµ‹è¯•é›†è¿›è¡Œæ¨ç†å¹¶ç”Ÿæˆæäº¤æ–‡ä»¶...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # åŠ è½½æµ‹è¯•æ•°æ®\n",
    "    test_csv_path = '/kaggle/input/nanogpt-fudannlp-cs-30040/test.csv'\n",
    "    if not os.path.exists(test_csv_path):\n",
    "        print(f\"è­¦å‘Š: æœªæ‰¾åˆ°æµ‹è¯•æ–‡ä»¶ {test_csv_path}ã€‚å°†åˆ›å»ºä¸€ä¸ªç”¨äºæ¼”ç¤ºçš„è™šæ‹Ÿæ–‡ä»¶ã€‚\")\n",
    "        dummy_test_data = {\n",
    "            'id': [f'test_{i}' for i in range(50)],\n",
    "            'dialogue': [\"A: What's the plan for tonight? B: Let's go to the movies.\" for _ in range(50)],\n",
    "        }\n",
    "        pd.DataFrame(dummy_test_data).to_csv(test_csv_path, index=False)\n",
    "        \n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "    print(f\"åŠ è½½äº† {len(test_df)} æ¡æµ‹è¯•æ ·æœ¬ã€‚\")\n",
    "\n",
    "    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæ¨ç†\n",
    "    # `trainer.model` å·²ç»åœ¨è®­ç»ƒç»“æŸååŠ è½½äº†æœ€ä½³æ¨¡å‹\n",
    "    # å¦‚æœæ˜¯ eval_only æ¨¡å¼ï¼Œéœ€è¦æ‰‹åŠ¨åŠ è½½\n",
    "    if config.eval_only:\n",
    "        from peft import PeftModel\n",
    "        from transformers import AutoModelForSeq2SeqLM\n",
    "        \n",
    "        # æ‰¾åˆ°æœ€ä½³æ£€æŸ¥ç‚¹\n",
    "        best_checkpoint_path = trainer.state.best_model_checkpoint\n",
    "        if best_checkpoint_path is None:\n",
    "             print(\"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒã€‚\")\n",
    "             return\n",
    "\n",
    "        print(f\"ä» {best_checkpoint_path} åŠ è½½åŸºç¡€æ¨¡å‹...\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            config.model_name_or_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(\"åŠ è½½ LoRA é€‚é…å™¨...\")\n",
    "        inference_model = PeftModel.from_pretrained(base_model, best_checkpoint_path)\n",
    "        inference_model = inference_model.merge_and_unload() # åˆå¹¶æƒé‡ä»¥ä¾¿äºæ¨ç†\n",
    "        inference_model.eval()\n",
    "    else:\n",
    "        inference_model = trainer.model\n",
    "        if config.use_peft:\n",
    "            # å¦‚æœä½¿ç”¨çš„æ˜¯PEFTæ¨¡å‹ï¼Œæœ€å¥½åˆå¹¶æƒé‡ä»¥åŠ é€Ÿæ¨ç†\n",
    "            try:\n",
    "                inference_model = inference_model.merge_and_unload()\n",
    "            except:\n",
    "                print(\"æ— æ³•è‡ªåŠ¨åˆå¹¶LoRAæƒé‡ï¼Œå°†ä½¿ç”¨é€‚é…å™¨æ¨¡å¼è¿›è¡Œæ¨ç†ã€‚\")\n",
    "        inference_model.eval()\n",
    "\n",
    "\n",
    "    # å‡†å¤‡ç”Ÿæˆç»“æœ\n",
    "    results = []\n",
    "    \n",
    "    from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "    class InferenceDataset(Dataset):\n",
    "        def __init__(self, df, tokenizer, config):\n",
    "            self.df = df\n",
    "            self.tokenizer = tokenizer\n",
    "            self.config = config\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            dialogue = self.df.iloc[idx][self.config.source_column]\n",
    "            input_text = self.config.source_prefix + dialogue\n",
    "            return self.tokenizer(input_text, return_tensors=\"pt\", max_length=self.config.max_source_length, truncation=True)\n",
    "            \n",
    "    inference_dataset = InferenceDataset(test_df, tokenizer, config)\n",
    "    # ä½¿ç”¨ DataLoader è¿›è¡Œæ‰¹å¤„ç†ä»¥åŠ é€Ÿ\n",
    "    data_loader = DataLoader(inference_dataset, batch_size=config.per_device_eval_batch_size)\n",
    "\n",
    "    print(\"\\nå¼€å§‹ç”Ÿæˆæ‘˜è¦...\")\n",
    "    for batch in data_loader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(inference_model.device)\n",
    "            attention_mask = batch['attention_mask'].to(inference_model.device)\n",
    "            \n",
    "            outputs = inference_model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=config.max_target_length,\n",
    "                num_beams=4, # ä½¿ç”¨æŸæœç´¢\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # è§£ç \n",
    "            summaries = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            # æ‰¾åˆ°æ‰¹æ¬¡å¯¹åº”çš„ID\n",
    "            start_index = len(results)\n",
    "            ids = test_df['id'][start_index : start_index + len(summaries)].tolist()\n",
    "            \n",
    "            for sample_id, summary in zip(ids, summaries):\n",
    "                results.append({'id': sample_id, 'summary': summary.strip()})\n",
    "        \n",
    "        if len(results) % 100 == 0:\n",
    "            print(f\"å·²å¤„ç† {len(results)} / {len(test_df)}...\")\n",
    "\n",
    "    # ä¿å­˜åˆ° submission.csv\n",
    "    submission_df = pd.DataFrame(results)\n",
    "    submission_path = os.path.join(config.output_dir, \"submission.csv\")\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… æ¨ç†å®Œæˆï¼æäº¤æ–‡ä»¶å·²ä¿å­˜è‡³: {submission_path}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# è°ƒç”¨é¢„æµ‹å‡½æ•°\n",
    "predict_on_test_set(config)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14409389,
     "sourceId": 120001,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
