{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " # 处理数据\n",
    "\n",
    " 右侧点击Add Input，找到我们的比赛，然后添加\n",
    "\n",
    "\n",
    "\n",
    " 从Kaggle输入的train.csv读取数据，随机划分为训练集(95%)和验证集(5%)\n",
    "\n",
    "\n",
    "\n",
    " 保存到/kaggle/working/data/samsum目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"准备SAMSum数据集划分\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 读取原始CSV文件\n",
    "input_csv = '/kaggle/input/nanogpt-fudannlp-cs-30040/train.csv'\n",
    "print(f\"\\n读取数据: {input_csv}\")\n",
    "\n",
    "# 检查输入文件是否存在\n",
    "if not os.path.exists(input_csv):\n",
    "    print(f\"错误：找不到输入文件 {input_csv}，请确保已正确添加数据集。\")\n",
    "    # 如果在Kaggle环境下运行，需要确保数据集已挂载\n",
    "    # 为了让代码可以运行，这里假设文件存在\n",
    "    # 如果文件不存在，后续步骤会失败，这里不做更深处理\n",
    "    pass\n",
    "else:\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # 随机打乱数据\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # df = df[:100]  # 仅使用前100条数据进行快速测试\n",
    "\n",
    "    total_samples = len(df)\n",
    "    print(f\"总样本数: {total_samples}\")\n",
    "\n",
    "    # 计算划分点（5%作为验证集）\n",
    "    val_size = int(total_samples * 0.05)\n",
    "    train_size = total_samples - val_size\n",
    "\n",
    "    print(f\"训练集样本数: {train_size} ({train_size/total_samples*100:.1f}%)\")\n",
    "    print(f\"验证集样本数: {val_size} ({val_size/total_samples*100:.1f}%)\")\n",
    "\n",
    "    # 划分数据\n",
    "    train_df = df.iloc[:train_size]\n",
    "    val_df = df.iloc[train_size:]\n",
    "\n",
    "    # 创建输出目录\n",
    "    output_dir = '/kaggle/working/data/samsum'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 保存训练集\n",
    "    train_csv_path = os.path.join(output_dir, 'train.csv')\n",
    "    train_df.to_csv(train_csv_path, index=False)\n",
    "    print(f\"\\n训练集已保存: {train_csv_path}\")\n",
    "\n",
    "    # 保存验证集\n",
    "    val_csv_path = os.path.join(output_dir, 'validation.csv')\n",
    "    val_df.to_csv(val_csv_path, index=False)\n",
    "    print(f\"验证集已保存: {val_csv_path}\")\n",
    "\n",
    "    print(\"\\n数据集划分完成！\")\n",
    "    print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 配置和数据准备 (DDP 改造)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
    "\n",
    "# =============================================================================\n",
    "# 配置参数\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"配置类：包含所有可调参数\"\"\"\n",
    "    \n",
    "    # DDP/系统配置 (新增/修改)\n",
    "    ddp_world_size = 1 # 全局进程数\n",
    "    ddp_rank = 0       # 当前进程的全局排名\n",
    "    ddp_local_rank = 0 # 当前进程的本地排名 (GPU ID)\n",
    "    is_master = True   # 是否为主进程 (rank 0)\n",
    "    device = 'cuda'    # 默认设备\n",
    "    \n",
    "    # 数据集配置\n",
    "    dataset_path = '/kaggle/working/data/samsum'\n",
    "    dataset = 'samsum'\n",
    "    \n",
    "    # 模型配置\n",
    "    model_name = 'facebook/bart-base'\n",
    "    label_smoothing_factor = 0.1\n",
    "\n",
    "    # 训练配置\n",
    "    init_from = 'pretrained'  # 'pretrained', 'resume', 'scratch'\n",
    "    resume_from = None  # checkpoint路径，当init_from='resume'时使用\n",
    "    \n",
    "    # 批次配置 - 针对2个T4 GPU优化\n",
    "    batch_size = 64  # **每个 GPU** 的 batch size\n",
    "    gradient_accumulation_steps = 8  # 梯度累积步数\n",
    "    max_source_length = 512  # 输入对话的最大长度\n",
    "    max_target_length = 128  # 目标摘要的最大长度\n",
    "    \n",
    "    # 训练步数\n",
    "    max_iters = 10\n",
    "    \n",
    "    # 优化器配置\n",
    "    learning_rate = 3e-5\n",
    "    weight_decay = 0.01\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    grad_clip = 1.0\n",
    "    \n",
    "    # 学习率调度\n",
    "    decay_lr = True\n",
    "    warmup_iters = int(0.1 *max_iters)\n",
    "    lr_decay_iters = 500\n",
    "    min_lr = 1e-6\n",
    "    \n",
    "    # I/O配置\n",
    "    out_dir = 'out-bart-summarization'\n",
    "    eval_interval = 10\n",
    "    log_interval = 5\n",
    "    eval_iters = 40\n",
    "    eval_only = False\n",
    "    always_save_checkpoint = False\n",
    "    \n",
    "    # ROUGE评估配置\n",
    "    eval_rouge_during_training = True\n",
    "    rouge_eval_samples = 5 # 评估的样本数量\n",
    "    \n",
    "    # wandb日志 (DDP中通常只在主进程进行日志记录)\n",
    "    wandb_log = False\n",
    "    wandb_project = 'bart-summarization'\n",
    "    wandb_run_name = 'bart-base'\n",
    "    \n",
    "    # 系统配置\n",
    "    dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "    compile = False\n",
    "    \n",
    "    # 生成配置\n",
    "    num_test_samples = 10\n",
    "    num_beams = 8 # 束搜索宽度\n",
    "    temperature = 1.0\n",
    "    top_k = 50\n",
    "    top_p = 0.95\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# =============================================================================\n",
    "# DDP 初始化函数\n",
    "# =============================================================================\n",
    "\n",
    "def setup_ddp():\n",
    "    \"\"\"初始化 DDP 进程组并配置 DDP 相关的 config 变量\"\"\"\n",
    "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        config.ddp_rank = int(os.environ['RANK'])\n",
    "        config.ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "        config.ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "        \n",
    "        # 初始化进程组\n",
    "        dist.init_process_group(backend='nccl', init_method='env://')\n",
    "        \n",
    "        # 设置当前进程使用的 GPU 设备\n",
    "        config.device = f'cuda:{config.ddp_local_rank}'\n",
    "        torch.cuda.set_device(config.device)\n",
    "        config.is_master = (config.ddp_rank == 0)\n",
    "        \n",
    "        print(f\"DDP 初始化成功: Rank {config.ddp_rank}/{config.ddp_world_size}, Device: {config.device}\")\n",
    "        \n",
    "    else:\n",
    "        # 单 GPU 或 CPU 模式\n",
    "        config.ddp_world_size = 1\n",
    "        config.ddp_rank = 0\n",
    "        config.ddp_local_rank = 0\n",
    "        config.is_master = True\n",
    "        config.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        if config.device == 'cuda':\n",
    "            # 确保单 GPU 模式下，设备是 0\n",
    "            config.device = 'cuda:0'\n",
    "            torch.cuda.set_device(config.device)\n",
    "            \n",
    "    # 确保主进程能看到完整的配置信息\n",
    "    if config.is_master:\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"总有效 Batch Size (跨所有 GPU): {config.batch_size * config.ddp_world_size * config.gradient_accumulation_steps}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "\n",
    "# =============================================================================\n",
    "# 数据集类\n",
    "# =============================================================================\n",
    "\n",
    "class SummarizationDataset(Dataset):\n",
    "    \"\"\"BART摘要任务的Dataset类\"\"\"\n",
    "    # ... (与原代码相同，无需 DDP 修改)\n",
    "    def __init__(self, csv_path, tokenizer, max_source_length, max_target_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        \n",
    "        self.dialogues = []\n",
    "        self.summaries = []\n",
    "        with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                self.dialogues.append(row['dialogue'])\n",
    "                self.summaries.append(row['summary'])\n",
    "        \n",
    "        if config.is_master:\n",
    "            print(f\"  加载了 {len(self.dialogues)} 个样本\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dialogue = self.dialogues[idx]\n",
    "        summary = self.summaries[idx]\n",
    "        \n",
    "        source = self.tokenizer(\n",
    "            dialogue,\n",
    "            max_length=self.max_source_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        target = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_target_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        \n",
    "        labels = target_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': source_ids,\n",
    "            'attention_mask': source_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# 全局变量：缓存DataLoader和 Sampler (DDP 改造: Sampler 需缓存)\n",
    "_dataloaders = {'train': None, 'val': None}\n",
    "_data_iters = {'train': None, 'val': None}\n",
    "_samplers = {'train': None, 'val': None}\n",
    "\n",
    "def get_batch(split, tokenizer):\n",
    "    \"\"\"获取一个训练批次 (DDP 改造: 使用 DistributedSampler)\"\"\"\n",
    "    global _dataloaders, _data_iters, _samplers\n",
    "    \n",
    "    # 首次调用：创建 DataLoader 和 Sampler\n",
    "    if _dataloaders[split] is None:\n",
    "        csv_file = 'train.csv' if split == 'train' else 'validation.csv'\n",
    "        csv_path = os.path.join(config.dataset_path, csv_file)\n",
    "        \n",
    "        dataset = SummarizationDataset(\n",
    "            csv_path,\n",
    "            tokenizer,\n",
    "            config.max_source_length,\n",
    "            config.max_target_length\n",
    "        )\n",
    "        \n",
    "        shuffle = (split == 'train')\n",
    "        \n",
    "        # DDP Sampler\n",
    "        if config.ddp_world_size > 1:\n",
    "            sampler = DistributedSampler(\n",
    "                dataset, \n",
    "                num_replicas=config.ddp_world_size, \n",
    "                rank=config.ddp_rank, \n",
    "                shuffle=shuffle\n",
    "            )\n",
    "            # 如果使用 Sampler，DataLoader 中的 shuffle 必须设置为 False\n",
    "            shuffle = False\n",
    "            _samplers[split] = sampler\n",
    "        else:\n",
    "            sampler = None\n",
    "        \n",
    "        num_workers = min(4, os.cpu_count() or 1)\n",
    "        \n",
    "        _dataloaders[split] = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            sampler=sampler, # 使用 Sampler\n",
    "            shuffle=shuffle, # Sampler 存在时为 False\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=('cuda' in config.device),\n",
    "            persistent_workers=(num_workers > 0 and 'cuda' in config.device),\n",
    "        )\n",
    "        _data_iters[split] = iter(_dataloaders[split])\n",
    "    \n",
    "    # 获取下一个 batch\n",
    "    try:\n",
    "        batch = next(_data_iters[split])\n",
    "    except StopIteration:\n",
    "        # 如果是训练集，需要更新 Sampler 的 epoch\n",
    "        if split == 'train' and _samplers['train']:\n",
    "            # 在 train() 循环中通过 sampler.set_epoch(iter_num) 来处理\n",
    "            # 这里先重新初始化迭代器，确保能获取新的 batch\n",
    "            pass \n",
    "        _data_iters[split] = iter(_dataloaders[split])\n",
    "        batch = next(_data_iters[split])\n",
    "    \n",
    "    # 移动到设备\n",
    "    batch = {k: v.to(config.device, non_blocking=True) for k, v in batch.items()}\n",
    "    \n",
    "    return batch\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, ctx, tokenizer):\n",
    "    \"\"\"估计训练集和验证集上的损失 (DDP 改造: 跨进程平均损失)\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(config.eval_iters, device=config.device)\n",
    "        for k in range(config.eval_iters):\n",
    "            batch = get_batch(split, tokenizer)\n",
    "            with ctx:\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            # loss已经是该进程上的平均loss\n",
    "            losses[k] = loss.item()\n",
    "        \n",
    "        # 计算当前进程上的平均 loss\n",
    "        local_avg_loss = losses.mean()\n",
    "        \n",
    "        # DDP: 跨所有进程求平均\n",
    "        if config.ddp_world_size > 1:\n",
    "            # AllReduce 操作：求和后分摊\n",
    "            dist.all_reduce(local_avg_loss, op=dist.ReduceOp.AVG)\n",
    "            \n",
    "        out[split] = local_avg_loss.item()\n",
    "    \n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_lr(iter_num):\n",
    "    \"\"\"学习率调度：带预热的余弦衰减 (与原代码相同)\"\"\"\n",
    "    if iter_num < config.warmup_iters:\n",
    "        return config.learning_rate * (iter_num + 1) / (config.warmup_iters + 1)\n",
    "    \n",
    "    if iter_num > config.lr_decay_iters:\n",
    "        return config.min_lr\n",
    "    \n",
    "    decay_ratio = (iter_num - config.warmup_iters) / (config.lr_decay_iters - config.warmup_iters)\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return config.min_lr + coeff * (config.learning_rate - config.min_lr)\n",
    "\n",
    "# =============================================================================\n",
    "# ROUGE评估 (DDP 改造: 仅在主进程进行)\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_rouge(reference_summary, generated_summary):\n",
    "    \"\"\"计算ROUGE分数 (与原代码相同)\"\"\"\n",
    "    try:\n",
    "        from rouge_score import rouge_scorer\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        scores = scorer.score(reference_summary, generated_summary)\n",
    "        return {\n",
    "            'rouge1': scores['rouge1'].fmeasure,\n",
    "            'rouge2': scores['rouge2'].fmeasure,\n",
    "            'rougeL': scores['rougeL'].fmeasure\n",
    "        }\n",
    "    except ImportError:\n",
    "        return None\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_rouge_during_training(model, tokenizer, ctx, num_samples=5):\n",
    "    \"\"\"在训练过程中评估ROUGE分数 (DDP 改造: 仅主进程评估)\"\"\"\n",
    "    \n",
    "    if not config.is_master:\n",
    "        # 从进程直接返回 None\n",
    "        return None\n",
    "    \n",
    "    # 获取原始模型 (DDP 包装)\n",
    "    eval_model = model.module if hasattr(model, 'module') else model\n",
    "    eval_model.eval()\n",
    "    \n",
    "    # ... (其余逻辑与原代码相同)\n",
    "    val_csv = os.path.join(config.dataset_path, 'validation.csv')\n",
    "    if not os.path.exists(val_csv):\n",
    "        eval_model.train()\n",
    "        return None\n",
    "    \n",
    "    dialogues = []\n",
    "    summaries = []\n",
    "    with open(val_csv, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            dialogues.append(row['dialogue'])\n",
    "            summaries.append(row['summary'])\n",
    "    \n",
    "    import random\n",
    "    indices = random.sample(range(len(dialogues)), min(num_samples, len(dialogues)))\n",
    "    \n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        dialogue = dialogues[idx]\n",
    "        reference_summary = summaries[idx]\n",
    "        \n",
    "        # Tokenize输入\n",
    "        # 注意: inputs.to(config.device) 已经在 DDP setup 中指向了正确的 GPU\n",
    "        inputs = tokenizer(\n",
    "            dialogue,\n",
    "            max_length=config.max_source_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(config.device)\n",
    "        \n",
    "        # 生成摘要\n",
    "        with ctx:\n",
    "            # eval_model 是原始模型，没有 DDP 包装\n",
    "            outputs = eval_model.generate(\n",
    "                inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_length=config.max_target_length,\n",
    "                num_beams=config.num_beams,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 计算ROUGE\n",
    "        rouge_scores = calculate_rouge(reference_summary, generated_summary)\n",
    "        if rouge_scores:\n",
    "            rouge1_scores.append(rouge_scores['rouge1'])\n",
    "            rouge2_scores.append(rouge_scores['rouge2'])\n",
    "            rougeL_scores.append(rouge_scores['rougeL'])\n",
    "    \n",
    "    eval_model.train()\n",
    "    \n",
    "    if len(rouge1_scores) > 0:\n",
    "        return {\n",
    "            'rouge1': np.mean(rouge1_scores),\n",
    "            'rouge2': np.mean(rouge2_scores),\n",
    "            'rougeL': np.mean(rougeL_scores)\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# =============================================================================\n",
    "# 训练函数 (DDP 改造)\n",
    "# =============================================================================\n",
    "\n",
    "def train():\n",
    "    \"\"\"训练主函数 (DDP 改造)\"\"\"\n",
    "    \n",
    "    if config.is_master:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"开始训练...\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    # DDP: 设置随机种子\n",
    "    seed = 1337 + config.ddp_rank\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # 主进程创建输出目录\n",
    "    if config.is_master:\n",
    "        os.makedirs(config.out_dir, exist_ok=True)\n",
    "    \n",
    "    # DDP: 等待所有进程同步，确保主进程完成目录创建\n",
    "    if config.ddp_world_size > 1:\n",
    "        dist.barrier()\n",
    "        \n",
    "    # 设置设备和精度\n",
    "    device_type = 'cuda' if 'cuda' in config.device else 'cpu'\n",
    "    ptdtype = {\n",
    "        'float32': torch.float32,\n",
    "        'bfloat16': torch.bfloat16,\n",
    "        'float16': torch.float16\n",
    "    }[config.dtype]\n",
    "    ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(\n",
    "        device_type=device_type, dtype=ptdtype\n",
    "    )\n",
    "    \n",
    "    # 初始化tokenizer\n",
    "    if config.is_master:\n",
    "        print(f\"\\n加载tokenizer: {config.model_name}\")\n",
    "    tokenizer = BartTokenizer.from_pretrained(config.model_name)\n",
    "    \n",
    "    # 初始化模型\n",
    "    iter_num = 0\n",
    "    best_val_loss = 1e9\n",
    "    \n",
    "    if config.init_from == 'scratch':\n",
    "        if config.is_master:\n",
    "            print(\"从头开始训练新模型\")\n",
    "        model_config = BartConfig.from_pretrained(config.model_name)\n",
    "        model = BartForConditionalGeneration(model_config)\n",
    "        \n",
    "    elif config.init_from == 'resume':\n",
    "        if config.is_master:\n",
    "            print(f\"从checkpoint恢复训练\")\n",
    "        if config.resume_from is None:\n",
    "            ckpt_path = os.path.join(config.out_dir, 'ckpt.pt')\n",
    "        else:\n",
    "            ckpt_path = config.resume_from\n",
    "        \n",
    "        # 注意: 只有主进程加载和保存 checkpoint，所有进程等待加载完成\n",
    "        if config.is_master and not os.path.exists(ckpt_path):\n",
    "            raise FileNotFoundError(f\"Checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "        if config.ddp_world_size > 1:\n",
    "            dist.barrier() # 等待主进程检查文件\n",
    "            \n",
    "        map_location = {'cuda:%d' % 0: 'cuda:%d' % config.ddp_local_rank} if device_type == 'cuda' else 'cpu'\n",
    "        checkpoint = torch.load(ckpt_path, map_location=map_location)\n",
    "\n",
    "        model = BartForConditionalGeneration.from_pretrained(config.model_name)\n",
    "        \n",
    "        # 处理 DataParallel/DDP 保存的模型\n",
    "        state_dict = checkpoint['model']\n",
    "        from collections import OrderedDict\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k.replace('module.', '') if k.startswith('module.') else k\n",
    "            new_state_dict[name] = v\n",
    "        \n",
    "        model.load_state_dict(new_state_dict)\n",
    "        iter_num = checkpoint['iter_num']\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        \n",
    "    else:  # pretrained\n",
    "        if config.is_master:\n",
    "            print(f\"从预训练模型加载: {config.model_name}\")\n",
    "        model = BartForConditionalGeneration.from_pretrained(\n",
    "            config.model_name,\n",
    "            label_smoothing_factor=config.label_smoothing_factor\n",
    "        )\n",
    "        if config.is_master:\n",
    "            print(f\"  启用标签平滑, factor: {config.label_smoothing_factor}\")\n",
    "    \n",
    "    model.to(config.device)\n",
    "    \n",
    "    # DDP: 使用 DistributedDataParallel 包装模型\n",
    "    if config.ddp_world_size > 1:\n",
    "        model = DDP(model, device_ids=[config.ddp_local_rank])\n",
    "        raw_model = model.module\n",
    "        if config.is_master:\n",
    "            print(f\"\\n使用 DDP 在 {config.ddp_world_size} 个 GPU 上训练\")\n",
    "    else:\n",
    "        raw_model = model\n",
    "    \n",
    "    # 初始化优化器\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        betas=(config.beta1, config.beta2),\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    \n",
    "    if config.init_from == 'resume' and 'optimizer' in checkpoint:\n",
    "        # 注意：这里需要在所有进程上加载优化器状态\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    # 初始化 GradScaler (只有在 CUDA + fp16/bf16 时启用)\n",
    "    scaler = torch.amp.GradScaler(enabled=(config.dtype == 'float16'))\n",
    "    \n",
    "    # 编译模型（可选）\n",
    "    if config.compile:\n",
    "        if config.is_master:\n",
    "            print(\"编译模型...\")\n",
    "        # 注意：DDP 和 torch.compile 通常是兼容的\n",
    "        model = torch.compile(model)\n",
    "    \n",
    "    # 训练循环\n",
    "    if config.is_master:\n",
    "        print(\"\\n开始训练循环...\")\n",
    "        print(f\"总迭代次数: {config.max_iters}\")\n",
    "        print(f\"每 GPU 批次大小: {config.batch_size}\")\n",
    "        print(f\"GPU 数量: {config.ddp_world_size}\")\n",
    "        print(f\"梯度累积步数: {config.gradient_accumulation_steps}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    while True:\n",
    "        # DDP: 在训练开始前设置 Sampler 的 epoch\n",
    "        train_sampler = _samplers.get('train')\n",
    "        if train_sampler:\n",
    "            train_sampler.set_epoch(iter_num)\n",
    "            # DDP: 如果是新的 epoch，需要重置迭代器\n",
    "            if iter_num > 0 and train_sampler.epoch > 0 and iter_num % len(_dataloaders['train']) == 0:\n",
    "                global _data_iters\n",
    "                _data_iters['train'] = iter(_dataloaders['train'])\n",
    "\n",
    "        # 设置学习率\n",
    "        lr = get_lr(iter_num) if config.decay_lr else config.learning_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        # 评估和保存 checkpoint\n",
    "        if iter_num % config.eval_interval == 0:\n",
    "            losses = estimate_loss(model, ctx, tokenizer)\n",
    "            \n",
    "            if config.is_master:\n",
    "                print(f\"\\nStep {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "                \n",
    "                # 计算ROUGE分数\n",
    "                if iter_num > 0 and config.eval_rouge_during_training:\n",
    "                    print(\"  评估ROUGE分数...\")\n",
    "                    rouge_scores = evaluate_rouge_during_training(\n",
    "                        model, tokenizer, ctx, num_samples=config.rouge_eval_samples\n",
    "                    )\n",
    "                    if rouge_scores:\n",
    "                        print(f\"  ROUGE-1: {rouge_scores['rouge1']:.4f}, \"\n",
    "                              f\"ROUGE-2: {rouge_scores['rouge2']:.4f}, \"\n",
    "                              f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "                \n",
    "                # 保存checkpoint (仅主进程保存)\n",
    "                if losses['val'] < best_val_loss or config.always_save_checkpoint:\n",
    "                    best_val_loss = losses['val']\n",
    "                    if iter_num > 0:\n",
    "                        checkpoint = {\n",
    "                            'model': raw_model.state_dict(),\n",
    "                            'optimizer': optimizer.state_dict(),\n",
    "                            'iter_num': iter_num,\n",
    "                            'best_val_loss': best_val_loss,\n",
    "                            'config': vars(config),\n",
    "                        }\n",
    "                        print(f\"  保存checkpoint到 {config.out_dir}\")\n",
    "                        torch.save(checkpoint, os.path.join(config.out_dir, 'ckpt.pt'))\n",
    "        \n",
    "        # DDP: 等待所有进程完成评估和主进程的保存操作\n",
    "        if config.ddp_world_size > 1:\n",
    "            dist.barrier()\n",
    "\n",
    "        if iter_num == 0 and config.eval_only:\n",
    "            break\n",
    "        \n",
    "        # 训练步骤\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for micro_step in range(config.gradient_accumulation_steps):\n",
    "            batch = get_batch('train', tokenizer)\n",
    "            \n",
    "            # 使用梯度累积时，只有最后一个 micro_step 进行 backward\n",
    "            is_last_micro_step = (micro_step == config.gradient_accumulation_steps - 1)\n",
    "            \n",
    "            # DDP: 需要设置 no_sync 避免在中间 micro_step 同步梯度\n",
    "            if config.ddp_world_size > 1 and not is_last_micro_step:\n",
    "                # DDP 需要一个 context manager 来禁用梯度同步\n",
    "                # DDP 包装器的 `no_sync()` 方法\n",
    "                with model.no_sync():\n",
    "                    with ctx:\n",
    "                        outputs = model(**batch)\n",
    "                        loss = outputs.loss\n",
    "                        # loss 已经是本地 GPU 上的平均 loss (标量)\n",
    "                        loss = loss / config.gradient_accumulation_steps\n",
    "                    scaler.scale(loss).backward()\n",
    "            else:\n",
    "                with ctx:\n",
    "                    outputs = model(**batch)\n",
    "                    loss = outputs.loss\n",
    "                    loss = loss / config.gradient_accumulation_steps\n",
    "                scaler.scale(loss).backward()\n",
    "        \n",
    "        # 梯度裁剪\n",
    "        if config.grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "        \n",
    "        # 更新参数\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # 记录日志 (仅主进程记录)\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        \n",
    "        if config.is_master and iter_num % config.log_interval == 0:\n",
    "            # 这里的 loss 是最后一个 micro_step 的平均 loss 乘以累积步数\n",
    "            lossf = loss.item() * config.gradient_accumulation_steps \n",
    "            print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, lr {lr:.2e}\")\n",
    "        \n",
    "        iter_num += 1\n",
    "        \n",
    "        if iter_num > config.max_iters:\n",
    "            break\n",
    "    \n",
    "    if config.is_master:\n",
    "        print(\"\\n训练完成！\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    # DDP: 结束进程组\n",
    "    if config.ddp_world_size > 1:\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 主函数 (DDP 包装)\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    \n",
    "    # 1. 设置 DDP 环境\n",
    "    setup_ddp()\n",
    "    \n",
    "    # 2. 打印配置信息 (仅主进程)\n",
    "    if config.is_master:\n",
    "        print(\"\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"BART 摘要微调 (DDP 多GPU)\".center(80))\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\n当前配置:\")\n",
    "        print(f\"  数据集: {config.dataset_path}\")\n",
    "        print(f\"  模型: {config.model_name}\")\n",
    "        print(f\"  初始化方式: {config.init_from}\")\n",
    "        print(f\"  设备: {config.device}\")\n",
    "        print(f\"  精度: {config.dtype}\")\n",
    "        print(f\"  GPU 数量: {config.ddp_world_size}\")\n",
    "        print(f\"  每 GPU 批次大小: {config.batch_size}\")\n",
    "        print(f\"  最大迭代次数: {config.max_iters}\")\n",
    "        print(f\"  学习率: {config.learning_rate}\")\n",
    "    \n",
    "    # 3. 执行训练\n",
    "    if not config.eval_only:\n",
    "        train()\n",
    "    elif config.is_master:\n",
    "        print(\"\\neval_only=True，跳过训练\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 评估 (仅在主进程运行)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: 以下评估和预测函数为了 DDP 兼容性，只在主进程 (config.is_master) 上运行。\n",
    "# 但为了代码简洁，我们在 DDP 训练结束后，直接在 'main' 进程执行这些单进程任务。\n",
    "# 重新加载模型时，我们只需要确保配置的 device 是 'cuda:0' 或 'cuda'。\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"加载训练好的模型 (仅在主进程/单 GPU 上运行)\"\"\"\n",
    "    # 确保设备是可用的 CUDA 设备或 CPU\n",
    "    current_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    device_type = 'cuda' if 'cuda' in current_device else 'cpu'\n",
    "    ptdtype = {\n",
    "        'float32': torch.float32,\n",
    "        'bfloat16': torch.bfloat16,\n",
    "        'float16': torch.float16\n",
    "    }[config.dtype]\n",
    "    ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(\n",
    "        device_type=device_type, dtype=ptdtype\n",
    "    )\n",
    "    \n",
    "    tokenizer = BartTokenizer.from_pretrained(config.model_name)\n",
    "    \n",
    "    print(f\"\\n从 {config.out_dir} 加载模型到 {current_device}...\")\n",
    "    ckpt_path = os.path.join(config.out_dir, 'ckpt.pt')\n",
    "    \n",
    "    if not os.path.exists(ckpt_path):\n",
    "        print(f\"错误: 找不到checkpoint文件 {ckpt_path}\")\n",
    "        print(\"请先运行训练！\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # DDP 环境下，主进程加载 checkpoint\n",
    "    checkpoint = torch.load(ckpt_path, map_location=current_device)\n",
    "    model = BartForConditionalGeneration.from_pretrained(config.model_name)\n",
    "    \n",
    "    # 处理可能的 DataParallel/DDP 保存的模型\n",
    "    state_dict = checkpoint['model']\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k.replace('module.', '') if k.startswith('module.') else k\n",
    "        new_state_dict[name] = v\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.eval()\n",
    "    model.to(current_device)\n",
    "    \n",
    "    if config.compile:\n",
    "        print(\"编译模型...\")\n",
    "        model = torch.compile(model)\n",
    "    \n",
    "    # 确保 config.device 在评估时指向当前设备\n",
    "    config.device = current_device\n",
    "    \n",
    "    return model, tokenizer, ctx\n",
    "\n",
    "def evaluate():\n",
    "    \"\"\"评估模型性能 (仅在主进程运行)\"\"\"\n",
    "    if not (config.is_master or config.ddp_world_size == 1):\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"开始评估...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    model, tokenizer, ctx = load_model()\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    # ... (其余逻辑与原代码相同，使用 config.device)\n",
    "    print(\"\\n加载测试数据...\")\n",
    "    test_file = os.path.join(config.dataset_path, 'validation.csv')\n",
    "    \n",
    "    dialogues = []\n",
    "    summaries = []\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            if i >= config.num_test_samples:\n",
    "                break\n",
    "            dialogues.append(row['dialogue'])\n",
    "            summaries.append(row['summary'])\n",
    "    \n",
    "    print(f\"加载了 {len(dialogues)} 条测试样本\")\n",
    "    \n",
    "    rouge_result = calculate_rouge(\"test\", \"test\")\n",
    "    use_rouge = rouge_result is not None\n",
    "    if not use_rouge:\n",
    "        print(\"\\n警告: 未安装rouge_score库，将跳过ROUGE评分\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"开始生成和评估...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    all_rouge1_f = []\n",
    "    all_rouge2_f = []\n",
    "    all_rougeL_f = []\n",
    "    \n",
    "    for idx, (dialogue, reference_summary) in enumerate(zip(dialogues, summaries)):\n",
    "        # ... (推理和评估逻辑与原代码相同)\n",
    "        print(f\"\\n[样本 {idx+1}/{len(dialogues)}]\")\n",
    "        print(f\"对话: {dialogue[:100]}...\" if len(dialogue) > 100 else f\"对话: {dialogue}\")\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            dialogue,\n",
    "            max_length=config.max_source_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(config.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with ctx:\n",
    "                outputs = model.generate(\n",
    "                    inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    max_length=config.max_target_length,\n",
    "                    num_beams=config.num_beams,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "        \n",
    "        generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"真实摘要: {reference_summary}\")\n",
    "        print(f\"生成摘要: {generated_summary}\")\n",
    "        \n",
    "        if use_rouge:\n",
    "            rouge_scores = calculate_rouge(reference_summary, generated_summary)\n",
    "            if rouge_scores:\n",
    "                print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "                print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "                print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "                \n",
    "                all_rouge1_f.append(rouge_scores['rouge1'])\n",
    "                all_rouge2_f.append(rouge_scores['rouge2'])\n",
    "                all_rougeL_f.append(rouge_scores['rougeL'])\n",
    "    \n",
    "    if use_rouge and len(all_rouge1_f) > 0:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"平均ROUGE分数:\")\n",
    "        print(f\"  ROUGE-1: {np.mean(all_rouge1_f):.4f}\")\n",
    "        print(f\"  ROUGE-2: {np.mean(all_rouge2_f):.4f}\")\n",
    "        print(f\"  ROUGE-L: {np.mean(all_rougeL_f):.4f}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "if config.is_master or config.ddp_world_size == 1:\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_set():\n",
    "    \"\"\"对测试集进行推理 (仅在主进程运行)\"\"\"\n",
    "    if not (config.is_master or config.ddp_world_size == 1):\n",
    "        return None\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"开始对测试集进行推理...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    model, tokenizer, ctx = load_model()\n",
    "    if model is None:\n",
    "        return None\n",
    "    \n",
    "    # ... (其余逻辑与原代码相同)\n",
    "    print(\"\\n加载测试数据...\")\n",
    "    test_file = '/kaggle/input/nanogpt-fudannlp-cs-30040/test.csv'\n",
    "    \n",
    "    if not os.path.exists(test_file):\n",
    "        print(f\"错误: 找不到测试文件 {test_file}\")\n",
    "        return None\n",
    "    \n",
    "    test_data = []\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            test_data.append({\n",
    "                'id': row['id'],\n",
    "                'dialogue': row['dialogue']\n",
    "            })\n",
    "    \n",
    "    print(f\"加载了 {len(test_data)} 条测试样本\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"开始生成摘要...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, sample in enumerate(tqdm(test_data)):\n",
    "        sample_id = sample['id']\n",
    "        dialogue = sample['dialogue']\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            dialogue,\n",
    "            max_length=config.max_source_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(config.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with ctx:\n",
    "                outputs = model.generate(\n",
    "                    inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    max_length=config.max_target_length,\n",
    "                    num_beams=config.num_beams,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "        \n",
    "        generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        results.append({\n",
    "            'id': sample_id,\n",
    "            'summary': generated_summary\n",
    "        })\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n生成完成！总耗时: {total_time/60:.1f}分钟 | 平均: {total_time/len(test_data):.2f}秒/样本\")\n",
    "    \n",
    "    output_file = 'submission.csv'\n",
    "    output_path = os.path.join(config.out_dir, output_file)\n",
    "    \n",
    "    print(f\"\\n保存结果到 {output_path}\")\n",
    "    with open(output_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['id', 'summary'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "    \n",
    "    print(f\"完成！生成了 {len(results)} 条摘要\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "if config.is_master or config.ddp_world_size == 1:\n",
    "    predict_test_set()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
