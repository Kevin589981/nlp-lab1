{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # ä½¿ç”¨ FLAN-T5 å’Œ PEFT(LoRA) é«˜æ•ˆå¾®è°ƒæ‘˜è¦æ¨¡å‹\n",
    "\n",
    "\n",
    "\n",
    " æœ¬è„šæœ¬ä½¿ç”¨ Hugging Face `transformers`ã€`peft` å’Œ `datasets` åº“é‡æ„äº†åŸå§‹ä»£ç ã€‚\n",
    "\n",
    "\n",
    "\n",
    " ## ä¸»è¦æ”¹è¿›:\n",
    "\n",
    " 1.  **æ¨¡å‹æ¶æ„**: ä»è‡ªå®šä¹‰çš„ GPT-2 (Decoder-Only) æ›´æ¢ä¸º `google/flan-t5-base` (Encoder-Decoder)ï¼Œæ›´é€‚åˆæ‘˜è¦ä»»åŠ¡ã€‚\n",
    "\n",
    " 2.  **é«˜æ•ˆå¾®è°ƒ**: é›†æˆ PEFT (Parameter-Efficient Fine-Tuning) ä¸­çš„ LoRA æ–¹æ³•ï¼Œä»…è®­ç»ƒæ¨¡å‹ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œå¤§å¹…é™ä½æ˜¾å­˜å ç”¨å’Œè®­ç»ƒæ—¶é—´ã€‚\n",
    "\n",
    " 3.  **ç°ä»£åŒ–æ¡†æ¶**: å…¨é¢é‡‡ç”¨ `transformers.Trainer` APIï¼Œç®€åŒ–äº†è®­ç»ƒå¾ªç¯ã€è¯„ä¼°ã€æ—¥å¿—è®°å½•å’Œæ¨¡å‹ä¿å­˜ç­‰æµç¨‹ã€‚\n",
    "\n",
    " 4.  **æ•°æ®å¤„ç†**: ä½¿ç”¨ `datasets` åº“åŠ è½½å’Œå¤„ç†æ•°æ®ï¼Œæ›´åŠ é«˜æ•ˆå’Œè§„èŒƒã€‚\n",
    "\n",
    " 5.  **ç²¾ç¡®çš„ Loss Masking**: åˆ©ç”¨ `DataCollatorForSeq2Seq` è‡ªåŠ¨å¤„ç†æ ‡ç­¾å¡«å……ï¼ˆpaddingï¼‰ï¼Œç¡®ä¿æŸå¤±å‡½æ•°åªåœ¨æœ‰æ•ˆæ ‡ç­¾ä¸Šè®¡ç®—ã€‚\n",
    "\n",
    " 6.  **é…ç½®**: æ‰€æœ‰å…³é”®å‚æ•°ï¼ˆåŒ…æ‹¬ `eval_only` å’Œ `resume_from_checkpoint`ï¼‰éƒ½é›†ä¸­åœ¨ `TrainingConfig` ç±»ä¸­ï¼Œæ–¹ä¾¿ç®¡ç†ã€‚\n",
    "\n",
    " 7.  **æ··åˆç²¾åº¦**: é»˜è®¤å¯ç”¨ `fp16` (float16) è¿›è¡Œè®­ç»ƒï¼Œä»¥åŠ é€Ÿå¹¶å‡å°‘æ˜¾å­˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…å¿…è¦çš„åº“\n",
    "!pip install -q evaluate rouge-score peft bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import evaluate # Hugging Face çš„ evaluate åº“\n",
    "\n",
    "# å¿½ç•¥ä¸€äº›ä¸å¿…è¦çš„è­¦å‘Š\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\" # ç¦ç”¨ wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### ç¬¬1æ­¥ï¼šæ•°æ®é¢„å¤„ç†\n",
    "\n",
    " è¿™éƒ¨åˆ†ä»£ç ä¸åŸå§‹è„šæœ¬é€»è¾‘ç›¸åŒï¼Œè¯»å–`train.csv`å¹¶åˆ’åˆ†ä¸º95%çš„è®­ç»ƒé›†å’Œ5%çš„éªŒè¯é›†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"å‡†å¤‡SAMSumæ•°æ®é›†åˆ’åˆ†\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Kaggleç¯å¢ƒä¸‹çš„è¾“å…¥è·¯å¾„\n",
    "input_csv = '/kaggle/input/nanogpt-fudannlp-cs-30040/train.csv'\n",
    "\n",
    "# å¦‚æœè¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿæ–‡ä»¶ç”¨äºæœ¬åœ°æµ‹è¯•\n",
    "if not os.path.exists(input_csv):\n",
    "    print(f\"è­¦å‘Š: æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶ {input_csv}ã€‚å°†åˆ›å»ºä¸€ä¸ªç”¨äºæ¼”ç¤ºçš„è™šæ‹Ÿæ–‡ä»¶ã€‚\")\n",
    "    os.makedirs('/kaggle/input/nanogpt-fudannlp-cs-30040', exist_ok=True)\n",
    "    dummy_data = {\n",
    "        'id': [f'id_{i}' for i in range(100)],\n",
    "        'dialogue': [\"A: Hi! B: Hello. How are you? A: I'm fine, thanks.\" for _ in range(100)],\n",
    "        'summary': [\"A and B greeted each other.\" for _ in range(100)]\n",
    "    }\n",
    "    pd.DataFrame(dummy_data).to_csv(input_csv, index=False)\n",
    "\n",
    "\n",
    "print(f\"\\nè¯»å–æ•°æ®: {input_csv}\")\n",
    "df = pd.read_csv(input_csv)\n",
    "total_samples = len(df)\n",
    "print(f\"æ€»æ ·æœ¬æ•°: {total_samples}\")\n",
    "\n",
    "# éšæœºæ‰“ä¹±æ•°æ®\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# è®¡ç®—åˆ’åˆ†ç‚¹\n",
    "val_size = int(total_samples * 0.05)\n",
    "train_size = total_samples - val_size\n",
    "print(f\"è®­ç»ƒé›†æ ·æœ¬æ•°: {train_size} ({train_size/total_samples*100:.1f}%)\")\n",
    "print(f\"éªŒè¯é›†æ ·æœ¬æ•°: {val_size} ({val_size/total_samples*100:.1f}%)\")\n",
    "\n",
    "# åˆ’åˆ†æ•°æ®\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:]\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "output_dir = '/kaggle/working/data/samsum'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ä¿å­˜æ–‡ä»¶\n",
    "train_csv_path = os.path.join(output_dir, 'train.csv')\n",
    "val_csv_path = os.path.join(output_dir, 'validation.csv')\n",
    "train_df.to_csv(train_csv_path, index=False)\n",
    "val_df.to_csv(val_csv_path, index=False)\n",
    "\n",
    "print(f\"\\nè®­ç»ƒé›†å·²ä¿å­˜: {train_csv_path}\")\n",
    "print(f\"éªŒè¯é›†å·²ä¿å­˜: {val_csv_path}\")\n",
    "print(\"\\næ•°æ®é›†åˆ’åˆ†å®Œæˆï¼\")\n",
    "print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### ç¬¬2æ­¥ï¼šé…ç½®å‚æ•°\n",
    "\n",
    " å°†æ‰€æœ‰è®­ç»ƒå’Œæ¨¡å‹ç›¸å…³çš„å‚æ•°é›†ä¸­åœ¨ä¸€ä¸ªé…ç½®ç±»ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # æ¨¡å‹å’Œåˆ†è¯å™¨é…ç½®\n",
    "    model_name_or_path: str = \"google/flan-t5-base\" # T5-base çº¦250Må‚æ•°\n",
    "    tokenizer_name_or_path: str = \"google/flan-t5-base\"\n",
    "    \n",
    "    # æ•°æ®é…ç½®\n",
    "    data_path: str = output_dir\n",
    "    source_prefix: str = \"summarize: \" # T5/FLAN-T5 æ¨èä¸ºä»»åŠ¡æ·»åŠ å‰ç¼€\n",
    "    source_column: str = \"dialogue\"\n",
    "    target_column: str = \"summary\"\n",
    "    max_source_length: int = 512 # è¾“å…¥å¯¹è¯çš„æœ€å¤§é•¿åº¦\n",
    "    max_target_length: int = 128 # è¾“å‡ºæ‘˜è¦çš„æœ€å¤§é•¿åº¦\n",
    "    \n",
    "    # PEFT (LoRA) é…ç½®\n",
    "    use_peft: bool = True\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.1\n",
    "    \n",
    "    # è®­ç»ƒæ§åˆ¶å‚æ•°\n",
    "    output_dir: str = \"/kaggle/working/flan-t5-samsum-lora\" # æ¨¡å‹æƒé‡å’Œç»“æœçš„è¾“å‡ºç›®å½•\n",
    "    eval_only: bool = False # æ˜¯å¦åªè¿›è¡Œè¯„ä¼°è€Œä¸è®­ç»ƒ\n",
    "    resume_from_checkpoint: bool = False # æ˜¯å¦ä»ä¸Šä¸€ä¸ªæ–­ç‚¹ç»§ç»­è®­ç»ƒ\n",
    "    \n",
    "    # Seq2SeqTrainingArguments å‚æ•°\n",
    "    per_device_train_batch_size: int = 4\n",
    "    per_device_eval_batch_size: int = 8\n",
    "    gradient_accumulation_steps: int = 8 # æœ‰æ•ˆæ‰¹æ¬¡å¤§å° = 4 * 8 = 32\n",
    "    learning_rate: float = 3e-4 # LoRA å¾®è°ƒé€šå¸¸ä½¿ç”¨æ¯”å…¨é‡å¾®è°ƒç¨å¤§çš„å­¦ä¹ ç‡\n",
    "    num_train_epochs: int = 3\n",
    "    logging_steps: int = 10 # æ¯10æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—\n",
    "    eval_steps: int = 50 # æ¯50æ­¥è¯„ä¼°ä¸€æ¬¡\n",
    "    save_steps: int = 50 # æ¯50æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹\n",
    "    evaluation_strategy: str = \"steps\"\n",
    "    save_strategy: str = \"steps\"\n",
    "    save_total_limit: int = 2 # åªä¿ç•™æœ€æ–°çš„2ä¸ªæ£€æŸ¥ç‚¹\n",
    "    load_best_model_at_end: bool = True # è®­ç»ƒç»“æŸååŠ è½½æœ€ä½³æ¨¡å‹\n",
    "    predict_with_generate: bool = True # åœ¨è¯„ä¼°æ—¶ä½¿ç”¨generateç”Ÿæˆæ–‡æœ¬\n",
    "    metric_for_best_model: str = \"eval_rougeL\" # ä½¿ç”¨ROUGE-Lä½œä¸ºè¡¡é‡æœ€ä½³æ¨¡å‹çš„æŒ‡æ ‡\n",
    "    fp16: bool = True # å¿…é¡»ä½¿ç”¨ float16\n",
    "\n",
    "\n",
    "config = TrainingConfig()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### ç¬¬3æ­¥ï¼šåŠ è½½æ¨¡å‹ã€åˆ†è¯å™¨å’Œæ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name_or_path)\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "# `torch_dtype=torch.float16` ç¡®ä¿æ¨¡å‹ä»¥fp16åŠ è½½\n",
    "# `device_map=\"auto\"` ä¼šè‡ªåŠ¨å°†æ¨¡å‹åˆ†é…åˆ°å¯ç”¨çš„è®¾å¤‡ä¸Šï¼ˆä¾‹å¦‚GPUï¼‰\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    config.model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# å¦‚æœé…ç½®ä½¿ç”¨PEFT (LoRA)\n",
    "if config.use_peft:\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        target_modules=[\"q\", \"v\"] # åœ¨T5çš„qå’ŒvæŠ•å½±çŸ©é˜µä¸Šåº”ç”¨LoRA\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    print(\"\\nPEFT (LoRA) æ¨¡å‹å·²å¯ç”¨:\")\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "data_files = {\n",
    "    \"train\": os.path.join(config.data_path, \"train.csv\"),\n",
    "    \"validation\": os.path.join(config.data_path, \"validation.csv\"),\n",
    "}\n",
    "# ä½¿ç”¨ datasets åº“åŠ è½½CSVæ–‡ä»¶\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "print(\"\\næ•°æ®é›†ç»“æ„:\")\n",
    "print(dataset)\n",
    "print(\"\\nè®­ç»ƒé›†æ ·æœ¬ç¤ºä¾‹:\")\n",
    "print(dataset[\"train\"][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### ç¬¬4æ­¥ï¼šæ•°æ®é¢„å¤„ç†å‡½æ•°å’Œè¯„ä¼°æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼Œç”¨äºå¯¹æ•°æ®è¿›è¡Œåˆ†è¯\"\"\"\n",
    "    inputs = [config.source_prefix + doc for doc in examples[config.source_column]]\n",
    "    model_inputs = tokenizer(inputs, max_length=config.max_source_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # å¯¹æ ‡ç­¾è¿›è¡Œåˆ†è¯\n",
    "    labels = tokenizer(text_target=examples[config.target_column], max_length=config.max_target_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# ä½¿ç”¨ .map() æ–¹æ³•å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œé¢„å¤„ç†\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "print(\"\\nåˆ†è¯åæ•°æ®é›†çš„åˆ—:\", tokenized_dataset[\"train\"].column_names)\n",
    "\n",
    "\n",
    "# åŠ è½½ ROUGE è¯„ä¼°æŒ‡æ ‡\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"è®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    # å°†æ¨¡å‹ç”Ÿæˆçš„ token ID è§£ç ä¸ºæ–‡æœ¬\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # å°†æ ‡ç­¾ä¸­çš„ -100 æ›¿æ¢ä¸º padding token IDï¼Œä»¥ä¾¿è§£ç \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # ROUGEæœŸæœ›æ¯å¥æ‘˜è¦åéƒ½æœ‰ä¸€ä¸ªæ¢è¡Œç¬¦\n",
    "    decoded_preds = [\"\\n\".join(pred.strip().split()) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.strip().split()) for label in decoded_labels]\n",
    "\n",
    "    # è®¡ç®— ROUGE åˆ†æ•°\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    # æå–ä¸»è¦çš„ f-measure åˆ†æ•°\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # æ·»åŠ ç”Ÿæˆæ–‡æœ¬çš„å¹³å‡é•¿åº¦\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### ç¬¬5æ­¥ï¼šè®¾ç½® Trainer å¹¶å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰æ•°æ®æ•´ç†å™¨ï¼Œç”¨äºåŠ¨æ€å¡«å……æ‰¹æ¬¡ä¸­çš„æ•°æ®\n",
    "# `label_pad_token_id=-100` æ˜¯å…³é”®ï¼Œå®ƒèƒ½ç¡®ä¿å¡«å……çš„æ ‡ç­¾åœ¨è®¡ç®—æŸå¤±æ—¶è¢«å¿½ç•¥\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# å®šä¹‰è®­ç»ƒå‚æ•°\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    predict_with_generate=config.predict_with_generate,\n",
    "    fp16=config.fp16,\n",
    "    learning_rate=config.learning_rate,\n",
    "    num_train_epochs=config.num_train_epochs,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=config.logging_steps,\n",
    "    evaluation_strategy=config.evaluation_strategy,\n",
    "    eval_steps=config.eval_steps,\n",
    "    save_strategy=config.save_strategy,\n",
    "    save_steps=config.save_steps,\n",
    "    save_total_limit=config.save_total_limit,\n",
    "    load_best_model_at_end=config.load_best_model_at_end,\n",
    "    metric_for_best_model=config.metric_for_best_model,\n",
    "    report_to=\"none\", # ç¦ç”¨ wandb/tensorboard\n",
    ")\n",
    "\n",
    "# åˆå§‹åŒ– Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒ\n",
    "if not config.eval_only:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸš€ å¼€å§‹æ¨¡å‹å¾®è°ƒ...\")\n",
    "    print(\"=\" * 80)\n",
    "    # å¦‚æœ resume_from_checkpoint ä¸º Trueï¼ŒTrainer ä¼šè‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹\n",
    "    trainer.train(resume_from_checkpoint=config.resume_from_checkpoint)\n",
    "    \n",
    "    # ä¿å­˜æœ€ç»ˆçš„ LoRA é€‚é…å™¨æƒé‡\n",
    "    final_model_path = os.path.join(config.output_dir, \"final_model\")\n",
    "    trainer.save_model(final_model_path)\n",
    "    print(f\"\\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {final_model_path}\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ” eval_only=Trueï¼Œè·³è¿‡è®­ç»ƒï¼Œç›´æ¥è¿›è¡Œè¯„ä¼°ã€‚\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# è¯„ä¼°æ¨¡å‹\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“ˆ å¼€å§‹æœ€ç»ˆè¯„ä¼°...\")\n",
    "print(\"=\" * 80)\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"\\næœ€ç»ˆè¯„ä¼°ç»“æœ:\")\n",
    "print(eval_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### ç¬¬6æ­¥ï¼šç”Ÿæˆæµ‹è¯•é›†ç»“æœå¹¶å¯¼å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test_set(config):\n",
    "    \"\"\"\n",
    "    åŠ è½½å¾®è°ƒåçš„æ¨¡å‹ï¼Œå¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼Œå¹¶ç”Ÿæˆ submission.csv æ–‡ä»¶ã€‚\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“¦ å¼€å§‹å¯¹æµ‹è¯•é›†è¿›è¡Œæ¨ç†å¹¶ç”Ÿæˆæäº¤æ–‡ä»¶...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # åŠ è½½æµ‹è¯•æ•°æ®\n",
    "    test_csv_path = '/kaggle/input/nanogpt-fudannlp-cs-30040/test.csv'\n",
    "    if not os.path.exists(test_csv_path):\n",
    "        print(f\"è­¦å‘Š: æœªæ‰¾åˆ°æµ‹è¯•æ–‡ä»¶ {test_csv_path}ã€‚å°†åˆ›å»ºä¸€ä¸ªç”¨äºæ¼”ç¤ºçš„è™šæ‹Ÿæ–‡ä»¶ã€‚\")\n",
    "        dummy_test_data = {\n",
    "            'id': [f'test_{i}' for i in range(50)],\n",
    "            'dialogue': [\"A: What's the plan for tonight? B: Let's go to the movies.\" for _ in range(50)],\n",
    "        }\n",
    "        pd.DataFrame(dummy_test_data).to_csv(test_csv_path, index=False)\n",
    "        \n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "    print(f\"åŠ è½½äº† {len(test_df)} æ¡æµ‹è¯•æ ·æœ¬ã€‚\")\n",
    "\n",
    "    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæ¨ç†\n",
    "    # `trainer.model` å·²ç»åœ¨è®­ç»ƒç»“æŸååŠ è½½äº†æœ€ä½³æ¨¡å‹\n",
    "    # å¦‚æœæ˜¯ eval_only æ¨¡å¼ï¼Œéœ€è¦æ‰‹åŠ¨åŠ è½½\n",
    "    if config.eval_only:\n",
    "        from peft import PeftModel\n",
    "        from transformers import AutoModelForSeq2SeqLM\n",
    "        \n",
    "        # æ‰¾åˆ°æœ€ä½³æ£€æŸ¥ç‚¹\n",
    "        best_checkpoint_path = trainer.state.best_model_checkpoint\n",
    "        if best_checkpoint_path is None:\n",
    "             print(\"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒã€‚\")\n",
    "             return\n",
    "\n",
    "        print(f\"ä» {best_checkpoint_path} åŠ è½½åŸºç¡€æ¨¡å‹...\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            config.model_name_or_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(\"åŠ è½½ LoRA é€‚é…å™¨...\")\n",
    "        inference_model = PeftModel.from_pretrained(base_model, best_checkpoint_path)\n",
    "        inference_model = inference_model.merge_and_unload() # åˆå¹¶æƒé‡ä»¥ä¾¿äºæ¨ç†\n",
    "        inference_model.eval()\n",
    "    else:\n",
    "        inference_model = trainer.model\n",
    "        if config.use_peft:\n",
    "            # å¦‚æœä½¿ç”¨çš„æ˜¯PEFTæ¨¡å‹ï¼Œæœ€å¥½åˆå¹¶æƒé‡ä»¥åŠ é€Ÿæ¨ç†\n",
    "            try:\n",
    "                inference_model = inference_model.merge_and_unload()\n",
    "            except:\n",
    "                print(\"æ— æ³•è‡ªåŠ¨åˆå¹¶LoRAæƒé‡ï¼Œå°†ä½¿ç”¨é€‚é…å™¨æ¨¡å¼è¿›è¡Œæ¨ç†ã€‚\")\n",
    "        inference_model.eval()\n",
    "\n",
    "\n",
    "    # å‡†å¤‡ç”Ÿæˆç»“æœ\n",
    "    results = []\n",
    "    \n",
    "    from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "    class InferenceDataset(Dataset):\n",
    "        def __init__(self, df, tokenizer, config):\n",
    "            self.df = df\n",
    "            self.tokenizer = tokenizer\n",
    "            self.config = config\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            dialogue = self.df.iloc[idx][self.config.source_column]\n",
    "            input_text = self.config.source_prefix + dialogue\n",
    "            return self.tokenizer(input_text, return_tensors=\"pt\", max_length=self.config.max_source_length, truncation=True)\n",
    "            \n",
    "    inference_dataset = InferenceDataset(test_df, tokenizer, config)\n",
    "    # ä½¿ç”¨ DataLoader è¿›è¡Œæ‰¹å¤„ç†ä»¥åŠ é€Ÿ\n",
    "    data_loader = DataLoader(inference_dataset, batch_size=config.per_device_eval_batch_size)\n",
    "\n",
    "    print(\"\\nå¼€å§‹ç”Ÿæˆæ‘˜è¦...\")\n",
    "    for batch in data_loader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(inference_model.device)\n",
    "            attention_mask = batch['attention_mask'].to(inference_model.device)\n",
    "            \n",
    "            outputs = inference_model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=config.max_target_length,\n",
    "                num_beams=4, # ä½¿ç”¨æŸæœç´¢\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # è§£ç \n",
    "            summaries = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            # æ‰¾åˆ°æ‰¹æ¬¡å¯¹åº”çš„ID\n",
    "            start_index = len(results)\n",
    "            ids = test_df['id'][start_index : start_index + len(summaries)].tolist()\n",
    "            \n",
    "            for sample_id, summary in zip(ids, summaries):\n",
    "                results.append({'id': sample_id, 'summary': summary.strip()})\n",
    "        \n",
    "        if len(results) % 100 == 0:\n",
    "            print(f\"å·²å¤„ç† {len(results)} / {len(test_df)}...\")\n",
    "\n",
    "    # ä¿å­˜åˆ° submission.csv\n",
    "    submission_df = pd.DataFrame(results)\n",
    "    submission_path = os.path.join(config.output_dir, \"submission.csv\")\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… æ¨ç†å®Œæˆï¼æäº¤æ–‡ä»¶å·²ä¿å­˜è‡³: {submission_path}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# è°ƒç”¨é¢„æµ‹å‡½æ•°\n",
    "predict_on_test_set(config)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
