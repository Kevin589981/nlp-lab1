# %% [markdown]
# # ä½¿ç”¨ FLAN-T5 å’Œ PEFT(LoRA) é«˜æ•ˆå¾®è°ƒæ‘˜è¦æ¨¡å‹
#
# æœ¬è„šæœ¬ä½¿ç”¨ Hugging Face `transformers`ã€`peft` å’Œ `datasets` åº“é‡æ„äº†åŸå§‹ä»£ç ã€‚
#
# ## ä¸»è¦æ”¹è¿›:
# 1.  **æ¨¡å‹æ¶æ„**: ä»è‡ªå®šä¹‰çš„ GPT-2 (Decoder-Only) æ›´æ¢ä¸º `google/flan-t5-base` (Encoder-Decoder)ï¼Œæ›´é€‚åˆæ‘˜è¦ä»»åŠ¡ã€‚
# 2.  **é«˜æ•ˆå¾®è°ƒ**: é›†æˆ PEFT (Parameter-Efficient Fine-Tuning) ä¸­çš„ LoRA æ–¹æ³•ï¼Œä»…è®­ç»ƒæ¨¡å‹ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œå¤§å¹…é™ä½æ˜¾å­˜å ç”¨å’Œè®­ç»ƒæ—¶é—´ã€‚
# 3.  **ç°ä»£åŒ–æ¡†æ¶**: å…¨é¢é‡‡ç”¨ `transformers.Trainer` APIï¼Œç®€åŒ–äº†è®­ç»ƒå¾ªç¯ã€è¯„ä¼°ã€æ—¥å¿—è®°å½•å’Œæ¨¡å‹ä¿å­˜ç­‰æµç¨‹ã€‚
# 4.  **æ•°æ®å¤„ç†**: ä½¿ç”¨ `datasets` åº“åŠ è½½å’Œå¤„ç†æ•°æ®ï¼Œæ›´åŠ é«˜æ•ˆå’Œè§„èŒƒã€‚
# 5.  **ç²¾ç¡®çš„ Loss Masking**: åˆ©ç”¨ `DataCollatorForSeq2Seq` è‡ªåŠ¨å¤„ç†æ ‡ç­¾å¡«å……ï¼ˆpaddingï¼‰ï¼Œç¡®ä¿æŸå¤±å‡½æ•°åªåœ¨æœ‰æ•ˆæ ‡ç­¾ä¸Šè®¡ç®—ã€‚
# 6.  **é…ç½®**: æ‰€æœ‰å…³é”®å‚æ•°ï¼ˆåŒ…æ‹¬ `eval_only` å’Œ `resume_from_checkpoint`ï¼‰éƒ½é›†ä¸­åœ¨ `TrainingConfig` ç±»ä¸­ï¼Œæ–¹ä¾¿ç®¡ç†ã€‚
# 7.  **æ··åˆç²¾åº¦**: é»˜è®¤å¯ç”¨ `fp16` (float16) è¿›è¡Œè®­ç»ƒï¼Œä»¥åŠ é€Ÿå¹¶å‡å°‘æ˜¾å­˜ã€‚

# %%
# å®‰è£…å¿…è¦çš„åº“
!pip install -q evaluate rouge-score peft bitsandbytes

# %%
import pandas as pd
import os
import torch
import numpy as np
import warnings
from dataclasses import dataclass, field

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
)
from peft import LoraConfig, get_peft_model, TaskType
import evaluate # Hugging Face çš„ evaluate åº“

# å¿½ç•¥ä¸€äº›ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore")
os.environ["WANDB_DISABLED"] = "true" # ç¦ç”¨ wandb

# %% [markdown]
# ### ç¬¬1æ­¥ï¼šæ•°æ®é¢„å¤„ç†
# è¿™éƒ¨åˆ†ä»£ç ä¸åŸå§‹è„šæœ¬é€»è¾‘ç›¸åŒï¼Œè¯»å–`train.csv`å¹¶åˆ’åˆ†ä¸º95%çš„è®­ç»ƒé›†å’Œ5%çš„éªŒè¯é›†ã€‚

# %%
print("\n" + "=" * 80)
print("å‡†å¤‡SAMSumæ•°æ®é›†åˆ’åˆ†")
print("=" * 80)

# Kaggleç¯å¢ƒä¸‹çš„è¾“å…¥è·¯å¾„
input_csv = '/kaggle/input/nanogpt-fudannlp-cs-30040/train.csv'

# å¦‚æœè¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿæ–‡ä»¶ç”¨äºæœ¬åœ°æµ‹è¯•
if not os.path.exists(input_csv):
    print(f"è­¦å‘Š: æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶ {input_csv}ã€‚å°†åˆ›å»ºä¸€ä¸ªç”¨äºæ¼”ç¤ºçš„è™šæ‹Ÿæ–‡ä»¶ã€‚")
    os.makedirs('/kaggle/input/nanogpt-fudannlp-cs-30040', exist_ok=True)
    dummy_data = {
        'id': [f'id_{i}' for i in range(100)],
        'dialogue': ["A: Hi! B: Hello. How are you? A: I'm fine, thanks." for _ in range(100)],
        'summary': ["A and B greeted each other." for _ in range(100)]
    }
    pd.DataFrame(dummy_data).to_csv(input_csv, index=False)


print(f"\nè¯»å–æ•°æ®: {input_csv}")
df = pd.read_csv(input_csv)
total_samples = len(df)
print(f"æ€»æ ·æœ¬æ•°: {total_samples}")

# éšæœºæ‰“ä¹±æ•°æ®
df = df.sample(frac=1, random_state=42).reset_index(drop=True)

# è®¡ç®—åˆ’åˆ†ç‚¹
val_size = int(total_samples * 0.05)
train_size = total_samples - val_size
print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {train_size} ({train_size/total_samples*100:.1f}%)")
print(f"éªŒè¯é›†æ ·æœ¬æ•°: {val_size} ({val_size/total_samples*100:.1f}%)")

# åˆ’åˆ†æ•°æ®
train_df = df.iloc[:train_size]
val_df = df.iloc[train_size:]

# åˆ›å»ºè¾“å‡ºç›®å½•
output_dir = '/kaggle/working/data/samsum'
os.makedirs(output_dir, exist_ok=True)

# ä¿å­˜æ–‡ä»¶
train_csv_path = os.path.join(output_dir, 'train.csv')
val_csv_path = os.path.join(output_dir, 'validation.csv')
train_df.to_csv(train_csv_path, index=False)
val_df.to_csv(val_csv_path, index=False)

print(f"\nè®­ç»ƒé›†å·²ä¿å­˜: {train_csv_path}")
print(f"éªŒè¯é›†å·²ä¿å­˜: {val_csv_path}")
print("\næ•°æ®é›†åˆ’åˆ†å®Œæˆï¼")
print("=" * 80)


# %% [markdown]
# ### ç¬¬2æ­¥ï¼šé…ç½®å‚æ•°
# å°†æ‰€æœ‰è®­ç»ƒå’Œæ¨¡å‹ç›¸å…³çš„å‚æ•°é›†ä¸­åœ¨ä¸€ä¸ªé…ç½®ç±»ä¸­ã€‚

# %%
@dataclass
class TrainingConfig:
    # æ¨¡å‹å’Œåˆ†è¯å™¨é…ç½®
    model_name_or_path: str = "google/flan-t5-base" # T5-base çº¦250Må‚æ•°
    tokenizer_name_or_path: str = "google/flan-t5-base"
    
    # æ•°æ®é…ç½®
    data_path: str = output_dir
    source_prefix: str = "summarize: " # T5/FLAN-T5 æ¨èä¸ºä»»åŠ¡æ·»åŠ å‰ç¼€
    source_column: str = "dialogue"
    target_column: str = "summary"
    max_source_length: int = 512 # è¾“å…¥å¯¹è¯çš„æœ€å¤§é•¿åº¦
    max_target_length: int = 128 # è¾“å‡ºæ‘˜è¦çš„æœ€å¤§é•¿åº¦
    
    # PEFT (LoRA) é…ç½®
    use_peft: bool = True
    lora_r: int = 8
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    
    # è®­ç»ƒæ§åˆ¶å‚æ•°
    output_dir: str = "/kaggle/working/flan-t5-samsum-lora" # æ¨¡å‹æƒé‡å’Œç»“æœçš„è¾“å‡ºç›®å½•
    eval_only: bool = False # æ˜¯å¦åªè¿›è¡Œè¯„ä¼°è€Œä¸è®­ç»ƒ
    resume_from_checkpoint: bool = False # æ˜¯å¦ä»ä¸Šä¸€ä¸ªæ–­ç‚¹ç»§ç»­è®­ç»ƒ
    
    # Seq2SeqTrainingArguments å‚æ•°
    per_device_train_batch_size: int = 4
    per_device_eval_batch_size: int = 8
    gradient_accumulation_steps: int = 8 # æœ‰æ•ˆæ‰¹æ¬¡å¤§å° = 4 * 8 = 32
    learning_rate: float = 3e-4 # LoRA å¾®è°ƒé€šå¸¸ä½¿ç”¨æ¯”å…¨é‡å¾®è°ƒç¨å¤§çš„å­¦ä¹ ç‡
    num_train_epochs: int = 3
    logging_steps: int = 10 # æ¯10æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
    eval_steps: int = 50 # æ¯50æ­¥è¯„ä¼°ä¸€æ¬¡
    save_steps: int = 50 # æ¯50æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
    evaluation_strategy: str = "steps"
    save_strategy: str = "steps"
    save_total_limit: int = 2 # åªä¿ç•™æœ€æ–°çš„2ä¸ªæ£€æŸ¥ç‚¹
    load_best_model_at_end: bool = True # è®­ç»ƒç»“æŸååŠ è½½æœ€ä½³æ¨¡å‹
    predict_with_generate: bool = True # åœ¨è¯„ä¼°æ—¶ä½¿ç”¨generateç”Ÿæˆæ–‡æœ¬
    metric_for_best_model: str = "eval_rougeL" # ä½¿ç”¨ROUGE-Lä½œä¸ºè¡¡é‡æœ€ä½³æ¨¡å‹çš„æŒ‡æ ‡
    fp16: bool = True # å¿…é¡»ä½¿ç”¨ float16


config = TrainingConfig()

# %% [markdown]
# ### ç¬¬3æ­¥ï¼šåŠ è½½æ¨¡å‹ã€åˆ†è¯å™¨å’Œæ•°æ®

# %%
# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name_or_path)

# åŠ è½½æ¨¡å‹
# `torch_dtype=torch.float16` ç¡®ä¿æ¨¡å‹ä»¥fp16åŠ è½½
# `device_map="auto"` ä¼šè‡ªåŠ¨å°†æ¨¡å‹åˆ†é…åˆ°å¯ç”¨çš„è®¾å¤‡ä¸Šï¼ˆä¾‹å¦‚GPUï¼‰
model = AutoModelForSeq2SeqLM.from_pretrained(
    config.model_name_or_path,
    torch_dtype=torch.float16,
    device_map="auto"
)

# å¦‚æœé…ç½®ä½¿ç”¨PEFT (LoRA)
if config.use_peft:
    peft_config = LoraConfig(
        task_type=TaskType.SEQ_2_SEQ_LM,
        inference_mode=False,
        r=config.lora_r,
        lora_alpha=config.lora_alpha,
        lora_dropout=config.lora_dropout,
        target_modules=["q", "v"] # åœ¨T5çš„qå’ŒvæŠ•å½±çŸ©é˜µä¸Šåº”ç”¨LoRA
    )
    model = get_peft_model(model, peft_config)
    print("\nPEFT (LoRA) æ¨¡å‹å·²å¯ç”¨:")
    model.print_trainable_parameters()


# åŠ è½½æ•°æ®é›†
data_files = {
    "train": os.path.join(config.data_path, "train.csv"),
    "validation": os.path.join(config.data_path, "validation.csv"),
}
# ä½¿ç”¨ datasets åº“åŠ è½½CSVæ–‡ä»¶
dataset = load_dataset("csv", data_files=data_files)

print("\næ•°æ®é›†ç»“æ„:")
print(dataset)
print("\nè®­ç»ƒé›†æ ·æœ¬ç¤ºä¾‹:")
print(dataset["train"][0])


# %% [markdown]
# ### ç¬¬4æ­¥ï¼šæ•°æ®é¢„å¤„ç†å‡½æ•°å’Œè¯„ä¼°æŒ‡æ ‡

# %%
def preprocess_function(examples):
    """æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼Œç”¨äºå¯¹æ•°æ®è¿›è¡Œåˆ†è¯"""
    inputs = [config.source_prefix + doc for doc in examples[config.source_column]]
    model_inputs = tokenizer(inputs, max_length=config.max_source_length, truncation=True, padding="max_length")

    # å¯¹æ ‡ç­¾è¿›è¡Œåˆ†è¯
    labels = tokenizer(text_target=examples[config.target_column], max_length=config.max_target_length, truncation=True, padding="max_length")

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# ä½¿ç”¨ .map() æ–¹æ³•å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œé¢„å¤„ç†
tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset["train"].column_names)
print("\nåˆ†è¯åæ•°æ®é›†çš„åˆ—:", tokenized_dataset["train"].column_names)


# åŠ è½½ ROUGE è¯„ä¼°æŒ‡æ ‡
rouge = evaluate.load("rouge")

def compute_metrics(eval_pred):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°"""
    predictions, labels = eval_pred
    # å°†æ¨¡å‹ç”Ÿæˆçš„ token ID è§£ç ä¸ºæ–‡æœ¬
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # å°†æ ‡ç­¾ä¸­çš„ -100 æ›¿æ¢ä¸º padding token IDï¼Œä»¥ä¾¿è§£ç 
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # ROUGEæœŸæœ›æ¯å¥æ‘˜è¦åéƒ½æœ‰ä¸€ä¸ªæ¢è¡Œç¬¦
    decoded_preds = ["\n".join(pred.strip().split()) for pred in decoded_preds]
    decoded_labels = ["\n".join(label.strip().split()) for label in decoded_labels]

    # è®¡ç®— ROUGE åˆ†æ•°
    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
    
    # æå–ä¸»è¦çš„ f-measure åˆ†æ•°
    result = {key: value * 100 for key, value in result.items()}
    
    # æ·»åŠ ç”Ÿæˆæ–‡æœ¬çš„å¹³å‡é•¿åº¦
    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
    result["gen_len"] = np.mean(prediction_lens)

    return {k: round(v, 4) for k, v in result.items()}

# %% [markdown]
# ### ç¬¬5æ­¥ï¼šè®¾ç½® Trainer å¹¶å¼€å§‹è®­ç»ƒ

# %%
# å®šä¹‰æ•°æ®æ•´ç†å™¨ï¼Œç”¨äºåŠ¨æ€å¡«å……æ‰¹æ¬¡ä¸­çš„æ•°æ®
# `label_pad_token_id=-100` æ˜¯å…³é”®ï¼Œå®ƒèƒ½ç¡®ä¿å¡«å……çš„æ ‡ç­¾åœ¨è®¡ç®—æŸå¤±æ—¶è¢«å¿½ç•¥
data_collator = DataCollatorForSeq2Seq(
    tokenizer,
    model=model,
    label_pad_token_id=-100,
    pad_to_multiple_of=8
)

# å®šä¹‰è®­ç»ƒå‚æ•°
training_args = Seq2SeqTrainingArguments(
    output_dir=config.output_dir,
    per_device_train_batch_size=config.per_device_train_batch_size,
    per_device_eval_batch_size=config.per_device_eval_batch_size,
    gradient_accumulation_steps=config.gradient_accumulation_steps,
    predict_with_generate=config.predict_with_generate,
    fp16=config.fp16,
    learning_rate=config.learning_rate,
    num_train_epochs=config.num_train_epochs,
    logging_strategy="steps",
    logging_steps=config.logging_steps,
    evaluation_strategy=config.evaluation_strategy,
    eval_steps=config.eval_steps,
    save_strategy=config.save_strategy,
    save_steps=config.save_steps,
    save_total_limit=config.save_total_limit,
    load_best_model_at_end=config.load_best_model_at_end,
    metric_for_best_model=config.metric_for_best_model,
    report_to="none", # ç¦ç”¨ wandb/tensorboard
)

# åˆå§‹åŒ– Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    compute_metrics=compute_metrics,
)

# å¼€å§‹è®­ç»ƒ
if not config.eval_only:
    print("\n" + "=" * 80)
    print("ğŸš€ å¼€å§‹æ¨¡å‹å¾®è°ƒ...")
    print("=" * 80)
    # å¦‚æœ resume_from_checkpoint ä¸º Trueï¼ŒTrainer ä¼šè‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹
    trainer.train(resume_from_checkpoint=config.resume_from_checkpoint)
    
    # ä¿å­˜æœ€ç»ˆçš„ LoRA é€‚é…å™¨æƒé‡
    final_model_path = os.path.join(config.output_dir, "final_model")
    trainer.save_model(final_model_path)
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {final_model_path}")
else:
    print("\n" + "=" * 80)
    print("ğŸ” eval_only=Trueï¼Œè·³è¿‡è®­ç»ƒï¼Œç›´æ¥è¿›è¡Œè¯„ä¼°ã€‚")
    print("=" * 80)


# è¯„ä¼°æ¨¡å‹
print("\n" + "=" * 80)
print("ğŸ“ˆ å¼€å§‹æœ€ç»ˆè¯„ä¼°...")
print("=" * 80)
eval_results = trainer.evaluate()
print("\næœ€ç»ˆè¯„ä¼°ç»“æœ:")
print(eval_results)


# %% [markdown]
# ### ç¬¬6æ­¥ï¼šç”Ÿæˆæµ‹è¯•é›†ç»“æœå¹¶å¯¼å‡º

# %%
def predict_on_test_set(config):
    """
    åŠ è½½å¾®è°ƒåçš„æ¨¡å‹ï¼Œå¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼Œå¹¶ç”Ÿæˆ submission.csv æ–‡ä»¶ã€‚
    """
    print("\n" + "=" * 80)
    print("ğŸ“¦ å¼€å§‹å¯¹æµ‹è¯•é›†è¿›è¡Œæ¨ç†å¹¶ç”Ÿæˆæäº¤æ–‡ä»¶...")
    print("=" * 80)

    # åŠ è½½æµ‹è¯•æ•°æ®
    test_csv_path = '/kaggle/input/nanogpt-fudannlp-cs-30040/test.csv'
    if not os.path.exists(test_csv_path):
        print(f"è­¦å‘Š: æœªæ‰¾åˆ°æµ‹è¯•æ–‡ä»¶ {test_csv_path}ã€‚å°†åˆ›å»ºä¸€ä¸ªç”¨äºæ¼”ç¤ºçš„è™šæ‹Ÿæ–‡ä»¶ã€‚")
        dummy_test_data = {
            'id': [f'test_{i}' for i in range(50)],
            'dialogue': ["A: What's the plan for tonight? B: Let's go to the movies." for _ in range(50)],
        }
        pd.DataFrame(dummy_test_data).to_csv(test_csv_path, index=False)
        
    test_df = pd.read_csv(test_csv_path)
    print(f"åŠ è½½äº† {len(test_df)} æ¡æµ‹è¯•æ ·æœ¬ã€‚")

    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæ¨ç†
    # `trainer.model` å·²ç»åœ¨è®­ç»ƒç»“æŸååŠ è½½äº†æœ€ä½³æ¨¡å‹
    # å¦‚æœæ˜¯ eval_only æ¨¡å¼ï¼Œéœ€è¦æ‰‹åŠ¨åŠ è½½
    if config.eval_only:
        from peft import PeftModel
        from transformers import AutoModelForSeq2SeqLM
        
        # æ‰¾åˆ°æœ€ä½³æ£€æŸ¥ç‚¹
        best_checkpoint_path = trainer.state.best_model_checkpoint
        if best_checkpoint_path is None:
             print("é”™è¯¯ï¼šæ‰¾ä¸åˆ°æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒã€‚")
             return

        print(f"ä» {best_checkpoint_path} åŠ è½½åŸºç¡€æ¨¡å‹...")
        base_model = AutoModelForSeq2SeqLM.from_pretrained(
            config.model_name_or_path,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        print("åŠ è½½ LoRA é€‚é…å™¨...")
        inference_model = PeftModel.from_pretrained(base_model, best_checkpoint_path)
        inference_model = inference_model.merge_and_unload() # åˆå¹¶æƒé‡ä»¥ä¾¿äºæ¨ç†
        inference_model.eval()
    else:
        inference_model = trainer.model
        if config.use_peft:
            # å¦‚æœä½¿ç”¨çš„æ˜¯PEFTæ¨¡å‹ï¼Œæœ€å¥½åˆå¹¶æƒé‡ä»¥åŠ é€Ÿæ¨ç†
            try:
                inference_model = inference_model.merge_and_unload()
            except:
                print("æ— æ³•è‡ªåŠ¨åˆå¹¶LoRAæƒé‡ï¼Œå°†ä½¿ç”¨é€‚é…å™¨æ¨¡å¼è¿›è¡Œæ¨ç†ã€‚")
        inference_model.eval()


    # å‡†å¤‡ç”Ÿæˆç»“æœ
    results = []
    
    from torch.utils.data import DataLoader, Dataset

    class InferenceDataset(Dataset):
        def __init__(self, df, tokenizer, config):
            self.df = df
            self.tokenizer = tokenizer
            self.config = config

        def __len__(self):
            return len(self.df)

        def __getitem__(self, idx):
            dialogue = self.df.iloc[idx][self.config.source_column]
            input_text = self.config.source_prefix + dialogue
            return self.tokenizer(input_text, return_tensors="pt", max_length=self.config.max_source_length, truncation=True)
            
    inference_dataset = InferenceDataset(test_df, tokenizer, config)
    # ä½¿ç”¨ DataLoader è¿›è¡Œæ‰¹å¤„ç†ä»¥åŠ é€Ÿ
    data_loader = DataLoader(inference_dataset, batch_size=config.per_device_eval_batch_size)

    print("\nå¼€å§‹ç”Ÿæˆæ‘˜è¦...")
    for batch in data_loader:
        with torch.no_grad():
            input_ids = batch['input_ids'].to(inference_model.device)
            attention_mask = batch['attention_mask'].to(inference_model.device)
            
            outputs = inference_model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=config.max_target_length,
                num_beams=4, # ä½¿ç”¨æŸæœç´¢
                early_stopping=True
            )
            
            # è§£ç 
            summaries = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            # æ‰¾åˆ°æ‰¹æ¬¡å¯¹åº”çš„ID
            start_index = len(results)
            ids = test_df['id'][start_index : start_index + len(summaries)].tolist()
            
            for sample_id, summary in zip(ids, summaries):
                results.append({'id': sample_id, 'summary': summary.strip()})
        
        if len(results) % 100 == 0:
            print(f"å·²å¤„ç† {len(results)} / {len(test_df)}...")

    # ä¿å­˜åˆ° submission.csv
    submission_df = pd.DataFrame(results)
    submission_path = os.path.join(config.output_dir, "submission.csv")
    submission_df.to_csv(submission_path, index=False)
    
    print(f"\nâœ… æ¨ç†å®Œæˆï¼æäº¤æ–‡ä»¶å·²ä¿å­˜è‡³: {submission_path}")
    print("=" * 80)

# è°ƒç”¨é¢„æµ‹å‡½æ•°
predict_on_test_set(config)