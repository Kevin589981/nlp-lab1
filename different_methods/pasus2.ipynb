{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # 步骤 0: 环境设置\n\n 首先，安装所有必需的库。`accelerate` 和 `evaluate` 是 Hugging Face 生态系统的重要组成部分，可以简化训练和评估流程。","metadata":{}},{"cell_type":"code","source":"!pip install rouge_score\n!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T12:08:52.591964Z","iopub.execute_input":"2025-11-13T12:08:52.592478Z","iopub.status.idle":"2025-11-13T12:08:59.238950Z","shell.execute_reply.started":"2025-11-13T12:08:52.592453Z","shell.execute_reply":"2025-11-13T12:08:59.238149Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.3.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2025.11.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge_score) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.6)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.4.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.18)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":" # 步骤 1: 配置文件\n\n 我们将所有可配置的参数集中在一个类中，以便于管理和修改。\n\n 这包括模型名称、数据路径、训练参数等。\n\n 您可以在这里轻松切换 `eval_only` 和 `resume_from_checkpoint` 模式。","metadata":{}},{"cell_type":"code","source":"import torch\nfrom dataclasses import dataclass, field\n\n@dataclass\nclass TrainingConfig:\n    # --- 模型和数据路径配置 ---\n    model_checkpoint: str = \"google/pegasus-x-base\"\n    train_file_path: str = \"/kaggle/working/data/samsum/train.csv\"\n    validation_file_path: str = \"/kaggle/working/data/samsum/validation.csv\"\n    test_file_path: str = \"/kaggle/input/nanogpt-fudannlp-cs-30040/test.csv\"\n    output_dir: str = \"/kaggle/working/pegasus-samsum-finetuned\"\n    submission_file: str = \"/kaggle/working/submission.csv\"\n\n    # --- 运行模式配置 ---\n    # 如果为 True，将只进行评估，不进行训练。需要指定一个有效的 checkpoint 路径\n    eval_only: bool = False \n    # 从指定的 checkpoint 继续训练。设为 True 或 checkpoint 路径（例如 \"output_dir/checkpoint-500\"）\n    resume_from_checkpoint: bool = False\n\n    # --- 数据处理配置 ---\n    max_input_length: int = 1024  # 输入（对话）的最大长度\n    max_target_length: int = 128   # 输出（摘要）的最大长度\n    \n    # --- 训练参数配置 (Seq2SeqTrainingArguments) ---\n    # 评估策略，\"epoch\" 表示每个 epoch 结束后进行一次评估\n    evaluation_strategy: str = \"steps\"\n    eval_steps: int = 100 # 每100步评估一次\n    save_steps: int = 100 # 每100步保存一次\n    logging_steps: int = 25 # 每25步打印一次日志\n    \n    # 学习率和优化器\n    learning_rate: float = 2e-5\n    weight_decay: float = 0.01\n    \n    # 批次大小和梯度累积\n    per_device_train_batch_size: int = 2\n    per_device_eval_batch_size: int = 4\n    gradient_accumulation_steps: int = 8  # 有效批次大小 = 2 * 8 = 16\n    \n    # 训练周期\n    num_train_epochs: int = 3\n    \n    # 混合精度训练 (bf16 适用于 Ampere 架构如 T4, A100; fp16 适用于 P100)\n    bf16: bool = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n    fp16: bool = not bf16 and torch.cuda.is_available()\n    \n    # 其他\n    save_total_limit: int = 2 # 只保留最新的 2 个 checkpoint\n    predict_with_generate: bool = True # 在评估时使用 generate 方法，以计算 ROUGE\n    load_best_model_at_end: bool = True # 训练结束后加载最佳模型\n\n    # --- 生成参数配置 (用于评估和推理) ---\n    generation_num_beams: int = 4\n    generation_max_length: int = 128\n\n# 实例化配置\nconfig = TrainingConfig()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T12:08:59.241002Z","iopub.execute_input":"2025-11-13T12:08:59.241292Z","iopub.status.idle":"2025-11-13T12:08:59.251241Z","shell.execute_reply.started":"2025-11-13T12:08:59.241267Z","shell.execute_reply":"2025-11-13T12:08:59.250452Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":" # 步骤 2: 数据准备\n\n 这部分代码与您提供的原始代码相同。它读取原始的 `train.csv`，\n\n 将其打乱并划分为 95% 的训练集和 5% 的验证集，以便后续使用。\n\n 我们只取前1000条数据用于快速演示。如果要训练完整数据集，请注释掉 `df = df[:1000]` 这一行。","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"准备 SAMSum 数据集划分\")\nprint(\"=\" * 80)\n\n# 读取原始CSV文件\ninput_csv = '/kaggle/input/nanogpt-fudannlp-cs-30040/train.csv'\nprint(f\"\\n读取数据: {input_csv}\")\n\ndf = pd.read_csv(input_csv)\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# !!! 注意: 为了快速演示，我们只使用前100个样本。\n# !!! 如果要进行完整训练，请注释掉或删除下面这一行。\ndf = df[:100]\n\ntotal_samples = len(df)\nprint(f\"总样本数: {total_samples}\")\n\n# 计算划分点（5%作为验证集）\nval_size = int(total_samples * 0.05)\ntrain_size = total_samples - val_size\n\nprint(f\"训练集样本数: {train_size} ({train_size/total_samples*100:.1f}%)\")\nprint(f\"验证集样本数: {val_size} ({val_size/total_samples*100:.1f}%)\")\n\n# 划分数据\ntrain_df = df.iloc[:train_size]\nval_df = df.iloc[train_size:]\n\n# 创建输出目录\noutput_dir = os.path.dirname(config.train_file_path)\nos.makedirs(output_dir, exist_ok=True)\n\n# 保存训练集和验证集\ntrain_df.to_csv(config.train_file_path, index=False)\nprint(f\"\\n训练集已保存: {config.train_file_path}\")\n\nval_df.to_csv(config.validation_file_path, index=False)\nprint(f\"验证集已保存: {config.validation_file_path}\")\n\nprint(\"\\n数据集划分完成！\")\nprint(\"=\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T12:08:59.252041Z","iopub.execute_input":"2025-11-13T12:08:59.252309Z","iopub.status.idle":"2025-11-13T12:08:59.552333Z","shell.execute_reply.started":"2025-11-13T12:08:59.252293Z","shell.execute_reply":"2025-11-13T12:08:59.551537Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\n准备 SAMSum 数据集划分\n================================================================================\n\n读取数据: /kaggle/input/nanogpt-fudannlp-cs-30040/train.csv\n总样本数: 100\n训练集样本数: 95 (95.0%)\n验证集样本数: 5 (5.0%)\n\n训练集已保存: /kaggle/working/data/samsum/train.csv\n验证集已保存: /kaggle/working/data/samsum/validation.csv\n\n数据集划分完成！\n================================================================================\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":" # 步骤 3: 加载数据、模型和 Tokenizer\n\n 我们使用 `datasets` 库加载 CSV 文件，并加载 PEGASUS 模型及其对应的 Tokenizer。","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\n# 加载数据集\nraw_datasets = load_dataset('csv', data_files={'train': config.train_file_path, 'validation': config.validation_file_path})\n\nprint(\"\\n数据集结构:\")\nprint(raw_datasets)\n\n# 加载 Tokenizer 和模型\nprint(f\"\\n加载预训练模型: {config.model_checkpoint}\")\ntokenizer = AutoTokenizer.from_pretrained(config.model_checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(config.model_checkpoint)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T12:08:59.553234Z","iopub.execute_input":"2025-11-13T12:08:59.553778Z","execution_failed":"2025-11-13T12:08:58.623Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7a09b1c291440b994b685085543ecaf"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":" # 步骤 4: 数据预处理\n\n 定义一个函数来对数据进行分词（tokenize）。\n\n - `dialogue` 列作为模型的输入。\n\n - `summary` 列作为标签（label）。\n\n Tokenizer 会自动处理截断和填充的准备工作。","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = tokenizer(examples[\"dialogue\"], max_length=config.max_input_length, truncation=True, padding=\"max_length\")\n    \n    # 将摘要作为标签进行分词\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(examples[\"summary\"], max_length=config.max_target_length, truncation=True, padding=\"max_length\")\n\n    inputs[\"labels\"] = labels[\"input_ids\"]\n    return inputs\n\n# 使用 .map() 方法将预处理函数应用到整个数据集\n# batched=True 可以一次性处理多个样本，加快速度\nprint(\"\\n开始对数据集进行分词...\")\ntokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\nprint(\"分词完成！\")\n\nprint(\"\\n处理后的数据集结构:\")\nprint(tokenized_datasets)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-13T12:08:58.623Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" # 步骤 5: 定义评估指标 (ROUGE)\n\n 我们需要定义一个函数，在评估过程中计算 ROUGE 分数。\n\n 这个函数会在每个评估步骤被 `Trainer` 自动调用。","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np\nimport evaluate\n\n# 加载 ROUGE 评估指标\nrouge = evaluate.load(\"rouge\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    \n    # 解码生成的摘要\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    \n    # 将标签中的 -100 替换为 padding token ID，以便解码\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # ROUGE 需要在每句话后添加换行符\n    decoded_preds = [\"\\n\".join(pred.strip().split()) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(label.strip().split()) for label in decoded_labels]\n    \n    # 计算 ROUGE 分数\n    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    \n    # 提取关键指标\n    result = {key: value * 100 for key, value in result.items()}\n    \n    # 添加生成文本的平均长度\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    \n    return {k: round(v, 4) for k, v in result.items()}\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-13T12:08:58.624Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" # 步骤 6: 配置和初始化 Trainer\n\n - `Seq2SeqTrainingArguments`: 定义所有训练超参数。\n\n - `DataCollatorForSeq2Seq`: 负责在每个批次中智能地填充（pad）输入和标签。\n\n - `Seq2SeqTrainer`: 封装了所有训练和评估逻辑的核心类。","metadata":{}},{"cell_type":"code","source":"# 定义训练参数\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=config.output_dir,\n    eval_strategy=config.evaluation_strategy,\n    eval_steps=config.eval_steps,\n    save_steps=config.save_steps,\n    logging_steps=config.logging_steps,\n    learning_rate=config.learning_rate,\n    per_device_train_batch_size=config.per_device_train_batch_size,\n    per_device_eval_batch_size=config.per_device_eval_batch_size,\n    gradient_accumulation_steps=config.gradient_accumulation_steps,\n    weight_decay=config.weight_decay,\n    save_total_limit=config.save_total_limit,\n    num_train_epochs=config.num_train_epochs,\n    predict_with_generate=config.predict_with_generate,\n    fp16=config.fp16,\n    bf16=config.bf16,\n    load_best_model_at_end=config.load_best_model_at_end,\n    report_to=\"none\",  # 可设置为 \"wandb\", \"tensorboard\" 等\n)\n\n# 定义数据整理器\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n# 初始化 Trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-13T12:08:58.624Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" # 步骤 7: 执行训练或评估\n\n 根据 `config.eval_only` 的值，执行相应的操作。","metadata":{}},{"cell_type":"code","source":"if config.eval_only:\n    print(\"\\n\" + \"=\"*80)\n    print(\"模式: 只进行评估\")\n    print(\"=\"*80)\n    # 确保 resume_from_checkpoint 指向一个有效的模型路径\n    if not os.path.isdir(str(config.resume_from_checkpoint)):\n         raise ValueError(f\"eval_only=True, 但 resume_from_checkpoint ('{config.resume_from_checkpoint}') 不是一个有效的目录。\")\n    \n    print(f\"从 checkpoint 加载模型进行评估: {config.resume_from_checkpoint}\")\n    eval_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])\n    print(\"\\n评估结果:\")\n    print(eval_results)\nelse:\n    print(\"\\n\" + \"=\"*80)\n    print(\"模式: 开始训练\")\n    print(\"=\"*80)\n    # 如果设置了 resume_from_checkpoint，则从断点继续训练\n    train_result = trainer.train(resume_from_checkpoint=config.resume_from_checkpoint)\n    \n    # 保存最终的模型和训练状态\n    trainer.save_model()\n    trainer.save_state()\n    \n    print(\"\\n训练完成!\")\n    metrics = train_result.metrics\n    metrics[\"train_samples\"] = len(tokenized_datasets[\"train\"])\n    trainer.log_metrics(\"train\", metrics)\n    trainer.save_metrics(\"train\", metrics)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-13T12:08:58.624Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" # 步骤 8: 在测试集上进行推理并生成提交文件\n\n 训练（或加载）好的最佳模型现在可用于为测试集生成摘要。\n\n 我们将结果保存为 `submission.csv`。","metadata":{}},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"开始在测试集上进行推理...\")\nprint(\"=\"*80)\n\n# 加载测试数据\ntest_df = pd.read_csv(config.test_file_path)\nprint(f\"加载了 {len(test_df)} 条测试样本。\")\n\n# 使用 trainer.predict 进行高效推理\n# 注意：trainer 内部已经加载了训练过程中的最佳模型（因为 load_best_model_at_end=True）\ntest_dataset = load_dataset(\"csv\", data_files={\"test\": config.test_file_path})[\"test\"]\n\ndef tokenize_test_data(examples):\n    return tokenizer(examples[\"dialogue\"], max_length=config.max_input_length, truncation=True)\n\ntokenized_test_dataset = test_dataset.map(tokenize_test_data, batched=True)\n\nprint(\"开始生成预测...\")\npredictions = trainer.predict(\n    tokenized_test_dataset,\n    max_length=config.generation_max_length,\n    num_beams=config.generation_num_beams,\n)\n\n# 解码预测结果\ndecoded_summaries = tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True)\n\n# 清理生成的文本\ncleaned_summaries = [s.strip() for s in decoded_summaries]\n\n# 创建提交文件\nsubmission_df = pd.DataFrame({\n    'id': test_df['id'],\n    'summary': cleaned_summaries\n})\n\nsubmission_df.to_csv(config.submission_file, index=False)\n\nprint(f\"\\n推理完成！提交文件已保存至: {config.submission_file}\")\nprint(\"\\n提交文件预览:\")\nprint(submission_df.head())\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-13T12:08:58.624Z"}},"outputs":[],"execution_count":null}]}