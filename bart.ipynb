{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " # 处理数据\n",
    "\n",
    " 右侧点击Add Input，找到我们的比赛，然后添加\n",
    "\n",
    "\n",
    "\n",
    " 从Kaggle输入的train.csv读取数据，随机划分为训练集(95%)和验证集(5%)\n",
    "\n",
    "\n",
    "\n",
    " 保存到/kaggle/working/data/samsum目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"准备SAMSum数据集划分\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 读取原始CSV文件\n",
    "input_csv = '/kaggle/input/nanogpt-fudannlp-cs-30040/train.csv'\n",
    "print(f\"\\n读取数据: {input_csv}\")\n",
    "\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# 随机打乱数据\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df = df[:100]  # 仅使用前100条数据进行快速测试\n",
    "\n",
    "total_samples = len(df)\n",
    "print(f\"总样本数: {total_samples}\")\n",
    "\n",
    "# 计算划分点（5%作为验证集）\n",
    "val_size = int(total_samples * 0.05)\n",
    "train_size = total_samples - val_size\n",
    "\n",
    "print(f\"训练集样本数: {train_size} ({train_size/total_samples*100:.1f}%)\")\n",
    "print(f\"验证集样本数: {val_size} ({val_size/total_samples*100:.1f}%)\")\n",
    "\n",
    "# 划分数据\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:]\n",
    "\n",
    "# 创建输出目录\n",
    "output_dir = '/kaggle/working/data/samsum'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 保存训练集\n",
    "train_csv_path = os.path.join(output_dir, 'train.csv')\n",
    "train_df.to_csv(train_csv_path, index=False)\n",
    "print(f\"\\n训练集已保存: {train_csv_path}\")\n",
    "\n",
    "# 保存验证集\n",
    "val_csv_path = os.path.join(output_dir, 'validation.csv')\n",
    "val_df.to_csv(val_csv_path, index=False)\n",
    "print(f\"验证集已保存: {val_csv_path}\")\n",
    "\n",
    "print(\"\\n数据集划分完成！\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 配置和数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
    "\n",
    "# =============================================================================\n",
    "# 配置参数\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"配置类：包含所有可调参数\"\"\"\n",
    "    \n",
    "    # 数据集配置\n",
    "    dataset_path = '/kaggle/working/data/samsum'\n",
    "    dataset = 'samsum'\n",
    "    \n",
    "    # 模型配置\n",
    "    model_name = 'facebook/bart-base'\n",
    "    \n",
    "    # 训练配置\n",
    "    init_from = 'pretrained'  # 'pretrained', 'resume', 'scratch'\n",
    "    resume_from = None  # checkpoint路径，当init_from='resume'时使用\n",
    "    \n",
    "    # 批次配置\n",
    "    batch_size = 2\n",
    "    gradient_accumulation_steps = 64\n",
    "    max_source_length = 512  # 输入对话的最大长度\n",
    "    max_target_length = 128  # 目标摘要的最大长度\n",
    "    \n",
    "    # 训练步数\n",
    "    max_iters = 500\n",
    "    \n",
    "    # 优化器配置\n",
    "    learning_rate = 3e-5\n",
    "    weight_decay = 0.01\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    grad_clip = 1.0\n",
    "    \n",
    "    # 学习率调度\n",
    "    decay_lr = False\n",
    "    warmup_iters = 100\n",
    "    lr_decay_iters = 500\n",
    "    min_lr = 1e-6\n",
    "    \n",
    "    # I/O配置\n",
    "    out_dir = 'out-bart-summarization'\n",
    "    eval_interval = 10\n",
    "    log_interval = 5\n",
    "    eval_iters = 40\n",
    "    eval_only = False\n",
    "    always_save_checkpoint = False\n",
    "    \n",
    "    # ROUGE评估配置\n",
    "    eval_rouge_during_training = True\n",
    "    rouge_eval_samples = 5\n",
    "    \n",
    "    # wandb日志\n",
    "    wandb_log = False\n",
    "    wandb_project = 'bart-summarization'\n",
    "    wandb_run_name = 'bart-base'\n",
    "    \n",
    "    # 系统配置\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "    compile = False\n",
    "    \n",
    "    # 生成配置\n",
    "    num_test_samples = 10\n",
    "    num_beams = 4\n",
    "    temperature = 1.0\n",
    "    top_k = 50\n",
    "    top_p = 0.95\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# =============================================================================\n",
    "# 数据集类\n",
    "# =============================================================================\n",
    "\n",
    "class SummarizationDataset(Dataset):\n",
    "    \"\"\"BART摘要任务的Dataset类\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path, tokenizer, max_source_length, max_target_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        \n",
    "        # 读取CSV文件\n",
    "        self.dialogues = []\n",
    "        self.summaries = []\n",
    "        with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                self.dialogues.append(row['dialogue'])\n",
    "                self.summaries.append(row['summary'])\n",
    "        \n",
    "        print(f\"  加载了 {len(self.dialogues)} 个样本\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dialogue = self.dialogues[idx]\n",
    "        summary = self.summaries[idx]\n",
    "        \n",
    "        # Tokenize输入（对话）\n",
    "        source = self.tokenizer(\n",
    "            dialogue,\n",
    "            max_length=self.max_source_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize目标（摘要）\n",
    "        target = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_target_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        \n",
    "        # 将padding token设置为-100，这样在计算loss时会被忽略\n",
    "        labels = target_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': source_ids,\n",
    "            'attention_mask': source_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# 全局变量：缓存DataLoader\n",
    "_dataloaders = {'train': None, 'val': None}\n",
    "_data_iters = {'train': None, 'val': None}\n",
    "\n",
    "def get_batch(split, tokenizer):\n",
    "    \"\"\"获取一个训练批次\"\"\"\n",
    "    global _dataloaders, _data_iters\n",
    "    \n",
    "    # 首次调用：创建DataLoader\n",
    "    if _dataloaders[split] is None:\n",
    "        csv_file = 'train.csv' if split == 'train' else 'validation.csv'\n",
    "        csv_path = os.path.join(config.dataset_path, csv_file)\n",
    "        \n",
    "        dataset = SummarizationDataset(\n",
    "            csv_path,\n",
    "            tokenizer,\n",
    "            config.max_source_length,\n",
    "            config.max_target_length\n",
    "        )\n",
    "        \n",
    "        shuffle = (split == 'train')\n",
    "        \n",
    "        _dataloaders[split] = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if 'cuda' in config.device else False,\n",
    "        )\n",
    "        _data_iters[split] = iter(_dataloaders[split])\n",
    "    \n",
    "    # 获取下一个batch\n",
    "    try:\n",
    "        batch = next(_data_iters[split])\n",
    "    except StopIteration:\n",
    "        _data_iters[split] = iter(_dataloaders[split])\n",
    "        batch = next(_data_iters[split])\n",
    "    \n",
    "    # 移动到设备\n",
    "    if 'cuda' in config.device:\n",
    "        batch = {k: v.to(config.device, non_blocking=True) for k, v in batch.items()}\n",
    "    else:\n",
    "        batch = {k: v.to(config.device) for k, v in batch.items()}\n",
    "    \n",
    "    return batch\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, ctx, tokenizer):\n",
    "    \"\"\"估计训练集和验证集上的损失\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(config.eval_iters)\n",
    "        for k in range(config.eval_iters):\n",
    "            batch = get_batch(split, tokenizer)\n",
    "            with ctx:\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_lr(iter_num):\n",
    "    \"\"\"学习率调度：带预热的余弦衰减\"\"\"\n",
    "    if iter_num < config.warmup_iters:\n",
    "        return config.learning_rate * (iter_num + 1) / (config.warmup_iters + 1)\n",
    "    \n",
    "    if iter_num > config.lr_decay_iters:\n",
    "        return config.min_lr\n",
    "    \n",
    "    decay_ratio = (iter_num - config.warmup_iters) / (config.lr_decay_iters - config.warmup_iters)\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return config.min_lr + coeff * (config.learning_rate - config.min_lr)\n",
    "\n",
    "# =============================================================================\n",
    "# ROUGE评估\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_rouge(reference_summary, generated_summary):\n",
    "    \"\"\"计算ROUGE分数\"\"\"\n",
    "    try:\n",
    "        from rouge_score import rouge_scorer\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        scores = scorer.score(reference_summary, generated_summary)\n",
    "        return {\n",
    "            'rouge1': scores['rouge1'].fmeasure,\n",
    "            'rouge2': scores['rouge2'].fmeasure,\n",
    "            'rougeL': scores['rougeL'].fmeasure\n",
    "        }\n",
    "    except ImportError:\n",
    "        return None\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_rouge_during_training(model, tokenizer, ctx, num_samples=5):\n",
    "    \"\"\"在训练过程中评估ROUGE分数\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 读取验证集\n",
    "    val_csv = os.path.join(config.dataset_path, 'validation.csv')\n",
    "    if not os.path.exists(val_csv):\n",
    "        model.train()\n",
    "        return None\n",
    "    \n",
    "    dialogues = []\n",
    "    summaries = []\n",
    "    with open(val_csv, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            dialogues.append(row['dialogue'])\n",
    "            summaries.append(row['summary'])\n",
    "    \n",
    "    # 随机选择样本\n",
    "    import random\n",
    "    indices = random.sample(range(len(dialogues)), min(num_samples, len(dialogues)))\n",
    "    \n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        dialogue = dialogues[idx]\n",
    "        reference_summary = summaries[idx]\n",
    "        \n",
    "        # Tokenize输入\n",
    "        inputs = tokenizer(\n",
    "            dialogue,\n",
    "            max_length=config.max_source_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(config.device)\n",
    "        \n",
    "        # 生成摘要\n",
    "        with ctx:\n",
    "            outputs = model.generate(\n",
    "                inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_length=config.max_target_length,\n",
    "                num_beams=config.num_beams,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 计算ROUGE\n",
    "        rouge_scores = calculate_rouge(reference_summary, generated_summary)\n",
    "        if rouge_scores:\n",
    "            rouge1_scores.append(rouge_scores['rouge1'])\n",
    "            rouge2_scores.append(rouge_scores['rouge2'])\n",
    "            rougeL_scores.append(rouge_scores['rougeL'])\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    if len(rouge1_scores) > 0:\n",
    "        return {\n",
    "            'rouge1': np.mean(rouge1_scores),\n",
    "            'rouge2': np.mean(rouge2_scores),\n",
    "            'rougeL': np.mean(rougeL_scores)\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# =============================================================================\n",
    "# 训练函数\n",
    "# =============================================================================\n",
    "\n",
    "def train():\n",
    "    \"\"\"训练主函数\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"开始训练...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(1337)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # 创建输出目录\n",
    "    os.makedirs(config.out_dir, exist_ok=True)\n",
    "    \n",
    "    # 设置设备和精度\n",
    "    device_type = 'cuda' if 'cuda' in config.device else 'cpu'\n",
    "    ptdtype = {\n",
    "        'float32': torch.float32,\n",
    "        'bfloat16': torch.bfloat16,\n",
    "        'float16': torch.float16\n",
    "    }[config.dtype]\n",
    "    ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(\n",
    "        device_type=device_type, dtype=ptdtype\n",
    "    )\n",
    "    \n",
    "    # 初始化tokenizer\n",
    "    print(f\"\\n加载tokenizer: {config.model_name}\")\n",
    "    tokenizer = BartTokenizer.from_pretrained(config.model_name)\n",
    "    \n",
    "    # 初始化模型\n",
    "    print(f\"\\n模型初始化方式: {config.init_from}\")\n",
    "    \n",
    "    if config.init_from == 'scratch':\n",
    "        print(\"从头开始训练新模型\")\n",
    "        model_config = BartConfig.from_pretrained(config.model_name)\n",
    "        model = BartForConditionalGeneration(model_config)\n",
    "        iter_num = 0\n",
    "        best_val_loss = 1e9\n",
    "        \n",
    "    elif config.init_from == 'resume':\n",
    "        print(f\"从checkpoint恢复训练\")\n",
    "        if config.resume_from is None:\n",
    "            ckpt_path = os.path.join(config.out_dir, 'ckpt.pt')\n",
    "        else:\n",
    "            ckpt_path = config.resume_from\n",
    "        \n",
    "        checkpoint = torch.load(ckpt_path, map_location=config.device)\n",
    "        model = BartForConditionalGeneration.from_pretrained(config.model_name)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        iter_num = checkpoint['iter_num']\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        \n",
    "    else:  # pretrained\n",
    "        print(f\"从预训练模型加载: {config.model_name}\")\n",
    "        model = BartForConditionalGeneration.from_pretrained(config.model_name)\n",
    "        iter_num = 0\n",
    "        best_val_loss = 1e9\n",
    "    \n",
    "    model.to(config.device)\n",
    "    \n",
    "    # 初始化优化器\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        betas=(config.beta1, config.beta2),\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    \n",
    "    if config.init_from == 'resume' and 'optimizer' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    # 初始化GradScaler\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(config.dtype == 'float16'))\n",
    "    \n",
    "    # 编译模型（可选）\n",
    "    if config.compile:\n",
    "        print(\"编译模型...\")\n",
    "        model = torch.compile(model)\n",
    "    \n",
    "    # 训练循环\n",
    "    print(\"\\n开始训练循环...\")\n",
    "    print(f\"总迭代次数: {config.max_iters}\")\n",
    "    print(f\"批次大小: {config.batch_size}\")\n",
    "    print(f\"梯度累积步数: {config.gradient_accumulation_steps}\")\n",
    "    print(f\"有效批次大小: {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    local_iter_num = 0\n",
    "    \n",
    "    while True:\n",
    "        # 设置学习率\n",
    "        lr = get_lr(iter_num) if config.decay_lr else config.learning_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        # 评估和保存checkpoint\n",
    "        if iter_num % config.eval_interval == 0:\n",
    "            losses = estimate_loss(model, ctx, tokenizer)\n",
    "            print(f\"\\nStep {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            \n",
    "            # 计算ROUGE分数\n",
    "            if iter_num > 0 and config.eval_rouge_during_training:\n",
    "                print(\"  评估ROUGE分数...\")\n",
    "                rouge_scores = evaluate_rouge_during_training(\n",
    "                    model, tokenizer, ctx, num_samples=config.rouge_eval_samples\n",
    "                )\n",
    "                if rouge_scores:\n",
    "                    print(f\"  ROUGE-1: {rouge_scores['rouge1']:.4f}, \"\n",
    "                          f\"ROUGE-2: {rouge_scores['rouge2']:.4f}, \"\n",
    "                          f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "            \n",
    "            # 保存checkpoint\n",
    "            if losses['val'] < best_val_loss or config.always_save_checkpoint:\n",
    "                best_val_loss = losses['val']\n",
    "                if iter_num > 0:\n",
    "                    checkpoint = {\n",
    "                        'model': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'iter_num': iter_num,\n",
    "                        'best_val_loss': best_val_loss,\n",
    "                        'config': vars(config),\n",
    "                    }\n",
    "                    print(f\"  保存checkpoint到 {config.out_dir}\")\n",
    "                    torch.save(checkpoint, os.path.join(config.out_dir, 'ckpt.pt'))\n",
    "        \n",
    "        if iter_num == 0 and config.eval_only:\n",
    "            break\n",
    "        \n",
    "        # 训练步骤\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for micro_step in range(config.gradient_accumulation_steps):\n",
    "            batch = get_batch('train', tokenizer)\n",
    "            \n",
    "            with ctx:\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                loss = loss / config.gradient_accumulation_steps\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "        \n",
    "        # 梯度裁剪\n",
    "        if config.grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "        \n",
    "        # 更新参数\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # 记录日志\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        \n",
    "        if iter_num % config.log_interval == 0:\n",
    "            lossf = loss.item() * config.gradient_accumulation_steps\n",
    "            print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, lr {lr:.2e}\")\n",
    "        \n",
    "        iter_num += 1\n",
    "        local_iter_num += 1\n",
    "        \n",
    "        if iter_num > config.max_iters:\n",
    "            break\n",
    "    \n",
    "    print(\"\\n训练完成！\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# 主函数\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    print(\"\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"BART 摘要微调\".center(80))\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n当前配置:\")\n",
    "    print(f\"  数据集: {config.dataset_path}\")\n",
    "    print(f\"  模型: {config.model_name}\")\n",
    "    print(f\"  初始化方式: {config.init_from}\")\n",
    "    print(f\"  设备: {config.device}\")\n",
    "    print(f\"  精度: {config.dtype}\")\n",
    "    print(f\"  批次大小: {config.batch_size}\")\n",
    "    print(f\"  最大迭代次数: {config.max_iters}\")\n",
    "    print(f\"  学习率: {config.learning_rate}\")\n",
    "    \n",
    "    if not config.eval_only:\n",
    "        train()\n",
    "    else:\n",
    "        print(\"\\neval_only=True，跳过训练\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \"\"\"加载训练好的模型\"\"\"\n",
    "    device_type = 'cuda' if 'cuda' in config.device else 'cpu'\n",
    "    ptdtype = {\n",
    "        'float32': torch.float32,\n",
    "        'bfloat16': torch.bfloat16,\n",
    "        'float16': torch.float16\n",
    "    }[config.dtype]\n",
    "    ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(\n",
    "        device_type=device_type, dtype=ptdtype\n",
    "    )\n",
    "    \n",
    "    # 加载tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(config.model_name)\n",
    "    \n",
    "    # 加载模型\n",
    "    print(f\"\\n从 {config.out_dir} 加载模型...\")\n",
    "    ckpt_path = os.path.join(config.out_dir, 'ckpt.pt')\n",
    "    \n",
    "    if not os.path.exists(ckpt_path):\n",
    "        print(f\"错误: 找不到checkpoint文件 {ckpt_path}\")\n",
    "        print(\"请先运行训练！\")\n",
    "        return None, None, None\n",
    "    \n",
    "    checkpoint = torch.load(ckpt_path, map_location=config.device)\n",
    "    model = BartForConditionalGeneration.from_pretrained(config.model_name)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "    \n",
    "    if config.compile:\n",
    "        print(\"编译模型...\")\n",
    "        model = torch.compile(model)\n",
    "    \n",
    "    return model, tokenizer, ctx\n",
    "\n",
    "def evaluate():\n",
    "    \"\"\"评估模型性能\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"开始评估...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 加载模型\n",
    "    model, tokenizer, ctx = load_model()\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    # 加载测试数据\n",
    "    print(\"\\n加载测试数据...\")\n",
    "    test_file = os.path.join(config.dataset_path, 'validation.csv')\n",
    "    \n",
    "    dialogues = []\n",
    "    summaries = []\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            if i >= config.num_test_samples:\n",
    "                break\n",
    "            dialogues.append(row['dialogue'])\n",
    "            summaries.append(row['summary'])\n",
    "    \n",
    "    print(f\"加载了 {len(dialogues)} 条测试样本\")\n",
    "    \n",
    "    # 检查ROUGE\n",
    "    rouge_result = calculate_rouge(\"test\", \"test\")\n",
    "    use_rouge = rouge_result is not None\n",
    "    if not use_rouge:\n",
    "        print(\"\\n警告: 未安装rouge_score库，将跳过ROUGE评分\")\n",
    "    \n",
    "    # 评估每个样本\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"开始生成和评估...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    all_rouge1_f = []\n",
    "    all_rouge2_f = []\n",
    "    all_rougeL_f = []\n",
    "    \n",
    "    for idx, (dialogue, reference_summary) in enumerate(zip(dialogues, summaries)):\n",
    "        print(f\"\\n[样本 {idx+1}/{len(dialogues)}]\")\n",
    "        print(f\"对话: {dialogue[:100]}...\" if len(dialogue) > 100 else f\"对话: {dialogue}\")\n",
    "        \n",
    "        # Tokenize输入\n",
    "        inputs = tokenizer(\n",
    "            dialogue,\n",
    "            max_length=config.max_source_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(config.device)\n",
    "        \n",
    "        # 生成摘要\n",
    "        with torch.no_grad():\n",
    "            with ctx:\n",
    "                outputs = model.generate(\n",
    "                    inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    max_length=config.max_target_length,\n",
    "                    num_beams=config.num_beams,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "        \n",
    "        generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"真实摘要: {reference_summary}\")\n",
    "        print(f\"生成摘要: {generated_summary}\")\n",
    "        \n",
    "        # 计算ROUGE分数\n",
    "        if use_rouge:\n",
    "            rouge_scores = calculate_rouge(reference_summary, generated_summary)\n",
    "            if rouge_scores:\n",
    "                print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "                print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "                print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "                \n",
    "                all_rouge1_f.append(rouge_scores['rouge1'])\n",
    "                all_rouge2_f.append(rouge_scores['rouge2'])\n",
    "                all_rougeL_f.append(rouge_scores['rougeL'])\n",
    "    \n",
    "    # 打印平均分数\n",
    "    if use_rouge and len(all_rouge1_f) > 0:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"平均ROUGE分数:\")\n",
    "        print(f\"  ROUGE-1: {np.mean(all_rouge1_f):.4f}\")\n",
    "        print(f\"  ROUGE-2: {np.mean(all_rouge2_f):.4f}\")\n",
    "        print(f\"  ROUGE-L: {np.mean(all_rougeL_f):.4f}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_set():\n",
    "    \"\"\"对测试集进行推理\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"开始对测试集进行推理...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 加载模型\n",
    "    model, tokenizer, ctx = load_model()\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    # 读取测试数据\n",
    "    print(\"\\n加载测试数据...\")\n",
    "    test_file = '/kaggle/input/nanogpt-fudannlp-cs-30040/test.csv'\n",
    "    \n",
    "    if not os.path.exists(test_file):\n",
    "        print(f\"错误: 找不到测试文件 {test_file}\")\n",
    "        return\n",
    "    \n",
    "    test_data = []\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            test_data.append({\n",
    "                'id': row['id'],\n",
    "                'dialogue': row['dialogue']\n",
    "            })\n",
    "    \n",
    "    print(f\"加载了 {len(test_data)} 条测试样本\")\n",
    "    \n",
    "    # 准备保存结果\n",
    "    results = []\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"开始生成摘要...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, sample in enumerate(tqdm(test_data)):\n",
    "        sample_id = sample['id']\n",
    "        dialogue = sample['dialogue']\n",
    "        \n",
    "        # Tokenize输入\n",
    "        inputs = tokenizer(\n",
    "            dialogue,\n",
    "            max_length=config.max_source_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(config.device)\n",
    "        \n",
    "        # 生成摘要\n",
    "        with torch.no_grad():\n",
    "            with ctx:\n",
    "                outputs = model.generate(\n",
    "                    inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    max_length=config.max_target_length,\n",
    "                    num_beams=config.num_beams,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "        \n",
    "        generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        results.append({\n",
    "            'id': sample_id,\n",
    "            'summary': generated_summary\n",
    "        })\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n生成完成！总耗时: {total_time/60:.1f}分钟 | 平均: {total_time/len(test_data):.2f}秒/样本\")\n",
    "    \n",
    "    # 保存结果\n",
    "    output_file = 'submission.csv'\n",
    "    output_path = os.path.join(config.out_dir, output_file)\n",
    "    \n",
    "    print(f\"\\n保存结果到 {output_path}\")\n",
    "    with open(output_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['id', 'summary'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "    \n",
    "    print(f\"完成！生成了 {len(results)} 条摘要\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "predict_test_set()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
