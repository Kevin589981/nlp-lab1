{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 处理数据\n",
    "右侧点击Add Input，找到我们的比赛，然后添加\n",
    "\n",
    "从Kaggle输入的train.csv读取数据，随机划分为训练集(95%)和验证集(5%)\n",
    "\n",
    "保存到/kaggle/working/data/samsum目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:08:03.158008Z",
     "iopub.status.busy": "2025-11-12T07:08:03.157759Z",
     "iopub.status.idle": "2025-11-12T07:08:09.549735Z",
     "shell.execute_reply": "2025-11-12T07:08:09.549017Z",
     "shell.execute_reply.started": "2025-11-12T07:08:03.157988Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.3.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge-score) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge-score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge-score) (2024.2.0)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=ba58bff0a9e76b99905a425175897e1dd732bd1db3ade74aa9505d171323d4f5\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:08:09.551692Z",
     "iopub.status.busy": "2025-11-12T07:08:09.551370Z",
     "iopub.status.idle": "2025-11-12T07:08:11.073423Z",
     "shell.execute_reply": "2025-11-12T07:08:11.072768Z",
     "shell.execute_reply.started": "2025-11-12T07:08:09.551670Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "准备SAMSum数据集划分\n",
      "================================================================================\n",
      "\n",
      "读取数据: /kaggle/input/nanogpt-fudannlp-cs-30040/train.csv\n",
      "总样本数: 26141\n",
      "训练集样本数: 24834 (95.0%)\n",
      "验证集样本数: 1307 (5.0%)\n",
      "\n",
      "训练集已保存: /kaggle/working/data/samsum/train.csv\n",
      "验证集已保存: /kaggle/working/data/samsum/validation.csv\n",
      "\n",
      "数据集划分完成！\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import tiktoken\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"准备SAMSum数据集划分\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 读取原始CSV文件\n",
    "input_csv = '/kaggle/input/nanogpt-fudannlp-cs-30040/train.csv'\n",
    "print(f\"\\n读取数据: {input_csv}\")\n",
    "\n",
    "df = pd.read_csv(input_csv)\n",
    "total_samples = len(df)\n",
    "print(f\"总样本数: {total_samples}\")\n",
    "\n",
    "# 随机打乱数据\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 计算划分点（5%作为验证集）\n",
    "val_size = int(total_samples * 0.05)\n",
    "train_size = total_samples - val_size\n",
    "\n",
    "print(f\"训练集样本数: {train_size} ({train_size/total_samples*100:.1f}%)\")\n",
    "print(f\"验证集样本数: {val_size} ({val_size/total_samples*100:.1f}%)\")\n",
    "\n",
    "# 划分数据\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 创建输出目录\n",
    "output_dir = '/kaggle/working/data/samsum'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 保存训练集\n",
    "train_csv_path = os.path.join(output_dir, 'train.csv')\n",
    "train_df.to_csv(train_csv_path, index=False)\n",
    "print(f\"\\n训练集已保存: {train_csv_path}\")\n",
    "\n",
    "# 保存验证集\n",
    "val_csv_path = os.path.join(output_dir, 'validation.csv')\n",
    "val_df.to_csv(val_csv_path, index=False)\n",
    "print(f\"验证集已保存: {val_csv_path}\")\n",
    "\n",
    "print(\"\\n数据集划分完成！\")\n",
    "print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在模型架构部分之前添加LoRA层定义\n",
    "# %% [markdown]\n",
    "# ## LoRA层实现\n",
    "\n",
    "# %%\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA (Low-Rank Adaptation) 层\n",
    "    \n",
    "    原理：\n",
    "    对于原始权重矩阵 W ∈ R^(d×k)，添加低秩分解：\n",
    "    W' = W + BA，其中 B ∈ R^(d×r), A ∈ R^(r×k), r << min(d,k)\n",
    "    \n",
    "    训练时冻结W，只训练A和B，大幅减少可训练参数\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int, \n",
    "        out_features: int, \n",
    "        rank: int = 4,\n",
    "        lora_alpha: float = 1.0,\n",
    "        lora_dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.scaling = lora_alpha / rank\n",
    "        \n",
    "        # LoRA矩阵A和B\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "        # Dropout\n",
    "        self.lora_dropout = nn.Dropout(p=lora_dropout) if lora_dropout > 0 else nn.Identity()\n",
    "        \n",
    "        # 初始化\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LoRA的前向传播：x @ A^T @ B^T\n",
    "        return (self.lora_dropout(x) @ self.lora_A.t() @ self.lora_B.t()) * self.scaling\n",
    "\n",
    "\n",
    "class LinearWithLoRA(nn.Module):\n",
    "    \"\"\"\n",
    "    带LoRA的线性层\n",
    "    \n",
    "    组合原始Linear层（冻结）和LoRA适配器（可训练）\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        linear: nn.Linear,\n",
    "        rank: int = 4,\n",
    "        lora_alpha: float = 1.0,\n",
    "        lora_dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features,\n",
    "            linear.out_features,\n",
    "            rank=rank,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "        )\n",
    "        \n",
    "        # 冻结原始权重\n",
    "        self.linear.weight.requires_grad = False\n",
    "        if self.linear.bias is not None:\n",
    "            self.linear.bias.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 原始输出 + LoRA调整\n",
    "        return self.linear(x) + self.lora(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-12T07:08:11.074361Z",
     "iopub.status.busy": "2025-11-12T07:08:11.074131Z",
     "iopub.status.idle": "2025-11-12T07:08:15.050369Z",
     "shell.execute_reply": "2025-11-12T07:08:15.049591Z",
     "shell.execute_reply.started": "2025-11-12T07:08:11.074336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "===================================================================================\n",
    "GPT-2 文本摘要微调 - 完整教学脚本\n",
    "===================================================================================\n",
    "\n",
    "本脚本整合了完整的训练和评估流程，适合用于教学和学习。\n",
    "\n",
    "主要内容：\n",
    "1. GPT模型定义（完整的Transformer架构）\n",
    "2. 数据准备和加载\n",
    "3. 模型训练\n",
    "4. ROUGE评估\n",
    "5. 生成和测试\n",
    "\n",
    "学习建议：\n",
    "- 初学者：重点关注Config配置部分，了解各参数的作用\n",
    "- 进阶者：深入理解模型结构、训练循环和数据处理\n",
    "- 实践者：修改参数进行实验，观察结果变化\n",
    "\n",
    "===================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CausalSelfAttentio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:08:15.051414Z",
     "iopub.status.busy": "2025-11-12T07:08:15.051125Z",
     "iopub.status.idle": "2025-11-12T07:08:15.061524Z",
     "shell.execute_reply": "2025-11-12T07:08:15.060452Z",
     "shell.execute_reply.started": "2025-11-12T07:08:15.051384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        # flash attention\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x, past_kv=None, use_cache=False):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        \n",
    "        if past_kv is not None:\n",
    "            past_k, past_v = past_kv\n",
    "            k = torch.cat([past_k, k], dim=2)\n",
    "            v = torch.cat([past_v, v], dim=2)\n",
    "        \n",
    "        present_kv = (k, v) if use_cache else None\n",
    "        T_full = k.size(2)\n",
    "        \n",
    "        if self.flash:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(\n",
    "                q, k, v, attn_mask=None, \n",
    "                dropout_p=self.dropout if self.training else 0, \n",
    "                is_causal=True\n",
    "            )\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T_full] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y, present_kv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:08:15.063617Z",
     "iopub.status.busy": "2025-11-12T07:08:15.063313Z",
     "iopub.status.idle": "2025-11-12T07:08:15.077176Z",
     "shell.execute_reply": "2025-11-12T07:08:15.076548Z",
     "shell.execute_reply.started": "2025-11-12T07:08:15.063601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, past_kv=None, use_cache=False):\n",
    "        attn_out, present_kv = self.attn(self.ln_1(x), past_kv=past_kv, use_cache=use_cache)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x, present_kv\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, past_kv=None, use_cache=False):\n",
    "        attn_out, present_kv = self.attn(self.ln_1(x), past_kv=past_kv, use_cache=use_cache)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x, present_kv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:08:15.078103Z",
     "iopub.status.busy": "2025-11-12T07:08:15.077866Z",
     "iopub.status.idle": "2025-11-12T07:08:15.112687Z",
     "shell.execute_reply": "2025-11-12T07:08:15.111922Z",
     "shell.execute_reply.started": "2025-11-12T07:08:15.078081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None, past_key_values=None, use_cache=False):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        \n",
    "        # 计算position IDs\n",
    "        if past_key_values is None:\n",
    "            pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "        else:\n",
    "            # 如果使用cache，position从past的长度开始\n",
    "            past_length = past_key_values[0][0].size(2) if past_key_values is not None else 0\n",
    "            pos = torch.arange(past_length, past_length + t, dtype=torch.long, device=device)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        # 通过所有block，收集KV cache\n",
    "        present_key_values = [] if use_cache else None\n",
    "        for i, block in enumerate(self.transformer.h):\n",
    "            past_kv = past_key_values[i] if past_key_values is not None else None\n",
    "            x, present_kv = block(x, past_kv=past_kv, use_cache=use_cache)\n",
    "            if use_cache:\n",
    "                present_key_values.append(present_kv)\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        if use_cache:\n",
    "            return logits, loss, present_key_values\n",
    "        return logits, loss\n",
    "\n",
    "    def crop_block_size(self, block_size):\n",
    "        # model surgery to decrease the block size if necessary\n",
    "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
    "        # but want to use a smaller block size for some smaller, simpler model\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, override_args=None):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        override_args = override_args or {} # default to empty dict\n",
    "        # only dropout can be overridden see more notes below\n",
    "        assert all(k == 'dropout' for k in override_args)\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        config_args['bias'] = True # always True for GPT model checkpoints\n",
    "        # we can override the dropout rate, if desired\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
    "        # first estimate the number of flops we do per iteration.\n",
    "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = self.get_num_params()\n",
    "        cfg = self.config\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
    "        flops_promised = 106e9 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, eos_token_id=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \n",
    "        Args:\n",
    "            idx: 输入序列 (b, t)\n",
    "            max_new_tokens: 最大生成token数\n",
    "            temperature: 采样温度\n",
    "            top_k: top-k采样\n",
    "            eos_token_id: 结束token ID，遇到时提前停止（可以是单个ID或ID列表）\n",
    "        \"\"\"\n",
    "        # 将eos_token_id转换为列表\n",
    "        if eos_token_id is not None:\n",
    "            if isinstance(eos_token_id, int):\n",
    "                eos_token_id = [eos_token_id]\n",
    "            eos_token_id = set(eos_token_id)\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "            \n",
    "            # 检查是否生成了结束token\n",
    "            if eos_token_id is not None:\n",
    "                # 检查batch中所有序列是否都遇到了结束token\n",
    "                if idx_next[0, 0].item() in eos_token_id:\n",
    "                    break\n",
    "\n",
    "        return idx\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_with_kv_cache(self, idx, max_new_tokens, temperature=1.0, top_k=None, eos_token_id=None):\n",
    "        \"\"\"\n",
    "        使用KV cache加速的生成方法\n",
    "        \n",
    "        KV cache原理：\n",
    "        - 在自回归生成中，每一步只需要计算新token的attention\n",
    "        - 之前token的key和value可以缓存，避免重复计算\n",
    "        - 这样可以显著加速生成过程（特别是长序列）\n",
    "        \n",
    "        Args:\n",
    "            idx: 输入序列 (b, t)\n",
    "            max_new_tokens: 最大生成token数\n",
    "            temperature: 采样温度\n",
    "            top_k: top-k采样\n",
    "            eos_token_id: 结束token ID，遇到时提前停止（可以是单个ID或ID列表）\n",
    "        \"\"\"\n",
    "        # 将eos_token_id转换为集合\n",
    "        if eos_token_id is not None:\n",
    "            if isinstance(eos_token_id, int):\n",
    "                eos_token_id = [eos_token_id]\n",
    "            eos_token_id = set(eos_token_id)\n",
    "        \n",
    "        # 第一步：处理整个prompt，获取初始的KV cache\n",
    "        idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "        logits, _, past_key_values = self(idx_cond, use_cache=True)\n",
    "        \n",
    "        # 对第一个token进行采样\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        # 检查是否立即遇到结束token\n",
    "        if eos_token_id is not None and idx_next[0, 0].item() in eos_token_id:\n",
    "            return idx\n",
    "        \n",
    "        # 后续生成步骤：每次只处理一个新token，使用KV cache\n",
    "        for _ in range(max_new_tokens - 1):\n",
    "            # 只输入最后一个token，使用past_key_values\n",
    "            logits, _, past_key_values = self(\n",
    "                idx[:, [-1]], \n",
    "                past_key_values=past_key_values, \n",
    "                use_cache=True\n",
    "            )\n",
    "            \n",
    "            # 采样\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # 拼接新token\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "            \n",
    "            # 提前停止检查\n",
    "            if eos_token_id is not None and idx_next[0, 0].item() in eos_token_id:\n",
    "                break\n",
    "        \n",
    "        return idx\n",
    "def configure_optimizers_for_lora(model, weight_decay, learning_rate, betas, device_type):\n",
    "    \"\"\"\n",
    "    为LoRA配置优化器（只优化LoRA参数）\n",
    "    \n",
    "    参数:\n",
    "        model: GPT模型\n",
    "        weight_decay: 权重衰减\n",
    "        learning_rate: 学习率\n",
    "        betas: Adam的beta参数\n",
    "        device_type: 设备类型\n",
    "    \n",
    "    返回:\n",
    "        optimizer: 配置好的优化器\n",
    "    \"\"\"\n",
    "    # 只收集需要梯度的参数（LoRA参数）\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters() if p.requires_grad}\n",
    "    \n",
    "    # LoRA参数通常都需要weight decay\n",
    "    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "    nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "    \n",
    "    optim_groups = [\n",
    "        {'params': decay_params, 'weight_decay': weight_decay},\n",
    "        {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    \n",
    "    num_decay_params = sum(p.numel() for p in decay_params)\n",
    "    num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "    \n",
    "    print(f\"LoRA优化器配置:\")\n",
    "    print(f\"  需decay的参数: {len(decay_params)} tensors, {num_decay_params:,} parameters\")\n",
    "    print(f\"  不需decay的参数: {len(nodecay_params)} tensors, {num_nodecay_params:,} parameters\")\n",
    "    \n",
    "    fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "    use_fused = fused_available and device_type == 'cuda'\n",
    "    extra_args = dict(fused=True) if use_fused else dict()\n",
    "    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "    print(f\"  使用fused AdamW: {use_fused}\")\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:08:15.113668Z",
     "iopub.status.busy": "2025-11-12T07:08:15.113467Z",
     "iopub.status.idle": "2025-11-12T07:08:15.204798Z",
     "shell.execute_reply": "2025-11-12T07:08:15.204069Z",
     "shell.execute_reply.started": "2025-11-12T07:08:15.113653Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import csv\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "\n",
    "# 导入模型定义\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 第一部分：配置参数\n",
    "# 这些参数可以由学生根据需要进行调整\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    配置类：包含所有可调参数\n",
    "    参数组织方式便于学生理解，参数值与nanoGPT原始配置保持一致\n",
    "    \"\"\"\n",
    "    use_lora = True              # 是否使用LoRA微调\n",
    "    lora_rank = 8                # LoRA秩（越小参数越少，通常4-16）\n",
    "    lora_alpha = 16              # LoRA缩放系数（通常设为rank的2倍）\n",
    "    lora_dropout = 0.05          # LoRA的dropout率\n",
    "    lora_target_modules = ['c_attn', 'c_proj', 'c_fc']  # 要添加LoRA的模块\n",
    "    # =========================================================================\n",
    "    # 数据集配置\n",
    "    # =========================================================================\n",
    "    dataset_path = '/kaggle/working/data/samsum'  # 原始数据集路径\n",
    "    dataset = 'samsum'          # 数据集名称（处理后的数据会保存在data/{dataset}/目录）\n",
    "    \n",
    "    # 特殊token定义（用于分隔对话和摘要）\n",
    "    dialogue_start = \"\\n\\n### DIALOGUE:\\n\"  # 对话开始标记\n",
    "    summary_start = \"\\n\\n### SUMMARY:\\n\"     # 摘要开始标记\n",
    "    summary_end = \"<|endoftext|>\"            # 摘要结束标记（GPT-2的EOS token）\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 训练配置（建议学生重点关注这部分）\n",
    "    # =========================================================================\n",
    "    # 模型初始化\n",
    "    init_from = 'gpt2'       # 'scratch'(从头训练) 或 'resume'(继续训练) 或 'gpt2'/'gpt2-xl'(从预训练模型微调)\n",
    "    \n",
    "    # 批次配置\n",
    "    batch_size = 8              # 每个GPU的批次大小（micro-batch size）\n",
    "    gradient_accumulation_steps = 16  # 梯度累积步数，有效批次 = batch_size * gradient_accumulation_steps\n",
    "    block_size = 1024           # 上下文窗口大小（最大序列长度）\n",
    "    \n",
    "    # 训练步数\n",
    "    max_iters = 500              # 总训练迭代次数\n",
    "    \n",
    "    # 优化器配置（AdamW）\n",
    "    learning_rate = 1e-4 #6e-5        # 学习率（微调时使用较小的学习率）\n",
    "    weight_decay = 1e-2         # 权重衰减系数\n",
    "    beta1 = 0.9                 # Adam的beta1参数\n",
    "    beta2 = 0.999                # Adam的beta2参数\n",
    "    grad_clip = 1.0             # 梯度裁剪阈值（0.0表示不裁剪）\n",
    "    \n",
    "    # 学习率调度\n",
    "    decay_lr = True#False            # 是否使用学习率衰减（微调时通常用常数学习率）\n",
    "    warmup_iters = 100         # 学习率预热步数\n",
    "    lr_decay_iters = 2000     # 学习率衰减的总步数\n",
    "    min_lr = 1e-5               # 最小学习率\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 模型配置（从头训练时需要设置，从预训练模型加载时会被覆盖）\n",
    "    # =========================================================================\n",
    "    n_layer = 12                # Transformer层数\n",
    "    n_head = 12                 # 注意力头数\n",
    "    n_embd = 768                # 嵌入维度\n",
    "    dropout = 0.1               # Dropout率（预训练0.0，微调可尝试0.1+）\n",
    "    bias = False                # LayerNorm和Linear层是否使用bias\n",
    "    \n",
    "    # =========================================================================\n",
    "    # I/O配置\n",
    "    # =========================================================================\n",
    "    out_dir = 'out-summarization-lora' # checkpoint保存目录\n",
    "    eval_interval = 30           # 每多少步评估一次\n",
    "    log_interval = 5            # 每多少步打印日志\n",
    "    eval_iters = 40             # 评估时的迭代次数\n",
    "    eval_only = False           # 是否只评估不训练\n",
    "    always_save_checkpoint = True  # 是否每次评估都保存checkpoint（False表示只保存最佳模型）\n",
    "    \n",
    "    # ROUGE评估配置（训练过程中）\n",
    "    eval_rouge_during_training = True  # 是否在训练时评估ROUGE分数\n",
    "    rouge_eval_samples = 5      # 训练时ROUGE评估的样本数（较少避免太慢）\n",
    "    \n",
    "    # =========================================================================\n",
    "    # wandb日志配置（可选）\n",
    "    # =========================================================================\n",
    "    wandb_log = False           # 是否启用wandb日志\n",
    "    wandb_project = 'owt'       # wandb项目名\n",
    "    wandb_run_name = 'gpt2'     # wandb运行名称\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 系统配置\n",
    "    # =========================================================================\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'  # 训练设备\n",
    "    dtype = 'float16'#'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "    # dtype = 'float32'\n",
    "    compile = True#False             # 是否使用PyTorch 2.0编译（需要CUDA Capability >= 7.0，P100不支持）\n",
    "    backend = 'nccl'            # DDP后端（'nccl'用于GPU，'gloo'用于CPU）\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 测试/生成配置\n",
    "    # =========================================================================\n",
    "    num_test_samples = 10       # 测试时评估的样本数量\n",
    "    max_new_tokens = 100        # 生成时的最大token数\n",
    "    temperature = 0.7           # 生成温度（1.0=无变化，<1.0=更确定，>1.0=更随机）\n",
    "    top_k = 50                 # Top-K采样（保留概率最高的K个token）\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 添加LoRA应用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 添加LoRA应用函数\n",
    "\n",
    "# %%\n",
    "def apply_lora_to_model(model, config):\n",
    "    \"\"\"\n",
    "    将LoRA适配器应用到模型\n",
    "    \n",
    "    参数:\n",
    "        model: GPT模型\n",
    "        config: 配置对象\n",
    "    \n",
    "    返回:\n",
    "        修改后的模型，只有LoRA参数可训练\n",
    "    \"\"\"\n",
    "    if not config.use_lora:\n",
    "        return model\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"应用LoRA适配器...\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"LoRA配置:\")\n",
    "    print(f\"  Rank: {config.lora_rank}\")\n",
    "    print(f\"  Alpha: {config.lora_alpha}\")\n",
    "    print(f\"  Dropout: {config.lora_dropout}\")\n",
    "    print(f\"  目标模块: {config.lora_target_modules}\")\n",
    "    \n",
    "    # 统计信息\n",
    "    total_params = 0\n",
    "    lora_params = 0\n",
    "    \n",
    "    # 遍历所有Transformer块\n",
    "    for block_idx, block in enumerate(model.transformer.h):\n",
    "        # 处理attention层\n",
    "        if 'c_attn' in config.lora_target_modules:\n",
    "            original_c_attn = block.attn.c_attn\n",
    "            block.attn.c_attn = LinearWithLoRA(\n",
    "                original_c_attn,\n",
    "                rank=config.lora_rank,\n",
    "                lora_alpha=config.lora_alpha,\n",
    "                lora_dropout=config.lora_dropout,\n",
    "            )\n",
    "        \n",
    "        if 'c_proj' in config.lora_target_modules:\n",
    "            # attention输出投影\n",
    "            original_attn_proj = block.attn.c_proj\n",
    "            block.attn.c_proj = LinearWithLoRA(\n",
    "                original_attn_proj,\n",
    "                rank=config.lora_rank,\n",
    "                lora_alpha=config.lora_alpha,\n",
    "                lora_dropout=config.lora_dropout,\n",
    "            )\n",
    "        \n",
    "        # 处理MLP层\n",
    "        if 'c_fc' in config.lora_target_modules:\n",
    "            original_c_fc = block.mlp.c_fc\n",
    "            block.mlp.c_fc = LinearWithLoRA(\n",
    "                original_c_fc,\n",
    "                rank=config.lora_rank,\n",
    "                lora_alpha=config.lora_alpha,\n",
    "                lora_dropout=config.lora_dropout,\n",
    "            )\n",
    "        \n",
    "        if 'c_proj' in config.lora_target_modules:\n",
    "            # MLP输出投影\n",
    "            original_mlp_proj = block.mlp.c_proj\n",
    "            block.mlp.c_proj = LinearWithLoRA(\n",
    "                original_mlp_proj,\n",
    "                rank=config.lora_rank,\n",
    "                lora_alpha=config.lora_alpha,\n",
    "                lora_dropout=config.lora_dropout,\n",
    "            )\n",
    "    \n",
    "    # 冻结除LoRA之外的所有参数\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora_' not in name:\n",
    "            param.requires_grad = False\n",
    "            total_params += param.numel()\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "            lora_params += param.numel()\n",
    "            total_params += param.numel()\n",
    "    \n",
    "    print(f\"\\n参数统计:\")\n",
    "    print(f\"  总参数: {total_params:,}\")\n",
    "    print(f\"  LoRA参数: {lora_params:,}\")\n",
    "    print(f\"  可训练参数占比: {lora_params/total_params*100:.2f}%\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:08:15.205791Z",
     "iopub.status.busy": "2025-11-12T07:08:15.205553Z",
     "iopub.status.idle": "2025-11-12T07:08:15.220595Z",
     "shell.execute_reply": "2025-11-12T07:08:15.220023Z",
     "shell.execute_reply.started": "2025-11-12T07:08:15.205768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 第二部分：数据准备\n",
    "# 读取samsum数据集，格式化为训练格式，并进行tokenization\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_data():\n",
    "    \"\"\"\n",
    "    准备摘要数据集\n",
    "    \n",
    "    数据格式设计：\n",
    "    每条训练样本格式为：\n",
    "    \\n\\n### DIALOGUE:\\n{对话内容}\\n\\n### SUMMARY:\\n{摘要内容}<|endoftext|>\n",
    "    \n",
    "    重要：每个样本独立保存，不连接成长序列\n",
    "    \n",
    "    这样模型能学习到：\n",
    "    - 看到 DIALOGUE 标记后，理解后面是对话内容\n",
    "    - 看到 SUMMARY 标记后，开始生成摘要\n",
    "    - 看到 <|endoftext|> 表示摘要结束\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"准备数据集...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 创建数据目录\n",
    "    data_dir = os.path.join('data', config.dataset)\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # 初始化tokenizer\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    # 处理训练集和验证集\n",
    "    for split in ['train', 'validation']:\n",
    "        print(f\"\\n处理 {split} 数据集...\")\n",
    "        csv_file = os.path.join(config.dataset_path, f'{split}.csv')\n",
    "        \n",
    "        # 读取CSV文件\n",
    "        dialogues = []\n",
    "        summaries = []\n",
    "        with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                dialogues.append(row['dialogue'])\n",
    "                summaries.append(row['summary'])\n",
    "        \n",
    "        print(f\"  读取了 {len(dialogues)} 条数据\")\n",
    "        \n",
    "        # 格式化并tokenize每条数据，每个样本单独保存\n",
    "        samples = []  # 存储所有样本的token列表\n",
    "        valid_count = 0\n",
    "        skipped_count = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for dialogue, summary in zip(dialogues, summaries):\n",
    "            # 构建完整的训练样本\n",
    "            formatted_text = (\n",
    "                config.dialogue_start + dialogue +\n",
    "                config.summary_start + summary +\n",
    "                config.summary_end\n",
    "            )\n",
    "            \n",
    "            # Tokenize\n",
    "            tokens = enc.encode(formatted_text, allowed_special={config.summary_end})\n",
    "            \n",
    "            # 检查长度是否超过block_size\n",
    "            if len(tokens) <= config.block_size:\n",
    "                samples.append(tokens)\n",
    "                valid_count += 1\n",
    "                total_tokens += len(tokens)\n",
    "            else:\n",
    "                # 如果太长，进行截断（保留对话开始和摘要部分）\n",
    "                # 找到SUMMARY标记的位置\n",
    "                summary_tokens = enc.encode(config.summary_start, allowed_special={config.summary_end})\n",
    "                summary_pos = None\n",
    "                for i in range(len(tokens) - len(summary_tokens)):\n",
    "                    if tokens[i:i+len(summary_tokens)] == summary_tokens:\n",
    "                        summary_pos = i\n",
    "                        break\n",
    "                \n",
    "                if summary_pos and (len(tokens) - summary_pos) < config.block_size * 0.3:\n",
    "                    # 如果能找到摘要位置，且摘要部分不太长，则截断对话部分\n",
    "                    dialogue_tokens = enc.encode(config.dialogue_start, allowed_special={config.summary_end})\n",
    "                    available_space = config.block_size - (len(tokens) - summary_pos) - len(dialogue_tokens)\n",
    "                    \n",
    "                    if available_space > 0:\n",
    "                        # 截断对话内容\n",
    "                        truncated_tokens = (\n",
    "                            dialogue_tokens +\n",
    "                            tokens[len(dialogue_tokens):len(dialogue_tokens)+available_space] +\n",
    "                            tokens[summary_pos:]\n",
    "                        )\n",
    "                        samples.append(truncated_tokens)\n",
    "                        valid_count += 1\n",
    "                        total_tokens += len(truncated_tokens)\n",
    "                    else:\n",
    "                        skipped_count += 1\n",
    "                else:\n",
    "                    skipped_count += 1\n",
    "        \n",
    "        print(f\"  有效样本数: {valid_count}\")\n",
    "        print(f\"  跳过样本数: {skipped_count}\")\n",
    "        print(f\"  总token数: {total_tokens:,}\")\n",
    "        print(f\"  平均token数: {total_tokens // valid_count if valid_count > 0 else 0}\")\n",
    "        \n",
    "        # 保存为pickle文件（每个样本单独保存）\n",
    "        output_file = 'train.pkl' if split == 'train' else 'val.pkl'\n",
    "        output_path = os.path.join(data_dir, output_file)\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(samples, f)\n",
    "        print(f\"  保存到: {output_path}\")\n",
    "    \n",
    "    # 保存meta信息（词表大小）\n",
    "    meta = {\n",
    "        'vocab_size': enc.n_vocab,\n",
    "        'dialogue_start': config.dialogue_start,\n",
    "        'summary_start': config.summary_start,\n",
    "        'summary_end': config.summary_end,\n",
    "    }\n",
    "    with open(os.path.join(data_dir, 'meta.pkl'), 'wb') as f:\n",
    "        pickle.dump(meta, f)\n",
    "    \n",
    "    print(\"\\n数据准备完成！\")\n",
    "    print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class SummarizationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:08:15.221632Z",
     "iopub.status.busy": "2025-11-12T07:08:15.221381Z",
     "iopub.status.idle": "2025-11-12T07:08:15.236839Z",
     "shell.execute_reply": "2025-11-12T07:08:15.236212Z",
     "shell.execute_reply.started": "2025-11-12T07:08:15.221617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 第三部分：数据集类和数据加载\n",
    "# =============================================================================\n",
    "\n",
    "class SummarizationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    摘要任务的Dataset类\n",
    "    \n",
    "    每个样本是一个完整的\"对话+摘要\"序列，包含：\n",
    "    - dialogue_start + 对话内容 + summary_start + 摘要内容 + summary_end\n",
    "    \n",
    "    这个类负责：\n",
    "    1. 加载tokenized的样本\n",
    "    2. Padding/截断到固定长度\n",
    "    3. 构建输入(x)和目标(y)序列\n",
    "    \"\"\"\n",
    "    \n",
    "    # def __init__(self, data_path, block_size):\n",
    "    #     \"\"\"\n",
    "    #     参数:\n",
    "    #         data_path: pickle文件路径，包含样本的token列表\n",
    "    #         block_size: 序列的最大长度\n",
    "    #     \"\"\"\n",
    "        # with open(data_path, 'rb') as f:\n",
    "        #     self.samples = pickle.load(f)\n",
    "        # self.block_size = block_size\n",
    "        # print(f\"  加载了 {len(self.samples)} 个样本\")\n",
    "    def __init__(self, data_path, block_size):\n",
    "        with open(data_path, 'rb') as f:\n",
    "            self.samples = pickle.load(f)\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # 为了找到摘要开始的位置，我们需要提前tokenize摘要开始的标记\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.summary_token_ids = enc.encode(config.summary_start)\n",
    "        \n",
    "        print(f\"  加载了 {len(self.samples)} 个样本\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    # def __getitem__(self, idx):\n",
    "        # \"\"\"\n",
    "        # 返回一个样本的(x, y)对\n",
    "        \n",
    "        # x: 输入序列 [block_size]\n",
    "        # y: 目标序列 [block_size]，即x向右移动一位\n",
    "        \n",
    "        # 重要：x中不能有-1（会导致embedding错误），只有y中可以有-1用于padding\n",
    "        # \"\"\"\n",
    "        # sample_tokens = self.samples[idx]\n",
    "        # sample_len = len(sample_tokens)\n",
    "        \n",
    "        # if sample_len >= self.block_size + 1:\n",
    "        #     # 样本足够长，直接截断\n",
    "        #     x = torch.tensor(sample_tokens[:self.block_size], dtype=torch.long)\n",
    "        #     y = torch.tensor(sample_tokens[1:self.block_size + 1], dtype=torch.long)\n",
    "        # else:\n",
    "        #     # 样本较短，需要padding\n",
    "        #     # x: 用0 padding（GPT-2的<|endoftext|> token ID是50256，但0通常是安全的padding）\n",
    "        #     # y: 用-1 padding（会在loss计算时被ignore）\n",
    "        #     x_tokens = sample_tokens[:sample_len]\n",
    "        #     y_tokens = sample_tokens[1:sample_len] if sample_len > 0 else []\n",
    "            \n",
    "        #     # Padding到block_size\n",
    "        #     x_padding_length = self.block_size - len(x_tokens)\n",
    "        #     y_padding_length = self.block_size - len(y_tokens)\n",
    "            \n",
    "        #     # 使用50256（<|endoftext|>）作为x的padding，-1作为y的padding\n",
    "        #     x_tokens = x_tokens + [50256] * x_padding_length\n",
    "        #     y_tokens = y_tokens + [-1] * y_padding_length\n",
    "            \n",
    "        #     x = torch.tensor(x_tokens, dtype=torch.long)\n",
    "        #     y = torch.tensor(y_tokens, dtype=torch.long)\n",
    "        \n",
    "        # return x, y\n",
    "    def __getitem__(self, idx):\n",
    "        sample_tokens = self.samples[idx]\n",
    "        \n",
    "        # 1. 创建 x 和 y (与之前逻辑相同)\n",
    "        # x 是输入, y 是目标 (x 向右移动一位)\n",
    "        x_tokens = sample_tokens[:-1]\n",
    "        y_tokens = sample_tokens[1:]\n",
    "        \n",
    "        # 2. 应用损失掩码到 y 上\n",
    "        # 找到摘要开始的位置\n",
    "        summary_start_pos = -1\n",
    "        for i in range(len(x_tokens) - len(self.summary_token_ids)):\n",
    "            if x_tokens[i:i+len(self.summary_token_ids)] == self.summary_token_ids:\n",
    "                summary_start_pos = i\n",
    "                break\n",
    "        \n",
    "        # 如果找到了摘要标记，将它之前的所有目标 token 设置为 -1\n",
    "        if summary_start_pos != -1:\n",
    "            # 我们希望从 \"### SUMMARY:\\n\" 的最后一个 token 开始预测第一个摘要词\n",
    "            # 所以，掩码应该应用到这个位置之前的所有 token\n",
    "            mask_end_index = summary_start_pos + len(self.summary_token_ids)\n",
    "            for i in range(mask_end_index):\n",
    "                y_tokens[i] = -1\n",
    "        \n",
    "        # 3. Padding (与之前逻辑相同)\n",
    "        x_padding_len = self.block_size - len(x_tokens)\n",
    "        y_padding_len = self.block_size - len(y_tokens)\n",
    "        \n",
    "        x_padded = x_tokens + [50256] * x_padding_len\n",
    "        y_padded = y_tokens + [-1] * y_padding_len\n",
    "        \n",
    "        # 截断以防万一\n",
    "        x = torch.tensor(x_padded[:self.block_size], dtype=torch.long)\n",
    "        y = torch.tensor(y_padded[:self.block_size], dtype=torch.long)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "\n",
    "# 全局变量：缓存DataLoader\n",
    "_dataloaders = {'train': None, 'val': None}\n",
    "_data_iters = {'train': None, 'val': None}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:08:15.237808Z",
     "iopub.status.busy": "2025-11-12T07:08:15.237515Z",
     "iopub.status.idle": "2025-11-12T07:08:15.251607Z",
     "shell.execute_reply": "2025-11-12T07:08:15.250928Z",
     "shell.execute_reply.started": "2025-11-12T07:08:15.237788Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_batch(split, data_dir):\n",
    "    \"\"\"\n",
    "    获取一个训练批次\n",
    "    \n",
    "    使用DataLoader实现，支持：\n",
    "    1. 自动batch处理\n",
    "    2. 可选的shuffle（训练集shuffle，验证集不shuffle）\n",
    "    3. 自动循环迭代（epoch结束后自动重新开始）\n",
    "    \n",
    "    参数:\n",
    "        split: 'train' 或 'val'\n",
    "        data_dir: 数据目录\n",
    "    \n",
    "    返回:\n",
    "        x: 输入序列 [batch_size, block_size]\n",
    "        y: 目标序列 [batch_size, block_size]\n",
    "    \"\"\"\n",
    "    global _dataloaders, _data_iters\n",
    "    \n",
    "    # 首次调用：创建DataLoader\n",
    "    if _dataloaders[split] is None:\n",
    "        data_path = os.path.join(data_dir, f'{split}.pkl')\n",
    "        dataset = SummarizationDataset(data_path, config.block_size)\n",
    "        \n",
    "        # 训练集shuffle，验证集不shuffle\n",
    "        shuffle = (split == 'train')\n",
    "        \n",
    "        _dataloaders[split] = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=4,  # 主线程加载数据（简单场景足够）\n",
    "            pin_memory=True if 'cuda' in config.device else False,\n",
    "        )\n",
    "        _data_iters[split] = iter(_dataloaders[split])\n",
    "    \n",
    "    # 获取下一个batch\n",
    "    try:\n",
    "        x, y = next(_data_iters[split])\n",
    "    except StopIteration:\n",
    "        # 当前epoch结束，重新开始\n",
    "        _data_iters[split] = iter(_dataloaders[split])\n",
    "        x, y = next(_data_iters[split])\n",
    "    \n",
    "    # 移动到设备\n",
    "    if 'cuda' in config.device:\n",
    "        x = x.to(config.device, non_blocking=True)\n",
    "        y = y.to(config.device, non_blocking=True)\n",
    "    else:\n",
    "        x = x.to(config.device)\n",
    "        y = y.to(config.device)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## estimate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:08:15.252590Z",
     "iopub.status.busy": "2025-11-12T07:08:15.252357Z",
     "iopub.status.idle": "2025-11-12T07:08:15.266613Z",
     "shell.execute_reply": "2025-11-12T07:08:15.265836Z",
     "shell.execute_reply.started": "2025-11-12T07:08:15.252571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, ctx, data_dir):\n",
    "    \"\"\"\n",
    "    估计训练集和验证集上的损失\n",
    "    \n",
    "    通过多次迭代求平均，得到更准确的损失估计\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(config.eval_iters)\n",
    "        for k in range(config.eval_iters):\n",
    "            X, Y = get_batch(split, data_dir)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:08:15.267756Z",
     "iopub.status.busy": "2025-11-12T07:08:15.267515Z",
     "iopub.status.idle": "2025-11-12T07:08:15.283502Z",
     "shell.execute_reply": "2025-11-12T07:08:15.282858Z",
     "shell.execute_reply.started": "2025-11-12T07:08:15.267732Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_rouge_during_training(model, ctx, data_dir, num_samples=3):\n",
    "    \"\"\"\n",
    "    在训练过程中评估ROUGE分数\n",
    "    \n",
    "    从验证集中随机选择几个样本，生成摘要并计算ROUGE分数\n",
    "    这可以帮助我们实时监控模型的摘要质量\n",
    "    \n",
    "    参数:\n",
    "        model: 模型\n",
    "        ctx: autocast上下文\n",
    "        data_dir: 数据目录（未使用，保留兼容性）\n",
    "        num_samples: 评估的样本数量（默认3个，避免评估时间过长）\n",
    "    \n",
    "    返回:\n",
    "        rouge_scores: 包含平均ROUGE分数的字典\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 初始化tokenizer\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    # 从验证集CSV文件中读取样本\n",
    "    val_csv = os.path.join(config.dataset_path, 'validation.csv')\n",
    "    if not os.path.exists(val_csv):\n",
    "        print(\"  (跳过ROUGE评估：未找到验证集)\")\n",
    "        model.train()\n",
    "        return None\n",
    "    \n",
    "    # 读取验证集\n",
    "    dialogues = []\n",
    "    summaries = []\n",
    "    with open(val_csv, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            dialogues.append(row['dialogue'])\n",
    "            summaries.append(row['summary'])\n",
    "    \n",
    "    # 随机选择num_samples个样本\n",
    "    import random\n",
    "    indices = random.sample(range(len(dialogues)), min(num_samples, len(dialogues)))\n",
    "    \n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    # 获取<|endoftext|>的token ID作为停止符\n",
    "    eos_token_id = enc.encode(config.summary_end, allowed_special={config.summary_end})[0]\n",
    "    \n",
    "    # 临时保存原始max_new_tokens，训练时使用较少的tokens以加快速度\n",
    "    original_max_new_tokens = config.max_new_tokens\n",
    "    config.max_new_tokens = 100  # 训练时用较少的tokens\n",
    "    \n",
    "    for idx in tqdm(indices):\n",
    "        dialogue = dialogues[idx]\n",
    "        reference_summary = summaries[idx]\n",
    "        \n",
    "        # 构建prompt\n",
    "        prompt = config.dialogue_start + dialogue + config.summary_start\n",
    "        prompt_tokens = enc.encode(prompt, allowed_special={config.summary_end})\n",
    "        \n",
    "        # 如果prompt太长，跳过\n",
    "        if len(prompt_tokens) > config.block_size - config.max_new_tokens:\n",
    "            print(f\"len(prompt_tokens) > config.block_size - config.max_new_tokens: {len(prompt_tokens)} > {config.block_size - config.max_new_tokens}\")\n",
    "            continue\n",
    "        \n",
    "        # 使用KV cache加速生成摘要（但训练时为了速度，使用原始的generate方法）\n",
    "        # 注意：训练时模型可能还没有完全训练好，所以使用简单的generate方法\n",
    "        x = torch.tensor(prompt_tokens, dtype=torch.long, device=config.device)[None, ...]\n",
    "        \n",
    "        with ctx:\n",
    "            y = model.generate(\n",
    "                x,\n",
    "                max_new_tokens=config.max_new_tokens,\n",
    "                temperature=0.8,\n",
    "                top_k=200,\n",
    "                eos_token_id=eos_token_id\n",
    "            )\n",
    "        \n",
    "        # 解码\n",
    "        generated_tokens = y[0].tolist()\n",
    "        generated_text = enc.decode(generated_tokens)\n",
    "        \n",
    "        # 提取摘要（使用公共函数）\n",
    "        generated_summary = extract_summary(generated_text, prompt, enc)\n",
    "        \n",
    "        # 计算ROUGE分数（使用公共函数）\n",
    "        if generated_summary:  # 确保生成了内容\n",
    "            rouge_scores = calculate_rouge(reference_summary, generated_summary)\n",
    "            if rouge_scores:\n",
    "                rouge1_scores.append(rouge_scores['rouge1'])\n",
    "                rouge2_scores.append(rouge_scores['rouge2'])\n",
    "                rougeL_scores.append(rouge_scores['rougeL'])\n",
    "    \n",
    "    # 恢复原始max_new_tokens\n",
    "    config.max_new_tokens = original_max_new_tokens\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # 返回平均分数\n",
    "    if len(rouge1_scores) > 0:\n",
    "        return {\n",
    "            'rouge1': np.mean(rouge1_scores),\n",
    "            'rouge2': np.mean(rouge2_scores),\n",
    "            'rougeL': np.mean(rougeL_scores)\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:08:15.284268Z",
     "iopub.status.busy": "2025-11-12T07:08:15.284101Z",
     "iopub.status.idle": "2025-11-12T07:08:15.304965Z",
     "shell.execute_reply": "2025-11-12T07:08:15.304142Z",
     "shell.execute_reply.started": "2025-11-12T07:08:15.284255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_lr(iter_num):\n",
    "    \"\"\"\n",
    "    学习率调度：带预热的余弦衰减\n",
    "    \n",
    "    1. 前warmup_iters步：线性增加\n",
    "    2. 之后：余弦衰减到min_lr\n",
    "    \"\"\"\n",
    "    # 线性预热\n",
    "    if iter_num < config.warmup_iters:\n",
    "        return config.learning_rate * (iter_num + 1) / (config.warmup_iters + 1)\n",
    "    \n",
    "    # 如果超过衰减步数，返回最小学习率\n",
    "    if iter_num > config.lr_decay_iters:\n",
    "        return config.min_lr\n",
    "    \n",
    "    # 余弦衰减\n",
    "    decay_ratio = (iter_num - config.warmup_iters) / (config.lr_decay_iters - config.warmup_iters)\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return config.min_lr + coeff * (config.learning_rate - config.min_lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "    \"\"\"训练主函数（LoRA版本）\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"开始训练（LoRA微调模式）...\" if config.use_lora else \"开始训练...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    torch.manual_seed(1337)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    os.makedirs(config.out_dir, exist_ok=True)\n",
    "    \n",
    "    device_type = 'cuda' if 'cuda' in config.device else 'cpu'\n",
    "    ptdtype = {\n",
    "        'float32': torch.float32,\n",
    "        'bfloat16': torch.bfloat16,\n",
    "        'float16': torch.float16\n",
    "    }[config.dtype]\n",
    "    ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(\n",
    "        device_type=device_type, dtype=ptdtype\n",
    "    )\n",
    "    \n",
    "    data_dir = os.path.join('data', config.dataset)\n",
    "    \n",
    "    meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "    meta_vocab_size = None\n",
    "    if os.path.exists(meta_path):\n",
    "        with open(meta_path, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "        meta_vocab_size = meta['vocab_size']\n",
    "        print(f\"从 {meta_path} 加载词表大小: {meta_vocab_size}\")\n",
    "    \n",
    "    # 初始化模型\n",
    "    print(f\"\\n模型初始化方式: {config.init_from}\")\n",
    "    model_args = dict(\n",
    "        n_layer=config.n_layer,\n",
    "        n_head=config.n_head,\n",
    "        n_embd=config.n_embd,\n",
    "        block_size=config.block_size,\n",
    "        bias=config.bias,\n",
    "        vocab_size=None,\n",
    "        dropout=config.dropout\n",
    "    )\n",
    "    \n",
    "    if config.init_from == 'scratch':\n",
    "        print(\"从头开始训练新模型\")\n",
    "        model_args['vocab_size'] = meta_vocab_size if meta_vocab_size else 50304\n",
    "        gptconf = GPTConfig(**model_args)\n",
    "        model = GPT(gptconf)\n",
    "        iter_num = 0\n",
    "        best_val_loss = 1e9\n",
    "        \n",
    "    elif config.init_from == 'resume':\n",
    "        print(f\"从 {config.out_dir} 恢复训练\")\n",
    "        ckpt_path = os.path.join(config.out_dir, 'ckpt.pt')\n",
    "        checkpoint = torch.load(ckpt_path, map_location=config.device)\n",
    "        checkpoint_model_args = checkpoint['model_args']\n",
    "        \n",
    "        for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "            model_args[k] = checkpoint_model_args[k]\n",
    "        \n",
    "        gptconf = GPTConfig(**model_args)\n",
    "        model = GPT(gptconf)\n",
    "        \n",
    "        state_dict = checkpoint['model']\n",
    "        unwanted_prefix = '_orig_mod.'\n",
    "        for k, v in list(state_dict.items()):\n",
    "            if k.startswith(unwanted_prefix):\n",
    "                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "        \n",
    "        model.load_state_dict(state_dict)\n",
    "        iter_num = checkpoint['iter_num']\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        \n",
    "    elif config.init_from.startswith('gpt2'):\n",
    "        print(f\"从OpenAI GPT-2加载: {config.init_from}\")\n",
    "        override_args = dict(dropout=config.dropout)\n",
    "        model = GPT.from_pretrained(config.init_from, override_args)\n",
    "        \n",
    "        for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "            model_args[k] = getattr(model.config, k)\n",
    "        \n",
    "        iter_num = 0\n",
    "        best_val_loss = 1e9\n",
    "    \n",
    "    if config.block_size < model.config.block_size:\n",
    "        model.crop_block_size(config.block_size)\n",
    "        model_args['block_size'] = config.block_size\n",
    "    \n",
    "    model.to(config.device)\n",
    "    \n",
    "    # ============ 应用LoRA（关键修改） ============\n",
    "    if config.use_lora:\n",
    "        model = apply_lora_to_model(model, config)\n",
    "        # 使用LoRA专用的优化器配置\n",
    "        optimizer = configure_optimizers_for_lora(\n",
    "            model,\n",
    "            config.weight_decay,\n",
    "            config.learning_rate,\n",
    "            (config.beta1, config.beta2),\n",
    "            device_type\n",
    "        )\n",
    "    else:\n",
    "        # 使用原始的优化器配置\n",
    "        optimizer = model.configure_optimizers(\n",
    "            config.weight_decay,\n",
    "            config.learning_rate,\n",
    "            (config.beta1, config.beta2),\n",
    "            device_type\n",
    "        )\n",
    "    \n",
    "    if config.init_from == 'resume' and 'optimizer' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    checkpoint = None\n",
    "    \n",
    "    if config.compile:\n",
    "        print(\"编译模型（首次会比较慢）...\")\n",
    "        unoptimized_model = model\n",
    "        model = torch.compile(model)\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(config.dtype == 'float16'))\n",
    "    \n",
    "    # 训练循环\n",
    "    print(\"\\n开始训练循环...\")\n",
    "    print(f\"总迭代次数: {config.max_iters}\")\n",
    "    print(f\"批次大小: {config.batch_size}\")\n",
    "    print(f\"梯度累积步数: {config.gradient_accumulation_steps}\")\n",
    "    print(f\"有效批次大小: {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "    if config.use_lora:\n",
    "        print(f\"LoRA模式: 只训练LoRA参数\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    X, Y = get_batch('train', data_dir)\n",
    "    t0 = time.time()\n",
    "    local_iter_num = 0\n",
    "    raw_model = model.module if hasattr(model, 'module') else model\n",
    "    running_mfu = -1.0\n",
    "    \n",
    "    while True:\n",
    "        lr = get_lr(iter_num) if config.decay_lr else config.learning_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        if iter_num % config.eval_interval == 0:\n",
    "            losses = estimate_loss(model, ctx, data_dir)\n",
    "            print(f\"\\nStep {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            \n",
    "            if iter_num > 0 and config.eval_rouge_during_training:\n",
    "                print(\"  评估ROUGE分数...\")\n",
    "                rouge_scores = evaluate_rouge_during_training(\n",
    "                    model, ctx, data_dir, num_samples=config.rouge_eval_samples\n",
    "                )\n",
    "                if rouge_scores:\n",
    "                    print(f\"  ROUGE-1: {rouge_scores['rouge1']:.4f}, \"\n",
    "                          f\"ROUGE-2: {rouge_scores['rouge2']:.4f}, \"\n",
    "                          f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "            \n",
    "            if losses['val'] < best_val_loss or config.always_save_checkpoint:\n",
    "                best_val_loss = losses['val']\n",
    "                if iter_num > 0:\n",
    "                    checkpoint = {\n",
    "                        'model': raw_model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'model_args': model_args,\n",
    "                        'iter_num': iter_num,\n",
    "                        'best_val_loss': best_val_loss,\n",
    "                        'config': vars(config),\n",
    "                    }\n",
    "                    print(f\"  保存checkpoint到 {config.out_dir}\")\n",
    "                    torch.save(checkpoint, os.path.join(config.out_dir, 'ckpt.pt'))\n",
    "        \n",
    "        if iter_num == 0 and config.eval_only:\n",
    "            break\n",
    "        \n",
    "        for micro_step in range(config.gradient_accumulation_steps):\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / config.gradient_accumulation_steps\n",
    "            \n",
    "            X, Y = get_batch('train', data_dir)\n",
    "            scaler.scale(loss).backward()\n",
    "        \n",
    "        if config.grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        \n",
    "        if iter_num % config.log_interval == 0:\n",
    "            lossf = loss.item() * config.gradient_accumulation_steps\n",
    "            if local_iter_num >= 5:\n",
    "                mfu = raw_model.estimate_mfu(\n",
    "                    config.batch_size * config.gradient_accumulation_steps, dt\n",
    "                )\n",
    "                running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n",
    "            print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "        \n",
    "        iter_num += 1\n",
    "        local_iter_num += 1\n",
    "        \n",
    "        if iter_num > config.max_iters:\n",
    "            break\n",
    "    \n",
    "    print(\"\\n训练完成！\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:08:15.307937Z",
     "iopub.status.busy": "2025-11-12T07:08:15.307398Z",
     "iopub.status.idle": "2025-11-12T07:08:15.319583Z",
     "shell.execute_reply": "2025-11-12T07:08:15.318981Z",
     "shell.execute_reply.started": "2025-11-12T07:08:15.307922Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 第四部分：测试和评估\n",
    "# =============================================================================\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    加载训练好的模型\n",
    "    \n",
    "    返回:\n",
    "        model: 加载的模型\n",
    "        enc: tokenizer\n",
    "        ctx: autocast上下文\n",
    "    \"\"\"\n",
    "    # 设置设备和精度\n",
    "    device_type = 'cuda' if 'cuda' in config.device else 'cpu'\n",
    "    ptdtype = {\n",
    "        'float32': torch.float32,\n",
    "        'bfloat16': torch.bfloat16,\n",
    "        'float16': torch.float16\n",
    "    }[config.dtype]\n",
    "    ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(\n",
    "        device_type=device_type, dtype=ptdtype\n",
    "    )\n",
    "    \n",
    "    # 加载模型\n",
    "    print(f\"\\n从 {config.out_dir} 加载模型...\")\n",
    "    ckpt_path = os.path.join(config.out_dir, 'ckpt.pt')\n",
    "\n",
    "    print(f\"path {ckpt_path}\")\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        print(f\"错误: 找不到checkpoint文件 {ckpt_path}\")\n",
    "        print(\"请先运行训练！\")\n",
    "        return None, None, None\n",
    "    \n",
    "    checkpoint = torch.load(ckpt_path, map_location=config.device)\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    \n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    \n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "    \n",
    "    if config.compile:\n",
    "        print(\"编译模型...\")\n",
    "        model = torch.compile(model)\n",
    "    \n",
    "    # 初始化tokenizer\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    return model, enc, ctx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:08:15.320754Z",
     "iopub.status.busy": "2025-11-12T07:08:15.320261Z",
     "iopub.status.idle": "2025-11-12T07:08:15.424966Z",
     "shell.execute_reply": "2025-11-12T07:08:15.424169Z",
     "shell.execute_reply.started": "2025-11-12T07:08:15.320729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_summary(model, prompt_tokens, enc, ctx, eos_token_id=None):\n",
    "    \"\"\"\n",
    "    使用KV cache加速生成摘要\n",
    "    \n",
    "    参数:\n",
    "        model: GPT模型\n",
    "        prompt_tokens: prompt的token列表\n",
    "        enc: tokenizer\n",
    "        ctx: autocast上下文\n",
    "        eos_token_id: 结束token ID，用于提前停止\n",
    "    \n",
    "    返回:\n",
    "        generated_text: 生成的完整文本（包含prompt和摘要）\n",
    "    \"\"\"\n",
    "    # 检查长度，如果太长则截断\n",
    "    if len(prompt_tokens) > config.block_size - config.max_new_tokens:\n",
    "        dialogue_start_tokens = enc.encode(config.dialogue_start)\n",
    "        summary_start_tokens = enc.encode(config.summary_start)\n",
    "        available_space = config.block_size - config.max_new_tokens - len(dialogue_start_tokens) - len(summary_start_tokens)\n",
    "        \n",
    "        if available_space > 0:\n",
    "            # 找到对话部分的tokens\n",
    "            # 解码prompt找到对话部分\n",
    "            prompt_text = enc.decode(prompt_tokens)\n",
    "            dialogue_start_pos = prompt_text.find(config.dialogue_start)\n",
    "            summary_start_pos = prompt_text.find(config.summary_start)\n",
    "            \n",
    "            if dialogue_start_pos >= 0 and summary_start_pos > dialogue_start_pos:\n",
    "                dialogue_text = prompt_text[dialogue_start_pos + len(config.dialogue_start):summary_start_pos]\n",
    "                dialogue_tokens = enc.encode(dialogue_text)\n",
    "                truncated_dialogue_tokens = dialogue_tokens[:available_space]\n",
    "                prompt_tokens = dialogue_start_tokens + truncated_dialogue_tokens + summary_start_tokens\n",
    "            else:\n",
    "                # 如果找不到标记，直接截断\n",
    "                prompt_tokens = prompt_tokens[:config.block_size - config.max_new_tokens]\n",
    "        else:\n",
    "            # 如果空间不足，只保留必要的标记\n",
    "            prompt_tokens = dialogue_start_tokens + summary_start_tokens\n",
    "    \n",
    "    # 转换为tensor\n",
    "    x = torch.tensor(prompt_tokens, dtype=torch.long, device=config.device)[None, ...]\n",
    "    \n",
    "    # 使用KV cache加速生成摘要\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            y = model.generate_with_kv_cache(\n",
    "                x,\n",
    "                max_new_tokens=config.max_new_tokens,\n",
    "                temperature=config.temperature,\n",
    "                top_k=config.top_k,\n",
    "                eos_token_id=eos_token_id\n",
    "            )\n",
    "    \n",
    "    # 解码生成的token\n",
    "    generated_tokens = y[0].tolist()\n",
    "    generated_text = enc.decode(generated_tokens)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "def extract_summary(generated_text, prompt_text, enc):\n",
    "    \"\"\"\n",
    "    从生成的文本中提取摘要部分\n",
    "    \n",
    "    参数:\n",
    "        generated_text: 生成的完整文本\n",
    "        prompt_text: 原始prompt文本\n",
    "        enc: tokenizer\n",
    "    \n",
    "    返回:\n",
    "        generated_summary: 提取的摘要文本\n",
    "    \"\"\"\n",
    "    # 提取生成的摘要（去除prompt部分）\n",
    "    if config.summary_start in generated_text:\n",
    "        generated_summary = generated_text.split(config.summary_start)[-1]\n",
    "        \n",
    "        # 去除结束标记\n",
    "        if config.summary_end in generated_summary:\n",
    "            generated_summary = generated_summary.split(config.summary_end)[0]\n",
    "    else:\n",
    "        # 如果没找到标记，就从prompt长度之后开始提取\n",
    "        if len(generated_text) > len(prompt_text):\n",
    "            generated_summary = generated_text[len(prompt_text):]\n",
    "        else:\n",
    "            generated_summary = \"\"\n",
    "    \n",
    "    # 清理生成的摘要\n",
    "    generated_summary = generated_summary.strip()\n",
    "    \n",
    "    # 如果生成的摘要为空，使用空字符串\n",
    "    if not generated_summary:\n",
    "        generated_summary = \"\"\n",
    "    \n",
    "    return generated_summary\n",
    "\n",
    "\n",
    "def calculate_rouge(reference_summary, generated_summary):\n",
    "    \"\"\"\n",
    "    计算ROUGE分数\n",
    "    \n",
    "    参数:\n",
    "        reference_summary: 参考摘要\n",
    "        generated_summary: 生成的摘要\n",
    "    \n",
    "    返回:\n",
    "        dict: 包含rouge1, rouge2, rougeL的字典，如果失败则返回None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from rouge_score import rouge_scorer\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        scores = scorer.score(reference_summary, generated_summary)\n",
    "        return {\n",
    "            'rouge1': scores['rouge1'].fmeasure,\n",
    "            'rouge2': scores['rouge2'].fmeasure,\n",
    "            'rougeL': scores['rougeL'].fmeasure\n",
    "        }\n",
    "    except ImportError:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:08:15.425890Z",
     "iopub.status.busy": "2025-11-12T07:08:15.425693Z",
     "iopub.status.idle": "2025-11-12T07:12:32.602940Z",
     "shell.execute_reply": "2025-11-12T07:12:32.601801Z",
     "shell.execute_reply.started": "2025-11-12T07:08:15.425875Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "                                 GPT-2 摘要微调教学脚本                                 \n",
      "================================================================================\n",
      "\n",
      "当前配置:\n",
      "  数据集: /kaggle/working/data/samsum\n",
      "  模型初始化: gpt2\n",
      "  设备: cuda\n",
      "  批次大小: 4\n",
      "  最大迭代次数: 500\n",
      "  学习率: 6e-05\n",
      "expect: train_pkl: data/samsum/train.pkl\n",
      "\n",
      "未找到处理后的数据文件，开始准备数据...\n",
      "================================================================================\n",
      "准备数据集...\n",
      "================================================================================\n",
      "\n",
      "处理 train 数据集...\n",
      "  读取了 24834 条数据\n",
      "  有效样本数: 24834\n",
      "  跳过样本数: 0\n",
      "  总token数: 4,393,055\n",
      "  平均token数: 176\n",
      "  保存到: data/samsum/train.pkl\n",
      "\n",
      "处理 validation 数据集...\n",
      "  读取了 1307 条数据\n",
      "  有效样本数: 1307\n",
      "  跳过样本数: 0\n",
      "  总token数: 233,808\n",
      "  平均token数: 178\n",
      "  保存到: data/samsum/val.pkl\n",
      "\n",
      "数据准备完成！\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "开始训练...\n",
      "================================================================================\n",
      "从 data/samsum/meta.pkl 加载词表大小: 50257\n",
      "\n",
      "模型初始化方式: gpt2\n",
      "从OpenAI GPT-2加载: gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 07:08:29.329259: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762931309.488041      39 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762931309.533344      39 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.1\n",
      "number of parameters: 123.65M\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4244baa990254d4e9c72c28de41cf81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358e052df0194143a02aeca8fef51f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58b340c6c8949338117150ae89f81e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "编译模型（首次会比较慢）...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39/530882074.py:142: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(config.dtype == 'float16'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始训练循环...\n",
      "总迭代次数: 500\n",
      "批次大小: 4\n",
      "梯度累积步数: 32\n",
      "有效批次大小: 128\n",
      "--------------------------------------------------------------------------------\n",
      "  加载了 24834 个样本\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1112 07:08:59.464000 39 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  加载了 1307 个样本\n",
      "\n",
      "Step 0: train loss 3.3283, val loss 3.3277\n",
      "iter 0: loss 3.1120, time 69683.29ms, mfu -100.00%\n",
      "iter 5: loss 3.7056, time 7347.55ms, mfu 14391.70%\n",
      "\n",
      "Step 10: train loss 3.1005, val loss 2.9844\n",
      "  评估ROUGE分数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-1: 0.1205, ROUGE-2: 0.0037, ROUGE-L: 0.0913\n",
      "  保存checkpoint到 out-summarization\n",
      "iter 10: loss 3.2436, time 21184.23ms, mfu 13451.69%\n",
      "iter 15: loss 2.8690, time 8176.20ms, mfu 13399.84%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39/826390364.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_39/826390364.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# 步骤2: 训练模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\neval_only=True，跳过训练\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_39/530882074.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;31m# 反向传播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# 梯度裁剪\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 第五部分：主函数\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数：整合数据准备、训练和评估\n",
    "    \n",
    "    执行流程：\n",
    "    1. 准备数据（如果数据文件不存在）\n",
    "    2. 训练模型\n",
    "    3. 评估模型\n",
    "    \"\"\"\n",
    "    print(\"\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"GPT-2 摘要微调教学脚本\".center(80))\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n当前配置:\")\n",
    "    print(f\"  数据集: {config.dataset_path}\")\n",
    "    print(f\"  模型初始化: {config.init_from}\")\n",
    "    print(f\"  设备: {config.device}\")\n",
    "    print(f\"  批次大小: {config.batch_size}\")\n",
    "    print(f\"  最大迭代次数: {config.max_iters}\")\n",
    "    print(f\"  学习率: {config.learning_rate}\")\n",
    "    \n",
    "    # 步骤1: 准备数据\n",
    "    data_dir = os.path.join('data', config.dataset)\n",
    "    train_pkl = os.path.join(data_dir, 'train.pkl')\n",
    "    \n",
    "    print(\"expect: train_pkl:\",train_pkl)\n",
    "    \n",
    "    if not os.path.exists(train_pkl):\n",
    "        print(\"\\n未找到处理后的数据文件，开始准备数据...\")\n",
    "        prepare_data()\n",
    "    else:\n",
    "        print(\"\\n找到已处理的数据文件，跳过数据准备步骤\")\n",
    "        print(f\"如需重新准备数据，请删除 {data_dir} 目录\")\n",
    "    \n",
    "    # 步骤2: 训练模型\n",
    "    if not config.eval_only:\n",
    "        train()\n",
    "    else:\n",
    "        print(\"\\neval_only=True，跳过训练\")\n",
    "    \n",
    "    \n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-12T07:12:32.603591Z",
     "iopub.status.idle": "2025-11-12T07:12:32.603902Z",
     "shell.execute_reply": "2025-11-12T07:12:32.603753Z",
     "shell.execute_reply.started": "2025-11-12T07:12:32.603736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # 在你的代码文件底部，或者一个新的 cell 中运行\n",
    "# from tqdm import tqdm\n",
    "# import time\n",
    "\n",
    "# # 假设你的数据集已经准备好了\n",
    "# data_path = 'data/samsum/train.pkl' \n",
    "# block_size = 1024\n",
    "# dataset = SummarizationDataset(data_path, block_size)\n",
    "\n",
    "# # 测试前1000个样本的加载时间\n",
    "# total_time = 0\n",
    "# num_samples_to_test = 1000\n",
    "\n",
    "# start_time = time.time()\n",
    "# for i in tqdm(range(num_samples_to_test)):\n",
    "#     x, y = dataset[i]\n",
    "# end_time = time.time()\n",
    "\n",
    "# avg_time = (end_time - start_time) / num_samples_to_test * 1000  # 转换为毫秒\n",
    "# print(f\"\\n平均每个样本的 __getitem__ 耗时: {avg_time:.4f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-12T07:12:32.605044Z",
     "iopub.status.idle": "2025-11-12T07:12:32.605373Z",
     "shell.execute_reply": "2025-11-12T07:12:32.605216Z",
     "shell.execute_reply.started": "2025-11-12T07:12:32.605202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate():\n",
    "    \"\"\"\n",
    "    评估模型性能\n",
    "    \n",
    "    测试流程：\n",
    "    1. 加载训练好的模型\n",
    "    2. 从测试集中读取样本\n",
    "    3. 给定对话，让模型生成摘要（使用KV cache加速）\n",
    "    4. 计算生成摘要与真实摘要的ROUGE分数\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"开始评估...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 加载模型\n",
    "    model, enc, ctx = load_model()\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    # 获取<|endoftext|>的token ID作为停止符\n",
    "    eos_token_id = enc.encode(config.summary_end, allowed_special={config.summary_end})[0]\n",
    "    \n",
    "    # 加载测试数据\n",
    "    print(\"\\n加载测试数据...\")\n",
    "    test_file = os.path.join(config.dataset_path, 'validation.csv')\n",
    "    \n",
    "    dialogues = []\n",
    "    summaries = []\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            if i >= config.num_test_samples:\n",
    "                break\n",
    "            dialogues.append(row['dialogue'])\n",
    "            summaries.append(row['summary'])\n",
    "    \n",
    "    print(f\"加载了 {len(dialogues)} 条测试样本\")\n",
    "    \n",
    "    # 检查是否可以计算ROUGE\n",
    "    rouge_result = calculate_rouge(\"test\", \"test\")\n",
    "    use_rouge = rouge_result is not None\n",
    "    if not use_rouge:\n",
    "        print(\"\\n警告: 未安装rouge_score库，将跳过ROUGE评分\")\n",
    "        print(\"安装命令: pip install rouge-score\")\n",
    "    \n",
    "    # 评估每个样本\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"开始生成和评估...\")\n",
    "    print(\"优化: 使用KV cache + 提前停止\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    all_rouge1_f = []\n",
    "    all_rouge2_f = []\n",
    "    all_rougeL_f = []\n",
    "    \n",
    "    for idx, (dialogue, reference_summary) in enumerate(zip(dialogues, summaries)):\n",
    "        print(f\"\\n[样本 {idx+1}/{len(dialogues)}]\")\n",
    "        print(f\"对话: {dialogue[:100]}...\" if len(dialogue) > 100 else f\"对话: {dialogue}\")\n",
    "        \n",
    "        # 构建prompt（对话 + 摘要开始标记）\n",
    "        prompt = config.dialogue_start + dialogue + config.summary_start\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        prompt_tokens = enc.encode(prompt, allowed_special={config.summary_end})\n",
    "        \n",
    "        # 检查长度\n",
    "        if len(prompt_tokens) > config.block_size - config.max_new_tokens:\n",
    "            print(\"  警告: prompt太长，进行截断\")\n",
    "        \n",
    "        # 使用KV cache加速生成摘要\n",
    "        generated_text = generate_summary(model, prompt_tokens, enc, ctx, eos_token_id=eos_token_id)\n",
    "        \n",
    "        # 提取摘要\n",
    "        generated_summary = extract_summary(generated_text, prompt, enc)\n",
    "        \n",
    "        print(f\"真实摘要: {reference_summary}\")\n",
    "        print(f\"生成摘要: {generated_summary}\")\n",
    "        \n",
    "        # 计算ROUGE分数\n",
    "        if use_rouge:\n",
    "            rouge_scores = calculate_rouge(reference_summary, generated_summary)\n",
    "            if rouge_scores:\n",
    "                rouge1_f = rouge_scores['rouge1']\n",
    "                rouge2_f = rouge_scores['rouge2']\n",
    "                rougeL_f = rouge_scores['rougeL']\n",
    "                \n",
    "                print(f\"ROUGE-1: {rouge1_f:.4f}\")\n",
    "                print(f\"ROUGE-2: {rouge2_f:.4f}\")\n",
    "                print(f\"ROUGE-L: {rougeL_f:.4f}\")\n",
    "                \n",
    "                all_rouge1_f.append(rouge1_f)\n",
    "                all_rouge2_f.append(rouge2_f)\n",
    "                all_rougeL_f.append(rougeL_f)\n",
    "    \n",
    "    # 打印平均分数\n",
    "    if use_rouge and len(all_rouge1_f) > 0:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"平均ROUGE分数:\")\n",
    "        print(f\"  ROUGE-1: {np.mean(all_rouge1_f):.4f}\")\n",
    "        print(f\"  ROUGE-2: {np.mean(all_rouge2_f):.4f}\")\n",
    "        print(f\"  ROUGE-L: {np.mean(all_rougeL_f):.4f}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def predict_test_set_fast():\n",
    "    \"\"\"\n",
    "    使用KV cache加速的测试集推理\n",
    "    \n",
    "    优势：\n",
    "    1. 使用KV cache，避免重复计算，大幅提升速度\n",
    "    2. 设置提前停止符，遇到结束token立即停止\n",
    "    3. 使用我们训练的模型，确保兼容性\n",
    "    \n",
    "    流程：\n",
    "    1. 加载训练好的模型\n",
    "    2. 读取测试集数据\n",
    "    3. 使用KV cache逐样本生成摘要\n",
    "    4. 保存为提交格式\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"开始对测试集进行KV cache加速推理...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 加载模型\n",
    "    model, enc, ctx = load_model()\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    # 获取<|endoftext|>的token ID作为停止符\n",
    "    eos_token_id = enc.encode(config.summary_end, allowed_special={config.summary_end})[0]\n",
    "    \n",
    "    # 读取测试数据\n",
    "    print(\"\\n加载测试数据...\")\n",
    "    test_file = '/kaggle/input/nanogpt-fudannlp-cs-30040/test.csv'\n",
    "    \n",
    "    if not os.path.exists(test_file):\n",
    "        print(f\"错误: 找不到测试文件 {test_file}\")\n",
    "        return\n",
    "    \n",
    "    test_data = []\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            test_data.append({\n",
    "                'id': row['id'],\n",
    "                'dialogue': row['dialogue']\n",
    "            })\n",
    "    \n",
    "    print(f\"加载了 {len(test_data)} 条测试样本\")\n",
    "    \n",
    "    # 准备保存结果\n",
    "    results = []\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"开始生成摘要...\")\n",
    "    print(f\"优化: 使用KV cache + 提前停止 (遇到 '{config.summary_end}' 立即停止)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # 对每个样本生成摘要\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, sample in enumerate(test_data):\n",
    "        sample_id = sample['id']\n",
    "        dialogue = sample['dialogue']\n",
    "        \n",
    "        if (idx + 1) % 10 == 0 or idx == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            if idx > 0:\n",
    "                avg_time = elapsed / idx\n",
    "                remaining = avg_time * (len(test_data) - idx)\n",
    "                print(f\"处理进度: {idx+1}/{len(test_data)} | 平均耗时: {avg_time:.2f}秒/样本 | 预计剩余: {remaining/60:.1f}分钟\")\n",
    "            else:\n",
    "                print(f\"处理进度: {idx+1}/{len(test_data)}\")\n",
    "        \n",
    "        # 构建prompt（对话 + 摘要开始标记）\n",
    "        prompt = config.dialogue_start + dialogue + config.summary_start\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        prompt_tokens = enc.encode(prompt, allowed_special={config.summary_end})\n",
    "        \n",
    "        # 使用KV cache加速生成摘要\n",
    "        generated_text = generate_summary(model, prompt_tokens, enc, ctx, eos_token_id=eos_token_id)\n",
    "        \n",
    "        # 提取摘要\n",
    "        generated_summary = extract_summary(generated_text, prompt, enc)\n",
    "        \n",
    "        results.append({\n",
    "            'id': sample_id,\n",
    "            'summary': generated_summary\n",
    "        })\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n生成完成！总耗时: {total_time/60:.1f}分钟 | 平均: {total_time/len(test_data):.2f}秒/样本\")\n",
    "    \n",
    "    # 保存结果到CSV文件\n",
    "    output_file = 'submission.csv'\n",
    "    output_path = os.path.join(config.out_dir, output_file)\n",
    "    \n",
    "    print(f\"\\n保存结果到 {output_path}\")\n",
    "    with open(output_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['id', 'summary'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "    \n",
    "    print(f\"完成！生成了 {len(results)} 条摘要\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-12T07:12:32.606553Z",
     "iopub.status.idle": "2025-11-12T07:12:32.606791Z",
     "shell.execute_reply": "2025-11-12T07:12:32.606700Z",
     "shell.execute_reply.started": "2025-11-12T07:12:32.606690Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install rouge-score\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-12T07:12:32.608138Z",
     "iopub.status.idle": "2025-11-12T07:12:32.608481Z",
     "shell.execute_reply": "2025-11-12T07:12:32.608316Z",
     "shell.execute_reply.started": "2025-11-12T07:12:32.608299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predict_test_set_fast()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14409389,
     "isSourceIdPinned": false,
     "sourceId": 120001,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
